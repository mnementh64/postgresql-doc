{
    "docs": [
        {
            "location": "/",
            "text": "PostgreSQL Overview\n\u00b6\n\n\nPostgreSQL is a MVCC database. It means all transactions generate new lines. For exaple, an update is in fact a delete and an insert and the old line still exists on disk.\nSo vacuum is a complementary process so important to regain space ! \n\n\nSee this \nblog post\n for a good explanation of MVCC model and vacuum.\n\n\nMVCC model (see \ndetails\n) means data are not really deleted, and updated data are in fact deleted then inserted. So, natural trends is to always occupy more and more space.\n\n\nVacuum processes (\nSee details\n) are made to recover / recycle this space. And Analyze \nSee details\n refresh the internal statistics so the query planner could build the most efficient query plan. \n\n\nTo optimize query response, PostgreSQL use an internal cache system, mainly known ads Shared buffers. \nSee details\n. \n\n\nAn overview of PostgreSQL writings\n\u00b6\n\n\n.\n\n\nAs a transaction is committed, it's first flushed to WAL files, designed to store transactions for data coherence insurance. \nSee details\n.\n\n\nAs the same time, shared buffers is used to store data, indexes to speed up backend queries execution. Data are flushed to disk in a regular way, using checkpoint process. \nSee details\n.    \n\n\nConfiguration\n\u00b6\n\n\nPostgreSQL configuration is located in the \n/etc/postgresql/9.6/main/postgresql.conf\n file.\n\n\nMany parameters could be hot reloaded by \nsudo /etc/init.d/postgres reload\n for example.\n\n\nSome others need a restart : \nsudo /etc/init.d/postgres restart\n.\n\n\nThe most useful way to customize the configuration is to include a file containing only customized parameters (or several files in a directory). See the official \ndocumentation\n. \n\n\nIn the \npostgres.conf\n file, add an \ninclude 'filename'\n directive where \nfilename\n could be either a path relative to the \npostgres.conf\n file location or an absolute path.",
            "title": "Overview"
        },
        {
            "location": "/#postgresql-overview",
            "text": "PostgreSQL is a MVCC database. It means all transactions generate new lines. For exaple, an update is in fact a delete and an insert and the old line still exists on disk.\nSo vacuum is a complementary process so important to regain space !   See this  blog post  for a good explanation of MVCC model and vacuum.  MVCC model (see  details ) means data are not really deleted, and updated data are in fact deleted then inserted. So, natural trends is to always occupy more and more space.  Vacuum processes ( See details ) are made to recover / recycle this space. And Analyze  See details  refresh the internal statistics so the query planner could build the most efficient query plan.   To optimize query response, PostgreSQL use an internal cache system, mainly known ads Shared buffers.  See details .",
            "title": "PostgreSQL Overview"
        },
        {
            "location": "/#an-overview-of-postgresql-writings",
            "text": ".  As a transaction is committed, it's first flushed to WAL files, designed to store transactions for data coherence insurance.  See details .  As the same time, shared buffers is used to store data, indexes to speed up backend queries execution. Data are flushed to disk in a regular way, using checkpoint process.  See details .",
            "title": "An overview of PostgreSQL writings"
        },
        {
            "location": "/#configuration",
            "text": "PostgreSQL configuration is located in the  /etc/postgresql/9.6/main/postgresql.conf  file.  Many parameters could be hot reloaded by  sudo /etc/init.d/postgres reload  for example.  Some others need a restart :  sudo /etc/init.d/postgres restart .  The most useful way to customize the configuration is to include a file containing only customized parameters (or several files in a directory). See the official  documentation .   In the  postgres.conf  file, add an  include 'filename'  directive where  filename  could be either a path relative to the  postgres.conf  file location or an absolute path.",
            "title": "Configuration"
        },
        {
            "location": "/mvcc/",
            "text": "MVCC model\n\u00b6\n\n\nWhat's MVCC ?\n\u00b6\n\n\nA good reading (rather complete) could be found here : \nhttp://momjian.us/main/writings/pgsql/mvcc.pdf\n. BTW, you could have a look to all PDFs available in \nMomjian\n repo : \nhttp://momjian.us/main/writings/pgsql/\n.\n\n\nPostgreSQL is a \nMultiversion Concurrency Control\n (MVCC) database : what does that mean ? \n\n\nMVCC ensure \nreaders never block writers, and writers never block readers\n.\n\n\nWhen a transaction begins, the changed rows are flagged with a transaction ID and as it's committed, it's flagged again with an expiration ID. \n\n\nAt any moment, there is a current transaction ID in the server. For any transaction at this moment, data rows are visible if their expiration ID is less than the current ID.\n\n\n.\n\n\nSo a deleted row is not really deleted, it's just not visible anymore. And an updated row is first deleted, and its new value is inserted.\n\n\nRegain space\n\u00b6\n\n\nOf course, one of the big downside of MVCC is its forever growing size. Two Postgres mecanism are dedicated to this task :\n\n\n\n\ncleanup : is only triggered by some queries and only concern some tuples. It doesn't clean indexes. \n\n\nautovacuum : does the full job but take a longer time and more CPU / IOs. See \ndetails\n.",
            "title": "MVCC model"
        },
        {
            "location": "/mvcc/#mvcc-model",
            "text": "",
            "title": "MVCC model"
        },
        {
            "location": "/mvcc/#whats-mvcc",
            "text": "A good reading (rather complete) could be found here :  http://momjian.us/main/writings/pgsql/mvcc.pdf . BTW, you could have a look to all PDFs available in  Momjian  repo :  http://momjian.us/main/writings/pgsql/ .  PostgreSQL is a  Multiversion Concurrency Control  (MVCC) database : what does that mean ?   MVCC ensure  readers never block writers, and writers never block readers .  When a transaction begins, the changed rows are flagged with a transaction ID and as it's committed, it's flagged again with an expiration ID.   At any moment, there is a current transaction ID in the server. For any transaction at this moment, data rows are visible if their expiration ID is less than the current ID.  .  So a deleted row is not really deleted, it's just not visible anymore. And an updated row is first deleted, and its new value is inserted.",
            "title": "What's MVCC ?"
        },
        {
            "location": "/mvcc/#regain-space",
            "text": "Of course, one of the big downside of MVCC is its forever growing size. Two Postgres mecanism are dedicated to this task :   cleanup : is only triggered by some queries and only concern some tuples. It doesn't clean indexes.   autovacuum : does the full job but take a longer time and more CPU / IOs. See  details .",
            "title": "Regain space"
        },
        {
            "location": "/cache_system/",
            "text": "Cache system\n\u00b6\n\n\nA good reading could be found on this \nblog\n.\n\n\nThe main cache system is \nShared buffers\n, tuned in \npostgres.conf\n by the \nshared_buffers = 128MB\n parameter. It's used to cache :\n\n\n\n\ntables data\n\n\ntables indexes\n\n\nquery plans\n\n\n\n\nEach time a data becomes dirty, it has to be flushed to the disk and to WAL files.\n\n\nThe \nwal_writer\n process writes to WAL files and can be done asynchronously by changing a setting (see \ndetails\n).\n\n\nThe \nbackground writer\n is responsible for the data writing : \n\n\n\n\ntriggered by checkpoint : it spreads the writing for almost all the configured checkpoint time.\n\n\ntriggered by backend request : it reads from the cache and in case of dirty pages, writes to disk then loads to cache.\n\n\ntriggered by clean process : it makes some place free in case of reallocation need.\n\n\n\n\nThe objective is to make the job mainly done by checkpointer process because :\n\n\n\n\nbackend buffers introduce latency for users queries\n\n\nclean process means the cache is too small",
            "title": "Cache system"
        },
        {
            "location": "/cache_system/#cache-system",
            "text": "A good reading could be found on this  blog .  The main cache system is  Shared buffers , tuned in  postgres.conf  by the  shared_buffers = 128MB  parameter. It's used to cache :   tables data  tables indexes  query plans   Each time a data becomes dirty, it has to be flushed to the disk and to WAL files.  The  wal_writer  process writes to WAL files and can be done asynchronously by changing a setting (see  details ).  The  background writer  is responsible for the data writing :    triggered by checkpoint : it spreads the writing for almost all the configured checkpoint time.  triggered by backend request : it reads from the cache and in case of dirty pages, writes to disk then loads to cache.  triggered by clean process : it makes some place free in case of reallocation need.   The objective is to make the job mainly done by checkpointer process because :   backend buffers introduce latency for users queries  clean process means the cache is too small",
            "title": "Cache system"
        },
        {
            "location": "/wal_files/",
            "text": "Wal files\n\u00b6\n\n\nWhat's is a WAL file ?\n\u00b6\n\n\nThis is a good reading\n.\n\n\nWAL means \"Write Ahead Log\". Any transaction is first written out as a WAL file before applied to on-disk data. See \ncheckpoints page\n for details. \n\n\nFile nomenclature\n\u00b6\n\n\nFiles are located into \n$PG_DIR/pg_xlog\n for version 9.x and in \n$PG_DIR/pg_wal\n since 10. \n\n\nEach file is sized 16MB. This value is computed by default from the \nshared_buffers\n parameters :\n\n\n#wal_buffers = -1                       # min 32kB, -1 sets based on shared_buffers\n                                        # (change requires restart)\n\n\n\n\n\nWAL files are named like : \n000000020000070A0000008E\n where :\n\n\n\n\n00000002\n : the timeline\n\n\n0000070A\n : the logical xlog file \n\n\n0000008E\n : the physical xlog file\n\n\n\n\nLet's have a look to a \npg_xlog\n content by typing \nls -l /var/lib/postgresql/9.6/main/pg_xlog/\n. It outputs :\n\n\n-rw------- 1 postgres postgres 16777216 Jan 23 04:56 0000000100001D710000001D\n-rw------- 1 postgres postgres 16777216 Jan 23 05:50 0000000100001D710000001E\n-rw------- 1 postgres postgres 16777216 Jan 23 05:58 0000000100001D710000001F\n-rw------- 1 postgres postgres 16777216 Jan 23 04:50 0000000100001D7100000020\n-rw------- 1 postgres postgres 16777216 Jan 23 05:36 0000000100001D7100000021\n-rw------- 1 postgres postgres 16777216 Jan 23 05:55 0000000100001D7100000022\n-rw------- 1 postgres postgres 16777216 Jan 23 04:35 0000000100001D7100000023\n-rw------- 1 postgres postgres 16777216 Jan 23 05:43 0000000100001D7100000024\n-rw------- 1 postgres postgres 16777216 Jan 23 04:55 0000000100001D7100000025\n-rw------- 1 postgres postgres 16777216 Jan 23 05:20 0000000100001D7100000026\n\n\n\n\n\nTo see a wal file content, use the \npg_xlogdump\n tool by typing :\n\n\n/usr/lib/postgresql/9.6/bin/pg_xlogdump -n 20 -f /var/lib/postgresql/9.6/main/pg_xlog/0000000100001D7000000037\n\n\n\n\n\nIt outputs a human readable version of the binary file content :\n\n\nrmgr\n:\n \nBtree\n       \nlen\n \n(\nrec\n/tot):      2/    64, tx:  265963650, lsn: 1D70/37000028, prev 1D70/36FFFFC0, desc: INSERT_LEAF off 243, blkref #0: rel 1663/20430/\n16757724\n \nblk\n \n678\n\n\nrmgr\n:\n \nHeap\n        \nlen\n \n(\nrec\n/tot):      3/   111, tx:  265963650, lsn: 1D70/37000068, prev 1D70/37000028, desc: INSERT off 51, blkref #0: rel 1663/20430/\n16757720\n \nblk\n \n9692\n\n\nrmgr\n:\n \nBtree\n       \nlen\n \n(\nrec\n/tot):      2/    64, tx:  265963650, lsn: 1D70/370000D8, prev 1D70/37000068, desc: INSERT_LEAF off 243, blkref #0: rel 1663/20430/\n16757723\n \nblk\n \n678\n\n\nrmgr\n:\n \nBtree\n       \nlen\n \n(\nrec\n/tot):      2/    64, tx:  265963650, lsn: 1D70/37000118, prev 1D70/370000D8, desc: INSERT_LEAF off 243, blkref #0: rel 1663/20430/\n16757724\n \nblk\n \n678\n\n\nrmgr\n:\n \nHeap\n        \nlen\n \n(\nrec\n/tot):      3/   111, tx:  265963650, lsn: 1D70/37000158, prev 1D70/37000118, desc: INSERT off 52, blkref #0: rel 1663/20430/\n16757720\n \nblk\n \n9692\n\n\nrmgr\n:\n \nBtree\n       \nlen\n \n(\nrec\n/tot):      2/    64, tx:  265963650, lsn: 1D70/370001C8, prev 1D70/37000158, desc: INSERT_LEAF off 243, blkref #0: rel 1663/20430/\n16757723\n \nblk\n \n678\n\n\nrmgr\n:\n \nBtree\n       \nlen\n \n(\nrec\n/tot):      2/    64, tx:  265963650, lsn: 1D70/37000208, prev 1D70/370001C8, desc: INSERT_LEAF off 243, blkref #0: rel 1663/20430/\n16757724\n \nblk\n \n678\n\n\nrmgr\n:\n \nHeap\n        \nlen\n \n(\nrec\n/tot):      3/   111, tx:  265963650, lsn: 1D70/37000248, prev 1D70/37000208, desc: INSERT off 53, blkref #0: rel 1663/20430/\n16757720\n \nblk\n \n9692\n\n\nrmgr\n:\n \nBtree\n       \nlen\n \n(\nrec\n/tot):      2/    64, tx:  265963650, lsn: 1D70/370002B8, prev 1D70/37000248, desc: INSERT_LEAF off 243, blkref #0: rel 1663/20430/\n16757723\n \nblk\n \n678\n\n\n\n\n\n\nThe first column displays the resource manager : what kind of resources is impacted by the transaction ? Index btree, Heap operation like INSERT, ...\n\n\nTransaction location\n\u00b6\n\n\nSomething like \n77FA/5F11E520\n is a transaction log location, it means the location of a transaction in a log file :\n\n\n\n\n77FA\n is the logical xlog file\n\n\n5F11E520\n is the offset inside the logical xlog.\n\n\n\n\nTo know in what log file a transaction is located, use \n\n\npostgres=# select pg_xlogfile_name('68A/16E1DA8');\n     pg_xlogfile_name  \n--------------------------\n 000000020000068A00000001\n(1 row)\n\n\n\n\n\nTo know in which log file postgreSQL is currently writing, use \n\n\npostgres=# select pg_xlogfile_name(pg_current_xlog_location());\n     pg_xlogfile_name  \n--------------------------\n 000000020000074B000000E4\n(1 row)",
            "title": "Wal files"
        },
        {
            "location": "/wal_files/#wal-files",
            "text": "",
            "title": "Wal files"
        },
        {
            "location": "/wal_files/#whats-is-a-wal-file",
            "text": "This is a good reading .  WAL means \"Write Ahead Log\". Any transaction is first written out as a WAL file before applied to on-disk data. See  checkpoints page  for details.",
            "title": "What's is a WAL file ?"
        },
        {
            "location": "/wal_files/#file-nomenclature",
            "text": "Files are located into  $PG_DIR/pg_xlog  for version 9.x and in  $PG_DIR/pg_wal  since 10.   Each file is sized 16MB. This value is computed by default from the  shared_buffers  parameters :  #wal_buffers = -1                       # min 32kB, -1 sets based on shared_buffers\n                                        # (change requires restart)  WAL files are named like :  000000020000070A0000008E  where :   00000002  : the timeline  0000070A  : the logical xlog file   0000008E  : the physical xlog file   Let's have a look to a  pg_xlog  content by typing  ls -l /var/lib/postgresql/9.6/main/pg_xlog/ . It outputs :  -rw------- 1 postgres postgres 16777216 Jan 23 04:56 0000000100001D710000001D\n-rw------- 1 postgres postgres 16777216 Jan 23 05:50 0000000100001D710000001E\n-rw------- 1 postgres postgres 16777216 Jan 23 05:58 0000000100001D710000001F\n-rw------- 1 postgres postgres 16777216 Jan 23 04:50 0000000100001D7100000020\n-rw------- 1 postgres postgres 16777216 Jan 23 05:36 0000000100001D7100000021\n-rw------- 1 postgres postgres 16777216 Jan 23 05:55 0000000100001D7100000022\n-rw------- 1 postgres postgres 16777216 Jan 23 04:35 0000000100001D7100000023\n-rw------- 1 postgres postgres 16777216 Jan 23 05:43 0000000100001D7100000024\n-rw------- 1 postgres postgres 16777216 Jan 23 04:55 0000000100001D7100000025\n-rw------- 1 postgres postgres 16777216 Jan 23 05:20 0000000100001D7100000026  To see a wal file content, use the  pg_xlogdump  tool by typing :  /usr/lib/postgresql/9.6/bin/pg_xlogdump -n 20 -f /var/lib/postgresql/9.6/main/pg_xlog/0000000100001D7000000037  It outputs a human readable version of the binary file content :  rmgr :   Btree         len   ( rec /tot):      2/    64, tx:  265963650, lsn: 1D70/37000028, prev 1D70/36FFFFC0, desc: INSERT_LEAF off 243, blkref #0: rel 1663/20430/ 16757724   blk   678  rmgr :   Heap          len   ( rec /tot):      3/   111, tx:  265963650, lsn: 1D70/37000068, prev 1D70/37000028, desc: INSERT off 51, blkref #0: rel 1663/20430/ 16757720   blk   9692  rmgr :   Btree         len   ( rec /tot):      2/    64, tx:  265963650, lsn: 1D70/370000D8, prev 1D70/37000068, desc: INSERT_LEAF off 243, blkref #0: rel 1663/20430/ 16757723   blk   678  rmgr :   Btree         len   ( rec /tot):      2/    64, tx:  265963650, lsn: 1D70/37000118, prev 1D70/370000D8, desc: INSERT_LEAF off 243, blkref #0: rel 1663/20430/ 16757724   blk   678  rmgr :   Heap          len   ( rec /tot):      3/   111, tx:  265963650, lsn: 1D70/37000158, prev 1D70/37000118, desc: INSERT off 52, blkref #0: rel 1663/20430/ 16757720   blk   9692  rmgr :   Btree         len   ( rec /tot):      2/    64, tx:  265963650, lsn: 1D70/370001C8, prev 1D70/37000158, desc: INSERT_LEAF off 243, blkref #0: rel 1663/20430/ 16757723   blk   678  rmgr :   Btree         len   ( rec /tot):      2/    64, tx:  265963650, lsn: 1D70/37000208, prev 1D70/370001C8, desc: INSERT_LEAF off 243, blkref #0: rel 1663/20430/ 16757724   blk   678  rmgr :   Heap          len   ( rec /tot):      3/   111, tx:  265963650, lsn: 1D70/37000248, prev 1D70/37000208, desc: INSERT off 53, blkref #0: rel 1663/20430/ 16757720   blk   9692  rmgr :   Btree         len   ( rec /tot):      2/    64, tx:  265963650, lsn: 1D70/370002B8, prev 1D70/37000248, desc: INSERT_LEAF off 243, blkref #0: rel 1663/20430/ 16757723   blk   678   The first column displays the resource manager : what kind of resources is impacted by the transaction ? Index btree, Heap operation like INSERT, ...",
            "title": "File nomenclature"
        },
        {
            "location": "/wal_files/#transaction-location",
            "text": "Something like  77FA/5F11E520  is a transaction log location, it means the location of a transaction in a log file :   77FA  is the logical xlog file  5F11E520  is the offset inside the logical xlog.   To know in what log file a transaction is located, use   postgres=# select pg_xlogfile_name('68A/16E1DA8');\n     pg_xlogfile_name  \n--------------------------\n 000000020000068A00000001\n(1 row)  To know in which log file postgreSQL is currently writing, use   postgres=# select pg_xlogfile_name(pg_current_xlog_location());\n     pg_xlogfile_name  \n--------------------------\n 000000020000074B000000E4\n(1 row)",
            "title": "Transaction location"
        },
        {
            "location": "/checkpoints/",
            "text": "Checkpoints\n\u00b6\n\n\nWhat's is a checkpoint ?\n\u00b6\n\n\nThis is a good reading\n from 2nd quadrant.\n\n\nData are first written to WAL (\nWrite Ahead Log\n) files before being copied to data files. That way to process ensures durability : in case of a crash, database will use WAL to recover data and write them to data files.\n\n\nUsers only have to wait for WAL files to be flushed to disk to have data available in DB. By this time, RAM cache could be used for users and be flushed to disk later. \n\n\nA checkpoint writes all dirty pages from memory to disk, marks them as \u201cclean\" in shared_buffers, and stores information that all of wal up to now is applied : WAL files are not needed before the checkpoint position.\n\n\n\n\nIn practice, we often want to find a good ratio between :\n\n\n\n\nhaving not too frequent checkpoints so IO for buffers is spread on a large amount of time\n\n\nbut frequent enough to limit the recovery time after a crash and to limit the disk usage for WAL files \n\n\n\n\nWhat trigger checkpoints ?\n\u00b6\n\n\nBasically, timeout or size :\n\n\n\n\nit's been more than the \ncheckpoint_timeout\n value since previous checkpoint ended\n\n\nWAL folder (\n/pgsql_folder/pg_xlog/\n before version 10 or \n/pgsql_folder/pg_wal/\n since) if full. Its size could not be more than the \nmax_wal_size\n value.\n\n\n\n\nCheckpoints should be mostly triggered by timeout, not size.\n\n\nSpread checkpoints\n\u00b6\n\n\nCheckpoints is basically a writing to disk of dirty buffers. Its job must be done during a fraction of the checkpoint total time, fraction set by the \ncheckpoint_completion_target\n parameters (0.5 by default).\n\n\nCheckpoint and Linux Cache\n\u00b6\n\n\nLinux cache by default can handle a great amount of data before flushing it to the storage. It could be harmful for the checkpoint process. If we tune the checkpoint to spread the writing on a quite long time and the OS cache get it and flush all at a single time, it will create an I/O pike. \n\n\nIt could be better not to let the cache keep too much data and be force to flush it more frequently.\n\n\nSee \ndetails on how to tune Linux Kernel\n for Database performance usage.\n\n\nControl activity\n\u00b6\n\n\nOf course, stats could be gathered. But also, a global status of the server is available by the command \n/usr/lib/postgresql/9.6/bin/pg_controldata /var/lib/postgresql/9.6/main/\n and outputs :\n\n\npg_control version number:            960\nCatalog version number:               201608131\nDatabase system identifier:           6382382084904487037\nDatabase cluster state:               in production\npg_control last modified:             Tue 23 Jan 2018 10:24:56 AM CET\nLatest checkpoint location:           1D70/D4518DB8\nPrior checkpoint location:            1D6F/58F47450\nLatest checkpoint's REDO location:    1D6F/80331B08\nLatest checkpoint's REDO WAL file:    0000000100001D6F00000080\nLatest checkpoint's TimeLineID:       1\nLatest checkpoint's PrevTimeLineID:   1\nLatest checkpoint's full_page_writes: on\nLatest checkpoint's NextXID:          0:265937941\nLatest checkpoint's NextOID:          17869586\nLatest checkpoint's NextMultiXactId:  1\nLatest checkpoint's NextMultiOffset:  0\nLatest checkpoint's oldestXID:        70567326\nLatest checkpoint's oldestXID's DB:   20430\nLatest checkpoint's oldestActiveXID:  0\nLatest checkpoint's oldestMultiXid:   1\nLatest checkpoint's oldestMulti's DB: 20430\nLatest checkpoint's oldestCommitTsXid:0\nLatest checkpoint's newestCommitTsXid:0\nTime of latest checkpoint:            Tue 23 Jan 2018 09:03:56 AM CET\nFake LSN counter for unlogged rels:   0/1\nMinimum recovery ending location:     0/0\nMin recovery ending loc's timeline:   0\nBackup start location:                0/0\nBackup end location:                  0/0\nEnd-of-backup record required:        no\nwal_level setting:                    minimal\nwal_log_hints setting:                off\nmax_connections setting:              200\nmax_worker_processes setting:         8\nmax_prepared_xacts setting:           0\nmax_locks_per_xact setting:           256\ntrack_commit_timestamp setting:       off\nMaximum data alignment:               8\nDatabase block size:                  8192\nBlocks per segment of large relation: 131072\nWAL block size:                       8192\nBytes per WAL segment:                16777216\nMaximum length of identifiers:        64\nMaximum columns in an index:          32\nMaximum size of a TOAST chunk:        1996\nSize of a large-object chunk:         2048\nDate/time type storage:               64-bit integers\nFloat4 argument passing:              by value\nFloat8 argument passing:              by value\nData page checksum version:           0\n\n\n\n\n\nThat is a mix of settings and running statuses.",
            "title": "Checkpoints"
        },
        {
            "location": "/checkpoints/#checkpoints",
            "text": "",
            "title": "Checkpoints"
        },
        {
            "location": "/checkpoints/#whats-is-a-checkpoint",
            "text": "This is a good reading  from 2nd quadrant.  Data are first written to WAL ( Write Ahead Log ) files before being copied to data files. That way to process ensures durability : in case of a crash, database will use WAL to recover data and write them to data files.  Users only have to wait for WAL files to be flushed to disk to have data available in DB. By this time, RAM cache could be used for users and be flushed to disk later.   A checkpoint writes all dirty pages from memory to disk, marks them as \u201cclean\" in shared_buffers, and stores information that all of wal up to now is applied : WAL files are not needed before the checkpoint position.   In practice, we often want to find a good ratio between :   having not too frequent checkpoints so IO for buffers is spread on a large amount of time  but frequent enough to limit the recovery time after a crash and to limit the disk usage for WAL files",
            "title": "What's is a checkpoint ?"
        },
        {
            "location": "/checkpoints/#what-trigger-checkpoints",
            "text": "Basically, timeout or size :   it's been more than the  checkpoint_timeout  value since previous checkpoint ended  WAL folder ( /pgsql_folder/pg_xlog/  before version 10 or  /pgsql_folder/pg_wal/  since) if full. Its size could not be more than the  max_wal_size  value.   Checkpoints should be mostly triggered by timeout, not size.",
            "title": "What trigger checkpoints ?"
        },
        {
            "location": "/checkpoints/#spread-checkpoints",
            "text": "Checkpoints is basically a writing to disk of dirty buffers. Its job must be done during a fraction of the checkpoint total time, fraction set by the  checkpoint_completion_target  parameters (0.5 by default).",
            "title": "Spread checkpoints"
        },
        {
            "location": "/checkpoints/#checkpoint-and-linux-cache",
            "text": "Linux cache by default can handle a great amount of data before flushing it to the storage. It could be harmful for the checkpoint process. If we tune the checkpoint to spread the writing on a quite long time and the OS cache get it and flush all at a single time, it will create an I/O pike.   It could be better not to let the cache keep too much data and be force to flush it more frequently.  See  details on how to tune Linux Kernel  for Database performance usage.",
            "title": "Checkpoint and Linux Cache"
        },
        {
            "location": "/checkpoints/#control-activity",
            "text": "Of course, stats could be gathered. But also, a global status of the server is available by the command  /usr/lib/postgresql/9.6/bin/pg_controldata /var/lib/postgresql/9.6/main/  and outputs :  pg_control version number:            960\nCatalog version number:               201608131\nDatabase system identifier:           6382382084904487037\nDatabase cluster state:               in production\npg_control last modified:             Tue 23 Jan 2018 10:24:56 AM CET\nLatest checkpoint location:           1D70/D4518DB8\nPrior checkpoint location:            1D6F/58F47450\nLatest checkpoint's REDO location:    1D6F/80331B08\nLatest checkpoint's REDO WAL file:    0000000100001D6F00000080\nLatest checkpoint's TimeLineID:       1\nLatest checkpoint's PrevTimeLineID:   1\nLatest checkpoint's full_page_writes: on\nLatest checkpoint's NextXID:          0:265937941\nLatest checkpoint's NextOID:          17869586\nLatest checkpoint's NextMultiXactId:  1\nLatest checkpoint's NextMultiOffset:  0\nLatest checkpoint's oldestXID:        70567326\nLatest checkpoint's oldestXID's DB:   20430\nLatest checkpoint's oldestActiveXID:  0\nLatest checkpoint's oldestMultiXid:   1\nLatest checkpoint's oldestMulti's DB: 20430\nLatest checkpoint's oldestCommitTsXid:0\nLatest checkpoint's newestCommitTsXid:0\nTime of latest checkpoint:            Tue 23 Jan 2018 09:03:56 AM CET\nFake LSN counter for unlogged rels:   0/1\nMinimum recovery ending location:     0/0\nMin recovery ending loc's timeline:   0\nBackup start location:                0/0\nBackup end location:                  0/0\nEnd-of-backup record required:        no\nwal_level setting:                    minimal\nwal_log_hints setting:                off\nmax_connections setting:              200\nmax_worker_processes setting:         8\nmax_prepared_xacts setting:           0\nmax_locks_per_xact setting:           256\ntrack_commit_timestamp setting:       off\nMaximum data alignment:               8\nDatabase block size:                  8192\nBlocks per segment of large relation: 131072\nWAL block size:                       8192\nBytes per WAL segment:                16777216\nMaximum length of identifiers:        64\nMaximum columns in an index:          32\nMaximum size of a TOAST chunk:        1996\nSize of a large-object chunk:         2048\nDate/time type storage:               64-bit integers\nFloat4 argument passing:              by value\nFloat8 argument passing:              by value\nData page checksum version:           0  That is a mix of settings and running statuses.",
            "title": "Control activity"
        },
        {
            "location": "/autovacuum/",
            "text": "Autovacuum\n\u00b6\n\n\nWhat is an autovacuum ?\n\u00b6\n\n\nIt's an automatic vacuum !\n\n\nA \ngood reading\n to understand what is autovacuum and why it exists.\n\n\nPostgreSQL is a MVCC database : what does that mean ? \n\n\nA good reading (quite complete !) could be found here : \nhttp://momjian.us/main/writings/pgsql/mvcc.pdf\n. \n\n\nBTW, you could have a look to all PDFs available in \nMomjian\n repo : \nhttp://momjian.us/main/writings/pgsql/\n.\n\n\nThe most important is to understand a MVCC database have a natural behavior to grow for ever if nothing is done.\n\n\nVacuum is a way :\n\n\n\n\nto free / reuse the space (pages) used by transactions once they are committed.\n\n\nto update tables statistics so the query planner could rely on good info (\nANALYZE\n command, see \nbelow\n for details)\n\n\nto maintain the \nvisibility map\n of what pages contain only tuples visible to all transaction or future one (to help vacuum process itself or help index scans).  \n\n\n\n\nAutovacuum\n is a PostgreSQL process to automate vacuum on databases.\n\n\nTo regain all possible space, we must execute a \nVACUUM FULL\n query, which needs an exclusive locks on the table : not easy in production environment !\n\n\n\n\nNote\n\n\nTRUNCATE\n command doesn't require to regain free space, so no auto VACUUM is launched after it. \n\n\n\n\n\n\nDocumentation\n\n\nOfficial link to \nVACUUM command\n\n\n\n\nTechnical precision\n\u00b6\n\n\nPeople often assume that VACUUM is the process that should return the disk space to the file system. It does do this but only in very specific cases.\n\n\nThat used space is contained in page files that make up the tables and indexes (called objects from now on) in the Postgres database system. \nPage files all have the same size and differently sized objects just have as many page files as they need. \nIf VACUUM happens to mark every row in a page file as unavailable AND that page also happens to be the final page for the entire object, THEN the disk space is returned to the file system. \n\n\nIf there is a single available row, or the page file is any other but the last one, the disk space is never returned by a normal VACUUM. \n\nThis is bloat.\n See \nbelow\n for details. \n\n\nWhat triggers Autovacuum ?\n\u00b6\n\n\nA worker seeks for tables with :\n\n\nvacuum threshold = autovacuum_vacuum_threshold + autovacuum_vacuum_scale_factor * number of tuples\n\n\n\n\n\nBut it keeps also a trace of I/O \"credits\" that are consumed. When credits used exceed the \nautovacuum_vacuum_cost_limit\n then autovacuum pauses all workers for \nautovacuum_vacuum_cost_delay\n milliseconds.\n\n\nDefault factor \nautovacuum_vacuum_scale_factor = 0.2\n is far too high for big tables. It couldn't help to control table bloating (see \nbelow\n for details).\n\n\n.\n\n\nBloating\n\u00b6\n\n\nIn MVCC database servers, tables updates, transactions uses spaces and need to be vacuumed to regain space. If it's not the case, table / index disk size possibly increase a lot : phenomenon named bloating.  \n\n\nIt could be estimated by some queries : see details in the \nstatistics page\n.\n\n\nBad cases could occur with PostgreSQL catalog tables themselves bloated !\n\n\nBloating could be bad because when a query is executed on a table, all rows visibility flags are checked to see if the row is actually available for the transaction, including rows of the bloated part. \n\n\nTo understand why bloating is not always bad could be found \nhere\n.\n\n\nA good example of investigation on table bloating cause : \nhttps://www.cybertec-postgresql.com/en/stale-statistics-cause-table-bloat/\n.\n\n\nIndex bloating\n\u00b6\n\n\nIndex bloating is even worse than table bloating because an index points to items, not tuples so it doesn't know if an item points to dead tuple or not. Thus, bloated indexes are slower.\n\n\nTo refresh an index :\n\n\nREINDEX INDEX the_index;\n\n\n\n\n\nTo refresh all indexes of table :\n\n\nREINDEX TABLE the_table;\n\n\n\n\n\nTo refresh all indexes of all system tables : \n\n\nREINDEX SYSTEM \"the_database\";\n\n\n\n\n\nTime series\n\u00b6\n\n\nOf course, for time series, standard autovacuum is an issue because table is never empty and space could never be regained. Some rows could be re-used but occupied space will grow forever, even slowly.\n\n\nOther solutions are described in this post : \nhttps://www.redpill-linpro.com/sysadvent/2017/12/08/pg_repack.html\n.\n\n\npg_repack\n\u00b6\n\n\nThis project could be a good solution : \nhttps://github.com/reorg/pg_repack/\n.\n\n\nInstall on Debian\n\n\napt-get update && apt-get install -y postgresql-9.6-repack\n\n\n\n\n\nCreate an extension in the DB \n\n\npsql -U my_user -h localhost -c \"CREATE EXTENSION pg_repack\" -d the_db\n\n\n\n\n\nLaunch pg_repack on a table \n\n\n/usr/lib/postgresql/9.6/bin/pg_repack --user my_user --table=probes_global_statistics --jobs 5 the_db\n\n\n\n\n\n\n\nWarning\n\n\nTables cleaned by pg_repack must have a primary key or not-null unique keys. Thus, netflow aggregation tables are not eligible for this : they don't have a primary key.\n\n\n\n\nAnalyze\n\u00b6\n\n\nANALYZE table [column (, ...)]\n SQL command updates query planner statistic for this table. It's important to have up-to-date stats so the planner could compute efficient strategies.\n\n\nIt's not useful to frequently ANALYZE columns with a quite constant data distribution. \n\n\nIt's really important to frequently ANALYZE columns with increasing values, like timestamp for example.\n\n\nAutomatic analyze are triggered when the number of modified rows in a table exceeds \n\n\nvacuum threshold = autovacuum_analyze_threshold + autovacuum_analyze_scale_factor * number of tuples\n\n\n\n\n\n\n\nDocumentation\n\n\nOfficial link to \nANALYZE command",
            "title": "Autovacuum"
        },
        {
            "location": "/autovacuum/#autovacuum",
            "text": "",
            "title": "Autovacuum"
        },
        {
            "location": "/autovacuum/#what-is-an-autovacuum",
            "text": "It's an automatic vacuum !  A  good reading  to understand what is autovacuum and why it exists.  PostgreSQL is a MVCC database : what does that mean ?   A good reading (quite complete !) could be found here :  http://momjian.us/main/writings/pgsql/mvcc.pdf .   BTW, you could have a look to all PDFs available in  Momjian  repo :  http://momjian.us/main/writings/pgsql/ .  The most important is to understand a MVCC database have a natural behavior to grow for ever if nothing is done.  Vacuum is a way :   to free / reuse the space (pages) used by transactions once they are committed.  to update tables statistics so the query planner could rely on good info ( ANALYZE  command, see  below  for details)  to maintain the  visibility map  of what pages contain only tuples visible to all transaction or future one (to help vacuum process itself or help index scans).     Autovacuum  is a PostgreSQL process to automate vacuum on databases.  To regain all possible space, we must execute a  VACUUM FULL  query, which needs an exclusive locks on the table : not easy in production environment !   Note  TRUNCATE  command doesn't require to regain free space, so no auto VACUUM is launched after it.     Documentation  Official link to  VACUUM command",
            "title": "What is an autovacuum ?"
        },
        {
            "location": "/autovacuum/#technical-precision",
            "text": "People often assume that VACUUM is the process that should return the disk space to the file system. It does do this but only in very specific cases.  That used space is contained in page files that make up the tables and indexes (called objects from now on) in the Postgres database system. \nPage files all have the same size and differently sized objects just have as many page files as they need. \nIf VACUUM happens to mark every row in a page file as unavailable AND that page also happens to be the final page for the entire object, THEN the disk space is returned to the file system.   If there is a single available row, or the page file is any other but the last one, the disk space is never returned by a normal VACUUM.  This is bloat.  See  below  for details.",
            "title": "Technical precision"
        },
        {
            "location": "/autovacuum/#what-triggers-autovacuum",
            "text": "A worker seeks for tables with :  vacuum threshold = autovacuum_vacuum_threshold + autovacuum_vacuum_scale_factor * number of tuples  But it keeps also a trace of I/O \"credits\" that are consumed. When credits used exceed the  autovacuum_vacuum_cost_limit  then autovacuum pauses all workers for  autovacuum_vacuum_cost_delay  milliseconds.  Default factor  autovacuum_vacuum_scale_factor = 0.2  is far too high for big tables. It couldn't help to control table bloating (see  below  for details).  .",
            "title": "What triggers Autovacuum ?"
        },
        {
            "location": "/autovacuum/#bloating",
            "text": "In MVCC database servers, tables updates, transactions uses spaces and need to be vacuumed to regain space. If it's not the case, table / index disk size possibly increase a lot : phenomenon named bloating.    It could be estimated by some queries : see details in the  statistics page .  Bad cases could occur with PostgreSQL catalog tables themselves bloated !  Bloating could be bad because when a query is executed on a table, all rows visibility flags are checked to see if the row is actually available for the transaction, including rows of the bloated part.   To understand why bloating is not always bad could be found  here .  A good example of investigation on table bloating cause :  https://www.cybertec-postgresql.com/en/stale-statistics-cause-table-bloat/ .",
            "title": "Bloating"
        },
        {
            "location": "/autovacuum/#index-bloating",
            "text": "Index bloating is even worse than table bloating because an index points to items, not tuples so it doesn't know if an item points to dead tuple or not. Thus, bloated indexes are slower.  To refresh an index :  REINDEX INDEX the_index;  To refresh all indexes of table :  REINDEX TABLE the_table;  To refresh all indexes of all system tables :   REINDEX SYSTEM \"the_database\";",
            "title": "Index bloating"
        },
        {
            "location": "/autovacuum/#time-series",
            "text": "Of course, for time series, standard autovacuum is an issue because table is never empty and space could never be regained. Some rows could be re-used but occupied space will grow forever, even slowly.  Other solutions are described in this post :  https://www.redpill-linpro.com/sysadvent/2017/12/08/pg_repack.html .",
            "title": "Time series"
        },
        {
            "location": "/autovacuum/#pg_repack",
            "text": "This project could be a good solution :  https://github.com/reorg/pg_repack/ .  Install on Debian  apt-get update && apt-get install -y postgresql-9.6-repack  Create an extension in the DB   psql -U my_user -h localhost -c \"CREATE EXTENSION pg_repack\" -d the_db  Launch pg_repack on a table   /usr/lib/postgresql/9.6/bin/pg_repack --user my_user --table=probes_global_statistics --jobs 5 the_db   Warning  Tables cleaned by pg_repack must have a primary key or not-null unique keys. Thus, netflow aggregation tables are not eligible for this : they don't have a primary key.",
            "title": "pg_repack"
        },
        {
            "location": "/autovacuum/#analyze",
            "text": "ANALYZE table [column (, ...)]  SQL command updates query planner statistic for this table. It's important to have up-to-date stats so the planner could compute efficient strategies.  It's not useful to frequently ANALYZE columns with a quite constant data distribution.   It's really important to frequently ANALYZE columns with increasing values, like timestamp for example.  Automatic analyze are triggered when the number of modified rows in a table exceeds   vacuum threshold = autovacuum_analyze_threshold + autovacuum_analyze_scale_factor * number of tuples   Documentation  Official link to  ANALYZE command",
            "title": "Analyze"
        },
        {
            "location": "/statistics/",
            "text": "Statistics\n\u00b6\n\n\nPostgreSQL provides a lot of statistics without any extension, and some more with some fine extensions.\n\n\nA very good survey of all available statistics and meaning can be found in \nthese slides\n.\n\n\nExtracted fom a slide, here is a schema with all these stats and the area they are related to.\n\n\n\n\nFor some general explanations on stats and the meaning of each field, you could read :\n\n\n\n\nthis french doc quite old but still useful\n\n\nthis doc from datadog, a monitoring tool\n\n\n\n\nCheckpoints\n\u00b6\n\n\nWhat is it ? Some \ndetails\n.\n\n\nIt's a good starting point to be sure checkpoints behave well.\n\n\nThe \npg_stat_bgwriter\n view contains a single row about that activity.\n\n\nthe_db=# select * from pg_stat_bgwriter;\n checkpoints_timed | checkpoints_req | checkpoint_write_time | checkpoint_sync_time | buffers_checkpoint | buffers_clean | maxwritten_clean | buffers_backend | buffers_backend_fsync | buffers_alloc |          stats_reset          \n-------------------+-----------------+-----------------------+----------------------+--------------------+---------------+------------------+-----------------+-----------------------+---------------+-------------------------------\n                84 |               1 |             405618914 |               403083 |            1570508 |     122328215 |          1021398 |       138562615 |                     0 |     380478655 | 2018-01-18 09:14:13.970255+01\n(1 row)\n\n\n\n\n\n\n\ncheckpoints_timed\n gives the number of checkpoints triggered by the tiemout (\nGOOD\n)\n\n\ncheckpoints_req\n gives the number of checkpoints triggered because the \npg_xlog\n folder had reached its limit \nmax_wal_size\n (\nBAD\n)\n\n\n\n\nThese specific stats could be resetted by :\n\n\nselect\n \npg_stat_reset_shared\n(\n'bgwriter'\n);\n\n\n\n\n\n\nAverage time\n\u00b6\n\n\nAnd the following query \n\n\nSELECT\n\n  \ntotal_checkpoints\n,\n\n  \nseconds_since_start\n \n/\n \ntotal_checkpoints\n \n/\n \n60\n \nAS\n \nminutes_between_checkpoints\n\n  \nFROM\n\n  \n(\nSELECT\n\n    \nEXTRACT\n(\nEPOCH\n \nFROM\n \n(\nnow\n()\n \n-\n \nstats_reset\n))\n \nAS\n \nseconds_since_start\n,\n\n    \n(\ncheckpoints_timed\n+\ncheckpoints_req\n)\n \nAS\n \ntotal_checkpoints\n\n    \nFROM\n \npg_stat_bgwriter\n\n  \n)\n \nAS\n \nsub\n;\n\n\n\n\n\n\ngives :\n\n\n\n\nthe total number of checkpoints that occured since last stat reset\n\n\nthe average time of a checkpoint - should be closed to the \ncheckpoint_timeout\n value \n\n\n\n\nWho does the writing ?\n\u00b6\n\n\nMore complex, the following query is given by (the great) \nGreg Smith\n :\n\n\nSELECT\n\n    \n(\n100\n \n*\n \ncheckpoints_req\n)\n \n/\n\n        \n(\ncheckpoints_timed\n \n+\n \ncheckpoints_req\n)\n \nAS\n \ncheckpoints_req_pct\n,\n\n    \npg_size_pretty\n(\nbuffers_checkpoint\n \n*\n \nblock_size\n \n/\n\n         \n(\ncheckpoints_timed\n \n+\n \ncheckpoints_req\n))\n \nAS\n \navg_checkpoint_write\n,\n\n    \npg_size_pretty\n(\nblock_size\n \n*\n\n        \n(\nbuffers_checkpoint\n \n+\n \nbuffers_clean\n \n+\n \nbuffers_backend\n))\n \nAS\n \ntotal_written\n,\n\n    \n100\n \n*\n \nbuffers_checkpoint\n \n/\n\n        \n(\nbuffers_checkpoint\n \n+\n \nbuffers_clean\n \n+\n \nbuffers_backend\n)\n \nAS\n \ncheckpoint_write_pct\n,\n\n    \n100\n \n*\n \nbuffers_backend\n \n/\n\n        \n(\nbuffers_checkpoint\n \n+\n \nbuffers_clean\n \n+\n \nbuffers_backend\n)\n \nAS\n \nbackend_write_pct\n,\n\n    \n100\n \n*\n \nbuffers_clean\n \n/\n\n        \n(\nbuffers_checkpoint\n \n+\n \nbuffers_clean\n \n+\n \nbuffers_backend\n)\n \nAS\n \nclean_write_pct\n,\n\n    \n*\n\n\nFROM\n \npg_stat_bgwriter\n,\n\n\n(\nSELECT\n \ncast\n(\ncurrent_setting\n(\n'block_size'\n)\n \nAS\n \ninteger\n)\n \nAS\n \nblock_size\n)\n \nbs\n;\n\n\n\n\n\n\nand provides :\n\n\n\n\n% of checkpoint triggered by size (almost \n100% is GOOD\n)\n\n\nHow much data the average checkpoint write ?\n\n\nTotal written by all processes\n\n\n% of buffers written by checkpoint process (spread in time : \nGOOD\n) \n\n\n% of buffers written by backend queries (read from disk because data not in cache / flushed from cache to disk then read again because data is dirty : \nBAD\n)\n\n\n% of buffers written by clean process (because need some space in cache : \nBAD\n) \n\n\n\n\nCheckpoints are the best way to persist data to disk\n because the I/O are spread on time. To be efficient, other processes (backend / clean) should use the cache most of the time, not the disk.\n\n\nIn addition, \npg_bg_writer\n table also gives \nmaxwritten_clean\n field that is the number of times the backgroung writer had to stop its writing (cleaning scan) because he has already written more buffers than specified in the \nbgwriter_lru_maxpages\n parameter.  \n\n\nAutovacuum\n\u00b6\n\n\nFirst of all, autovacuum activity could be traced with the same results in the \npg_stat_user_tables\n, \npg_stat_sys_tables\n and \npg_stat_all_tables\n views.\n\n\nFor each table created by the user, you could find :\n\n\nlast_vacuum      |\nlast_autovacuum  | 2008-04-15 11:56:11.622041+02\nlast_analyze     |\nlast_autoanalyze | 2008-04-15 11:56:11.622041+02\n\n\n\n\n\nfor both manual au auto vacuum processes.\n\n\nLogs are available through the conf parameter \nlog_autovacuum_min_duration\n to trace long vacuum queries.\n\n\nSome good metrics to monitor - they are expected to be stable in time :\n\n\n\n\npg_stat_all_tables.n_dead_tup\n \u2013 number of dead tuples in each table (both user tables and system catalogs)\n\n\n(n_dead_tup / n_live_tup)\n \u2013 ratio of dead/live tuples in each table\n\n\n(pg_class.relpages / pg_class.reltuples)\n \u2013 space \u201cper row\u201d\n\n\n\n\nNew view in 9.6 provides information about in-progress vacuums : \npg_stat_progress_vacuum\n.\n\n\nAn extension is available for more stats : \npgstattuple\n.\n\n\nTables & index size\n\u00b6\n\n\nTables and index sizes could be get by :\n\n\nSELECT\n \n    \ntable_name\n,\n \n    \npg_size_pretty\n(\npg_table_size\n(\ntable_name\n))\n \nAS\n \ntable_size\n,\n \n    \npg_size_pretty\n(\npg_indexes_size\n(\ntable_name\n))\n \nAS\n \nindex_size\n,\n\n    \npg_size_pretty\n(\npg_total_relation_size\n(\ntable_name\n))\n \nas\n \ntotal_size\n\n\nFROM\n \n\n(\n\n  \nSELECT\n \n(\n''\n \n||\n \ntable_schema\n \n||\n \n'.'\n \n||\n \ntable_name\n \n||\n \n''\n)\n \nAS\n \ntable_name\n \nFROM\n \ninformation_schema\n.\ntables\n \nWHERE\n \ntable_schema\n=\n'public'\n\n\n)\n \nAS\n \nall_tables1\n \nORDER\n \nBY\n \npg_total_relation_size\n(\ntable_name\n)\n \ndesc\n;\n\n\n\n\n\n\nwhich outputs :\n\n\n\n\n\n\n\n\ntable_name\n\n\ntable_size\n\n\nindex_size\n\n\ntotal_size\n\n\n\n\n\n\n\n\n\n\npublic.table1\n\n\n2102 MB\n\n\n738 MB\n\n\n2840 MB\n\n\n\n\n\n\npublic.table2\n\n\n77 MB\n\n\n130 MB\n\n\n207 MB\n\n\n\n\n\n\npublic.table3\n\n\n79 MB\n\n\n98 MB\n\n\n177 MB\n\n\n\n\n\n\n\n\nThe total for user tables is get by : \n\n\nSELECT\n \n    \npg_size_pretty\n(\nsum\n(\npg_table_size\n(\ntable_name\n)))\n \nAS\n \ntable_size\n,\n \n    \npg_size_pretty\n(\nsum\n(\npg_indexes_size\n(\ntable_name\n)))\n \nAS\n \nindex_size\n \n\nFROM\n \n\n(\n\n  \nSELECT\n \n(\n''\n \n||\n \ntable_schema\n \n||\n \n'.'\n \n||\n \ntable_name\n \n||\n \n''\n)\n \nAS\n \ntable_name\n \nFROM\n \ninformation_schema\n.\ntables\n \nWHERE\n \ntable_schema\n=\n'public'\n\n\n)\n \nAS\n \nall_tables1\n;\n\n\n\n\n\n\nwhich outputs : \n\n\n\n\n\n\n\n\ntable_size\n\n\nindex_size\n\n\n\n\n\n\n\n\n\n\n422 GB\n\n\n277 GB\n\n\n\n\n\n\n\n\nTable Bloating\n\u00b6\n\n\nBloating queries come from there : \nhttps://github.com/rach/pome/blob/develop/query.go\n.\n\n\nStats per table\n\u00b6\n\n\nEstimate system tables bloat size / ratio with this query :\n\n\nSELECT\n \nschemaname\n \nas\n \nschema\n,\n \ntblname\n \nas\n \ntable\n,\n\n  \nCASE\n \nWHEN\n \n(\ntblpages\n-\nest_tblpages_ff\n)\n*\nbs\n \n>\n \n0\n\n    \nTHEN\n \n((\ntblpages\n-\nest_tblpages_ff\n)\n*\nbs\n)::\nbigint\n\n    \nELSE\n \n0\n\n  \nEND\n \nAS\n \nbloat_bytes\n,\n\n  \nCASE\n \nWHEN\n \ntblpages\n \n-\n \nest_tblpages_ff\n \n>\n \n0\n\n    \nTHEN\n \nround\n((\n100\n \n*\n \n(\ntblpages\n \n-\n \nest_tblpages_ff\n)\n/\ntblpages\n::\nfloat\n)::\nnumeric\n,\n \n1\n)\n\n    \nELSE\n \n0\n\n  \nEND\n \nAS\n \nbloat_ratio\n\n\nFROM\n \n(\n\n  \nSELECT\n \nceil\n(\n \nreltuples\n \n/\n \n(\n \n(\nbs\n-\npage_hdr\n)\n/\ntpl_size\n \n)\n \n)\n \n+\n \nceil\n(\n \ntoasttuples\n \n/\n \n4\n \n)\n \nAS\n \nest_tblpages\n,\n\n    \nceil\n(\n \nreltuples\n \n/\n \n(\n \n(\nbs\n-\npage_hdr\n)\n*\nfillfactor\n/\n(\ntpl_size\n*\n100\n)\n \n)\n \n)\n \n+\n \nceil\n(\n \ntoasttuples\n \n/\n \n4\n \n)\n \nAS\n \nest_tblpages_ff\n,\n\n    \ntblpages\n,\n \nfillfactor\n,\n \nbs\n,\n \ntblid\n,\n \nschemaname\n,\n \ntblname\n,\n \nheappages\n,\n \ntoastpages\n,\n \nis_na\n\n  \nFROM\n \n(\n\n    \nSELECT\n\n      \n(\n \n4\n \n+\n \ntpl_hdr_size\n \n+\n \ntpl_data_size\n \n+\n \n(\n2\n*\nma\n)\n\n        \n-\n \nCASE\n \nWHEN\n \ntpl_hdr_size\n%\nma\n \n=\n \n0\n \nTHEN\n \nma\n \nELSE\n \ntpl_hdr_size\n%\nma\n \nEND\n\n        \n-\n \nCASE\n \nWHEN\n \nceil\n(\ntpl_data_size\n)::\nint\n%\nma\n \n=\n \n0\n \nTHEN\n \nma\n \nELSE\n \nceil\n(\ntpl_data_size\n)::\nint\n%\nma\n \nEND\n\n      \n)\n \nAS\n \ntpl_size\n,\n \nbs\n \n-\n \npage_hdr\n \nAS\n \nsize_per_block\n,\n \n(\nheappages\n \n+\n \ntoastpages\n)\n \nAS\n \ntblpages\n,\n \nheappages\n,\n\n      \ntoastpages\n,\n \nreltuples\n,\n \ntoasttuples\n,\n \nbs\n,\n \npage_hdr\n,\n \ntblid\n,\n \nschemaname\n,\n \ntblname\n,\n \nfillfactor\n,\n \nis_na\n\n    \nFROM\n \n(\n\n      \nSELECT\n\n        \ntbl\n.\noid\n \nAS\n \ntblid\n,\n \nns\n.\nnspname\n \nAS\n \nschemaname\n,\n \ntbl\n.\nrelname\n \nAS\n \ntblname\n,\n \ntbl\n.\nreltuples\n,\n\n        \ntbl\n.\nrelpages\n \nAS\n \nheappages\n,\n \ncoalesce\n(\ntoast\n.\nrelpages\n,\n \n0\n)\n \nAS\n \ntoastpages\n,\n\n        \ncoalesce\n(\ntoast\n.\nreltuples\n,\n \n0\n)\n \nAS\n \ntoasttuples\n,\n\n        \ncoalesce\n(\nsubstring\n(\n\n          \narray_to_string\n(\ntbl\n.\nreloptions\n,\n \n' '\n)\n\n          \nFROM\n \n'%fillfactor=#\"__#\"%'\n \nFOR\n \n'#'\n)::\nsmallint\n,\n \n100\n)\n \nAS\n \nfillfactor\n,\n\n        \ncurrent_setting\n(\n'block_size'\n)::\nnumeric\n \nAS\n \nbs\n,\n\n        \nCASE\n \nWHEN\n \nversion\n()\n~\n'mingw32'\n \nOR\n \nversion\n()\n~\n'64-bit|x86_64|ppc64|ia64|amd64'\n \nTHEN\n \n8\n \nELSE\n \n4\n \nEND\n \nAS\n \nma\n,\n\n        \n24\n \nAS\n \npage_hdr\n,\n\n        \n23\n \n+\n \nCASE\n \nWHEN\n \nMAX\n(\ncoalesce\n(\nnull_frac\n,\n0\n))\n \n>\n \n0\n \nTHEN\n \n(\n \n7\n \n+\n \ncount\n(\n*\n)\n \n)\n \n/\n \n8\n \nELSE\n \n0\n::\nint\n \nEND\n\n          \n+\n \nCASE\n \nWHEN\n \ntbl\n.\nrelhasoids\n \nTHEN\n \n4\n \nELSE\n \n0\n \nEND\n \nAS\n \ntpl_hdr_size\n,\n\n        \nsum\n(\n \n(\n1\n-\ncoalesce\n(\ns\n.\nnull_frac\n,\n \n0\n))\n \n*\n \ncoalesce\n(\ns\n.\navg_width\n,\n \n1024\n)\n \n)\n \nAS\n \ntpl_data_size\n,\n\n        \nbool_or\n(\natt\n.\natttypid\n \n=\n \n'pg_catalog.name'\n::\nregtype\n)\n \nAS\n \nis_na\n\n      \nFROM\n \npg_attribute\n \nAS\n \natt\n\n        \nJOIN\n \npg_class\n \nAS\n \ntbl\n \nON\n \natt\n.\nattrelid\n \n=\n \ntbl\n.\noid\n\n        \nJOIN\n \npg_namespace\n \nAS\n \nns\n \nON\n \nns\n.\noid\n \n=\n \ntbl\n.\nrelnamespace\n\n        \nJOIN\n \npg_stats\n \nAS\n \ns\n \nON\n \ns\n.\nschemaname\n=\nns\n.\nnspname\n\n          \nAND\n \ns\n.\ntablename\n \n=\n \ntbl\n.\nrelname\n \nAND\n \ns\n.\ninherited\n=\nfalse\n \nAND\n \ns\n.\nattname\n=\natt\n.\nattname\n\n        \nLEFT\n \nJOIN\n \npg_class\n \nAS\n \ntoast\n \nON\n \ntbl\n.\nreltoastrelid\n \n=\n \ntoast\n.\noid\n\n      \nWHERE\n \natt\n.\nattnum\n \n>\n \n0\n \nAND\n \nNOT\n \natt\n.\nattisdropped\n\n        \nAND\n \ntbl\n.\nrelkind\n \n=\n \n'r'\n \nAND\n \nns\n.\nnspname\n \n=\n \n'pg_catalog'\n\n      \nGROUP\n \nBY\n \n1\n,\n2\n,\n3\n,\n4\n,\n5\n,\n6\n,\n7\n,\n8\n,\n9\n,\n10\n,\n \ntbl\n.\nrelhasoids\n\n      \nORDER\n \nBY\n \n2\n,\n3\n\n    \n)\n \nAS\n \ns\n\n  \n)\n \nAS\n \ns2\n\n\n)\n \nAS\n \ns3\n \norder\n \nby\n \nbloat_ratio\n \ndesc\n;\n\n\n\n\n\n\nFor user tables, set the schemaname value with \nAND ns.nspname='public'\n and possibly add a relname filter with \nAND tbl.relname like '%Myfilter%'\n .\n\n\nExample of output indicating some bad bloating ratio :\n\n\n   schema   |       table        | bloat_bytes | bloat_ratio \n------------+--------------------+-------------+-------------\n pg_catalog | pg_class           |    39116800 |        68.3\n pg_catalog | pg_index           |    17539072 |        61.8\n pg_catalog | pg_depend          |    15507456 |        57.2\n pg_catalog | pg_type            |    15269888 |        56.1\n pg_catalog | pg_attribute       |    99336192 |        55.6\n pg_catalog | pg_constraint      |      172032 |        36.2\n pg_catalog | pg_shdepend        |      598016 |        14.5\n pg_catalog | pg_description     |       16384 |         5.9\n\n\n\n\n\nTo illustrate Vacuum effect on table bloating : \n\n\n# Gather bloating stats before vacuum\n   schema   |       table        | bloat_bytes | bloat_ratio \n------------+--------------------+-------------+-------------\n public     | my_bloated_table   |   128622592 | 68.698315467075\n\n# Then vacuum\nvacuum (full, analyze) my_bloated_table;\n\n# Finally refresh stats\n   schema   |       table        | bloat_bytes | bloat_ratio \n------------+--------------------+-------------+-------------\n public     | my_bloated_table   |   6520832   | 9.74176967323461\n\n\n\n\n\nIt's much better !\n\n\nStats total\n\u00b6\n\n\nTo get total bloating of all databases of a server, execute :\n\n\nSELECT\n \npg_size_pretty\n(\nSUM\n(\nreal_size\n)::\nbigint\n)\n \nas\n \nreal_size\n,\n\n       \npg_size_pretty\n(\nSUM\n(\nbloat_bytes\n)::\nbigint\n)\n \nas\n \nbloat_size\n \nFROM\n \n\n(\n\n  \nSELECT\n \n    \nbs\n*\ntblpages\n \nAS\n \nreal_size\n,\n\n    \nCASE\n \nWHEN\n \n(\ntblpages\n-\nest_tblpages_ff\n)\n*\nbs\n \n>\n \n0\n\n      \nTHEN\n \n((\ntblpages\n-\nest_tblpages_ff\n)\n*\nbs\n)::\nbigint\n\n      \nELSE\n \n0\n\n    \nEND\n \nAS\n \nbloat_bytes\n\n  \nFROM\n \n(\n\n    \nSELECT\n \nceil\n(\n \nreltuples\n \n/\n \n(\n \n(\nbs\n-\npage_hdr\n)\n/\ntpl_size\n \n)\n \n)\n \n+\n \nceil\n(\n \ntoasttuples\n \n/\n \n4\n \n)\n \nAS\n \nest_tblpages\n,\n\n      \nceil\n(\n \nreltuples\n \n/\n \n(\n \n(\nbs\n-\npage_hdr\n)\n*\nfillfactor\n/\n(\ntpl_size\n*\n100\n)\n \n)\n \n)\n \n+\n \nceil\n(\n \ntoasttuples\n \n/\n \n4\n \n)\n \nAS\n \nest_tblpages_ff\n,\n\n      \ntblpages\n,\n \nfillfactor\n,\n \nbs\n,\n \ntblid\n,\n \nschemaname\n,\n \ntblname\n,\n \nheappages\n,\n \ntoastpages\n,\n \nis_na\n\n    \nFROM\n \n(\n\n      \nSELECT\n\n        \n(\n \n4\n \n+\n \ntpl_hdr_size\n \n+\n \ntpl_data_size\n \n+\n \n(\n2\n*\nma\n)\n\n          \n-\n \nCASE\n \nWHEN\n \ntpl_hdr_size\n%\nma\n \n=\n \n0\n \nTHEN\n \nma\n \nELSE\n \ntpl_hdr_size\n%\nma\n \nEND\n\n          \n-\n \nCASE\n \nWHEN\n \nceil\n(\ntpl_data_size\n)::\nint\n%\nma\n \n=\n \n0\n \nTHEN\n \nma\n \nELSE\n \nceil\n(\ntpl_data_size\n)::\nint\n%\nma\n \nEND\n\n        \n)\n \nAS\n \ntpl_size\n,\n \nbs\n \n-\n \npage_hdr\n \nAS\n \nsize_per_block\n,\n \n(\nheappages\n \n+\n \ntoastpages\n)\n \nAS\n \ntblpages\n,\n \nheappages\n,\n\n        \ntoastpages\n,\n \nreltuples\n,\n \ntoasttuples\n,\n \nbs\n,\n \npage_hdr\n,\n \ntblid\n,\n \nschemaname\n,\n \ntblname\n,\n \nfillfactor\n,\n \nis_na\n\n      \nFROM\n \n(\n\n        \nSELECT\n\n          \ntbl\n.\noid\n \nAS\n \ntblid\n,\n \nns\n.\nnspname\n \nAS\n \nschemaname\n,\n \ntbl\n.\nrelname\n \nAS\n \ntblname\n,\n \ntbl\n.\nreltuples\n,\n\n          \ntbl\n.\nrelpages\n \nAS\n \nheappages\n,\n \ncoalesce\n(\ntoast\n.\nrelpages\n,\n \n0\n)\n \nAS\n \ntoastpages\n,\n\n          \ncoalesce\n(\ntoast\n.\nreltuples\n,\n \n0\n)\n \nAS\n \ntoasttuples\n,\n\n          \ncoalesce\n(\nsubstring\n(\n\n            \narray_to_string\n(\ntbl\n.\nreloptions\n,\n \n' '\n)\n\n            \nFROM\n \n'%fillfactor=#\"__#\"%'\n \nFOR\n \n'#'\n)::\nsmallint\n,\n \n100\n)\n \nAS\n \nfillfactor\n,\n\n          \ncurrent_setting\n(\n'block_size'\n)::\nnumeric\n \nAS\n \nbs\n,\n\n          \nCASE\n \nWHEN\n \nversion\n()\n~\n'mingw32'\n \nOR\n \nversion\n()\n~\n'64-bit|x86_64|ppc64|ia64|amd64'\n \nTHEN\n \n8\n \nELSE\n \n4\n \nEND\n \nAS\n \nma\n,\n\n          \n24\n \nAS\n \npage_hdr\n,\n\n          \n23\n \n+\n \nCASE\n \nWHEN\n \nMAX\n(\ncoalesce\n(\nnull_frac\n,\n0\n))\n \n>\n \n0\n \nTHEN\n \n(\n \n7\n \n+\n \ncount\n(\n*\n)\n \n)\n \n/\n \n8\n \nELSE\n \n0\n::\nint\n \nEND\n\n            \n+\n \nCASE\n \nWHEN\n \ntbl\n.\nrelhasoids\n \nTHEN\n \n4\n \nELSE\n \n0\n \nEND\n \nAS\n \ntpl_hdr_size\n,\n\n          \nsum\n(\n \n(\n1\n-\ncoalesce\n(\ns\n.\nnull_frac\n,\n \n0\n))\n \n*\n \ncoalesce\n(\ns\n.\navg_width\n,\n \n1024\n)\n \n)\n \nAS\n \ntpl_data_size\n,\n\n          \nbool_or\n(\natt\n.\natttypid\n \n=\n \n'pg_catalog.name'\n::\nregtype\n)\n \nAS\n \nis_na\n\n        \nFROM\n \npg_attribute\n \nAS\n \natt\n\n          \nJOIN\n \npg_class\n \nAS\n \ntbl\n \nON\n \natt\n.\nattrelid\n \n=\n \ntbl\n.\noid\n\n          \nJOIN\n \npg_namespace\n \nAS\n \nns\n \nON\n \nns\n.\noid\n \n=\n \ntbl\n.\nrelnamespace\n\n          \nJOIN\n \npg_stats\n \nAS\n \ns\n \nON\n \ns\n.\nschemaname\n=\nns\n.\nnspname\n\n            \nAND\n \ns\n.\ntablename\n \n=\n \ntbl\n.\nrelname\n \nAND\n \ns\n.\ninherited\n=\nfalse\n \nAND\n \ns\n.\nattname\n=\natt\n.\nattname\n\n          \nLEFT\n \nJOIN\n \npg_class\n \nAS\n \ntoast\n \nON\n \ntbl\n.\nreltoastrelid\n \n=\n \ntoast\n.\noid\n\n        \nWHERE\n \natt\n.\nattnum\n \n>\n \n0\n \nAND\n \nNOT\n \natt\n.\nattisdropped\n\n          \nAND\n \ntbl\n.\nrelkind\n \n=\n \n'r'\n \nAND\n \nns\n.\nnspname\n \n=\n \n'pg_catalog'\n\n        \nGROUP\n \nBY\n \n1\n,\n2\n,\n3\n,\n4\n,\n5\n,\n6\n,\n7\n,\n8\n,\n9\n,\n10\n,\n \ntbl\n.\nrelhasoids\n\n        \nORDER\n \nBY\n \n2\n,\n3\n\n      \n)\n \nAS\n \ns\n\n    \n)\n \nAS\n \ns2\n\n  \n)\n \nAS\n \ns3\n\n\n)\n \nas\n \ns4\n;\n\n\n\n\n\n\nIt outputs for example : \n\n\n  real_size | bloat_size \n -----------+-------------\n  310 MB    | 179 MB\n\n\n\n\n\nIndex bloating\n\u00b6\n\n\nBloating queries come from there : \nhttps://github.com/rach/pome/blob/develop/query.go\n.\n\n\nStats per index\n\u00b6\n\n\nEstimate system index bloat size / ratio with this query :\n\n\nWITH\n \nbtree_index_atts\n \nAS\n \n(\n\n    \nSELECT\n \nnspname\n,\n \nrelname\n,\n \nreltuples\n,\n \nrelpages\n,\n \nindrelid\n,\n \nrelam\n,\n\n        \nregexp_split_to_table\n(\nindkey\n::\ntext\n,\n \n' '\n)::\nsmallint\n \nAS\n \nattnum\n,\n\n        \nindexrelid\n \nas\n \nindex_oid\n\n    \nFROM\n \npg_index\n\n    \nJOIN\n \npg_class\n \nON\n \npg_class\n.\noid\n=\npg_index\n.\nindexrelid\n\n    \nJOIN\n \npg_namespace\n \nON\n \npg_namespace\n.\noid\n \n=\n \npg_class\n.\nrelnamespace\n\n    \nJOIN\n \npg_am\n \nON\n \npg_class\n.\nrelam\n \n=\n \npg_am\n.\noid\n\n    \nWHERE\n \npg_am\n.\namname\n \n=\n \n'btree'\n \nAND\n \npg_namespace\n.\nnspname\n \n=\n \n'pg_catalog'\n\n    \n),\n\n\nindex_item_sizes\n \nAS\n \n(\n\n    \nSELECT\n\n    \ni\n.\nnspname\n,\n \ni\n.\nrelname\n,\n \ni\n.\nreltuples\n,\n \ni\n.\nrelpages\n,\n \ni\n.\nrelam\n,\n\n    \n(\nquote_ident\n(\ns\n.\nschemaname\n)\n \n||\n \n'.'\n \n||\n \nquote_ident\n(\ns\n.\ntablename\n))::\nregclass\n \nAS\n \nstarelid\n,\n \na\n.\nattrelid\n \nAS\n \ntable_oid\n,\n \nindex_oid\n,\n\n    \ncurrent_setting\n(\n'block_size'\n)::\nnumeric\n \nAS\n \nbs\n,\n\n    \n/* MAXALIGN: 4 on 32bits, 8 on 64bits (and mingw32 ?) */\n\n    \nCASE\n\n        \nWHEN\n \nversion\n()\n \n~\n \n'mingw32'\n \nOR\n \nversion\n()\n \n~\n \n'64-bit'\n \nTHEN\n \n8\n\n        \nELSE\n \n4\n\n    \nEND\n \nAS\n \nmaxalign\n,\n\n    \n24\n \nAS\n \npagehdr\n,\n\n    \n/* per tuple header: add index_attribute_bm if some cols are null-able */\n\n    \nCASE\n \nWHEN\n \nmax\n(\ncoalesce\n(\ns\n.\nnull_frac\n,\n0\n))\n \n=\n \n0\n\n        \nTHEN\n \n2\n\n        \nELSE\n \n6\n\n    \nEND\n \nAS\n \nindex_tuple_hdr\n,\n\n    \n/* data len: we remove null values save space using it fractionnal part from stats */\n\n    \nsum\n(\n \n(\n1\n-\ncoalesce\n(\ns\n.\nnull_frac\n,\n \n0\n))\n \n*\n \ncoalesce\n(\ns\n.\navg_width\n,\n \n2048\n)\n \n)\n \nAS\n \nnulldatawidth\n\n    \nFROM\n \npg_attribute\n \nAS\n \na\n\n    \nJOIN\n \npg_stats\n \nAS\n \ns\n \nON\n \n(\nquote_ident\n(\ns\n.\nschemaname\n)\n \n||\n \n'.'\n \n||\n \nquote_ident\n(\ns\n.\ntablename\n))::\nregclass\n=\na\n.\nattrelid\n \nAND\n \ns\n.\nattname\n \n=\n \na\n.\nattname\n \n    \nJOIN\n \nbtree_index_atts\n \nAS\n \ni\n \nON\n \ni\n.\nindrelid\n \n=\n \na\n.\nattrelid\n \nAND\n \na\n.\nattnum\n \n=\n \ni\n.\nattnum\n\n    \nWHERE\n \na\n.\nattnum\n \n>\n \n0\n\n    \nGROUP\n \nBY\n \n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n,\n \n6\n,\n \n7\n,\n \n8\n,\n \n9\n\n\n),\n\n\nindex_aligned\n \nAS\n \n(\n\n    \nSELECT\n \nmaxalign\n,\n \nbs\n,\n \nnspname\n,\n \nrelname\n \nAS\n \nindex_name\n,\n \nreltuples\n,\n\n        \nrelpages\n,\n \nrelam\n,\n \ntable_oid\n,\n \nindex_oid\n,\n\n      \n(\n \n2\n \n+\n\n          \nmaxalign\n \n-\n \nCASE\n \n/* Add padding to the index tuple header to align on MAXALIGN */\n\n            \nWHEN\n \nindex_tuple_hdr\n%\nmaxalign\n \n=\n \n0\n \nTHEN\n \nmaxalign\n\n            \nELSE\n \nindex_tuple_hdr\n%\nmaxalign\n\n          \nEND\n\n        \n+\n \nnulldatawidth\n \n+\n \nmaxalign\n \n-\n \nCASE\n \n/* Add padding to the data to align on MAXALIGN */\n\n            \nWHEN\n \nnulldatawidth\n::\ninteger\n%\nmaxalign\n \n=\n \n0\n \nTHEN\n \nmaxalign\n\n            \nELSE\n \nnulldatawidth\n::\ninteger\n%\nmaxalign\n\n          \nEND\n\n      \n)::\nnumeric\n \nAS\n \nnulldatahdrwidth\n,\n \npagehdr\n\n    \nFROM\n \nindex_item_sizes\n \nAS\n \ns1\n\n\n),\n\n\notta_calc\n \nAS\n \n(\n\n  \nSELECT\n \nbs\n,\n \nnspname\n,\n \ntable_oid\n,\n \nindex_oid\n,\n \nindex_name\n,\n \nrelpages\n,\n \ncoalesce\n(\n\n    \nceil\n((\nreltuples\n*\n(\n4\n+\nnulldatahdrwidth\n))\n/\n(\nbs\n-\npagehdr\n::\nfloat\n))\n \n+\n\n      \nCASE\n \nWHEN\n \nam\n.\namname\n \nIN\n \n(\n'hash'\n,\n'btree'\n)\n \nTHEN\n \n1\n \nELSE\n \n0\n \nEND\n \n,\n \n0\n \n-- btree and hash have a metadata reserved block\n\n    \n)\n \nAS\n \notta\n\n  \nFROM\n \nindex_aligned\n \nAS\n \ns2\n\n    \nLEFT\n \nJOIN\n \npg_am\n \nam\n \nON\n \ns2\n.\nrelam\n \n=\n \nam\n.\noid\n\n\n),\n\n\nraw_bloat\n \nAS\n \n(\n\n    \nSELECT\n \ncurrent_database\n()\n \nas\n \ndbname\n,\n \nnspname\n,\n \nc\n.\nrelname\n \nAS\n \ntablename\n,\n \nindex_name\n,\n\n        \nbs\n*\n(\nsub\n.\nrelpages\n)::\nbigint\n \nAS\n \ntotalbytes\n,\n\n        \nCASE\n\n            \nWHEN\n \nsub\n.\nrelpages\n \n<=\n \notta\n \nTHEN\n \n0\n\n            \nELSE\n \nbs\n*\n(\nsub\n.\nrelpages\n-\notta\n)::\nbigint\n \nEND\n\n            \nAS\n \nwastedbytes\n,\n\n        \nCASE\n\n            \nWHEN\n \nsub\n.\nrelpages\n \n<=\n \notta\n\n            \nTHEN\n \n0\n \nELSE\n \nbs\n*\n(\nsub\n.\nrelpages\n-\notta\n)::\nbigint\n \n*\n \n100\n \n/\n \n(\nbs\n*\n(\nsub\n.\nrelpages\n)::\nbigint\n)\n \nEND\n\n            \nAS\n \nrealbloat\n,\n\n        \npg_relation_size\n(\nsub\n.\ntable_oid\n)\n \nas\n \ntable_bytes\n\n    \nFROM\n \notta_calc\n \nAS\n \nsub\n\n    \nJOIN\n \npg_class\n \nAS\n \nc\n \nON\n \nc\n.\noid\n=\nsub\n.\ntable_oid\n\n\n)\n\n\nSELECT\n  \nnspname\n \nAS\n \nschema\n,\n\n        \ntablename\n \nas\n \ntable\n,\n\n        \nindex_name\n \nAS\n \nindex\n,\n\n        \nwastedbytes\n \nas\n \nbloat_bytes\n,\n\n        \nround\n(\nrealbloat\n,\n \n1\n)\n \nas\n \nbloat_ratio\n,\n \n        \ntotalbytes\n \nas\n \nindex_size\n\n\nFROM\n \nraw_bloat\n\n\nORDER\n \nBY\n \nwastedbytes\n \nDESC\n;\n\n\n\n\n\n\nFor user tables, set the schemaname value with \nAND pg_namespace.nspname='public'\n and possibly add a relname filter with \nWHERE tablename='probes_global_statistics'\n before the last \nORDER BY\n.\n\n\nStats total\n\u00b6\n\n\nTo get total bloating of all databases of a server, execute :\n\n\nSELECT\n \npg_size_pretty\n(\nSUM\n(\nindex_size\n)::\nbigint\n)\n \nas\n \nindex_size\n,\n\n       \npg_size_pretty\n(\nSUM\n(\nbloat_bytes\n)::\nbigint\n)\n \nas\n \nbloat_size\n\n\nFROM\n \n(\n\n\n\nWITH\n \nbtree_index_atts\n \nAS\n \n(\n\n    \nSELECT\n \nnspname\n,\n \nrelname\n,\n \nreltuples\n,\n \nrelpages\n,\n \nindrelid\n,\n \nrelam\n,\n\n        \nregexp_split_to_table\n(\nindkey\n::\ntext\n,\n \n' '\n)::\nsmallint\n \nAS\n \nattnum\n,\n\n        \nindexrelid\n \nas\n \nindex_oid\n\n    \nFROM\n \npg_index\n\n    \nJOIN\n \npg_class\n \nON\n \npg_class\n.\noid\n=\npg_index\n.\nindexrelid\n\n    \nJOIN\n \npg_namespace\n \nON\n \npg_namespace\n.\noid\n \n=\n \npg_class\n.\nrelnamespace\n\n    \nJOIN\n \npg_am\n \nON\n \npg_class\n.\nrelam\n \n=\n \npg_am\n.\noid\n\n    \nWHERE\n \npg_am\n.\namname\n \n=\n \n'btree'\n \nAND\n \npg_namespace\n.\nnspname\n \n=\n \n'pg_catalog'\n\n    \n),\n\n\nindex_item_sizes\n \nAS\n \n(\n\n    \nSELECT\n\n    \ni\n.\nnspname\n,\n \ni\n.\nrelname\n,\n \ni\n.\nreltuples\n,\n \ni\n.\nrelpages\n,\n \ni\n.\nrelam\n,\n\n    \n(\nquote_ident\n(\ns\n.\nschemaname\n)\n \n||\n \n'.'\n \n||\n \nquote_ident\n(\ns\n.\ntablename\n))::\nregclass\n \nAS\n \nstarelid\n,\n \na\n.\nattrelid\n \nAS\n \ntable_oid\n,\n \nindex_oid\n,\n\n    \ncurrent_setting\n(\n'block_size'\n)::\nnumeric\n \nAS\n \nbs\n,\n\n    \n/* MAXALIGN: 4 on 32bits, 8 on 64bits (and mingw32 ?) */\n\n    \nCASE\n\n        \nWHEN\n \nversion\n()\n \n~\n \n'mingw32'\n \nOR\n \nversion\n()\n \n~\n \n'64-bit'\n \nTHEN\n \n8\n\n        \nELSE\n \n4\n\n    \nEND\n \nAS\n \nmaxalign\n,\n\n    \n24\n \nAS\n \npagehdr\n,\n\n    \n/* per tuple header: add index_attribute_bm if some cols are null-able */\n\n    \nCASE\n \nWHEN\n \nmax\n(\ncoalesce\n(\ns\n.\nnull_frac\n,\n0\n))\n \n=\n \n0\n\n        \nTHEN\n \n2\n\n        \nELSE\n \n6\n\n    \nEND\n \nAS\n \nindex_tuple_hdr\n,\n\n    \n/* data len: we remove null values save space using it fractionnal part from stats */\n\n    \nsum\n(\n \n(\n1\n-\ncoalesce\n(\ns\n.\nnull_frac\n,\n \n0\n))\n \n*\n \ncoalesce\n(\ns\n.\navg_width\n,\n \n2048\n)\n \n)\n \nAS\n \nnulldatawidth\n\n    \nFROM\n \npg_attribute\n \nAS\n \na\n\n    \nJOIN\n \npg_stats\n \nAS\n \ns\n \nON\n \n(\nquote_ident\n(\ns\n.\nschemaname\n)\n \n||\n \n'.'\n \n||\n \nquote_ident\n(\ns\n.\ntablename\n))::\nregclass\n=\na\n.\nattrelid\n \nAND\n \ns\n.\nattname\n \n=\n \na\n.\nattname\n \n    \nJOIN\n \nbtree_index_atts\n \nAS\n \ni\n \nON\n \ni\n.\nindrelid\n \n=\n \na\n.\nattrelid\n \nAND\n \na\n.\nattnum\n \n=\n \ni\n.\nattnum\n\n    \nWHERE\n \na\n.\nattnum\n \n>\n \n0\n\n    \nGROUP\n \nBY\n \n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n,\n \n6\n,\n \n7\n,\n \n8\n,\n \n9\n\n\n),\n\n\nindex_aligned\n \nAS\n \n(\n\n    \nSELECT\n \nmaxalign\n,\n \nbs\n,\n \nnspname\n,\n \nrelname\n \nAS\n \nindex_name\n,\n \nreltuples\n,\n\n        \nrelpages\n,\n \nrelam\n,\n \ntable_oid\n,\n \nindex_oid\n,\n\n      \n(\n \n2\n \n+\n\n          \nmaxalign\n \n-\n \nCASE\n \n/* Add padding to the index tuple header to align on MAXALIGN */\n\n            \nWHEN\n \nindex_tuple_hdr\n%\nmaxalign\n \n=\n \n0\n \nTHEN\n \nmaxalign\n\n            \nELSE\n \nindex_tuple_hdr\n%\nmaxalign\n\n          \nEND\n\n        \n+\n \nnulldatawidth\n \n+\n \nmaxalign\n \n-\n \nCASE\n \n/* Add padding to the data to align on MAXALIGN */\n\n            \nWHEN\n \nnulldatawidth\n::\ninteger\n%\nmaxalign\n \n=\n \n0\n \nTHEN\n \nmaxalign\n\n            \nELSE\n \nnulldatawidth\n::\ninteger\n%\nmaxalign\n\n          \nEND\n\n      \n)::\nnumeric\n \nAS\n \nnulldatahdrwidth\n,\n \npagehdr\n\n    \nFROM\n \nindex_item_sizes\n \nAS\n \ns1\n\n\n),\n\n\notta_calc\n \nAS\n \n(\n\n  \nSELECT\n \nbs\n,\n \nnspname\n,\n \ntable_oid\n,\n \nindex_oid\n,\n \nindex_name\n,\n \nrelpages\n,\n \ncoalesce\n(\n\n    \nceil\n((\nreltuples\n*\n(\n4\n+\nnulldatahdrwidth\n))\n/\n(\nbs\n-\npagehdr\n::\nfloat\n))\n \n+\n\n      \nCASE\n \nWHEN\n \nam\n.\namname\n \nIN\n \n(\n'hash'\n,\n'btree'\n)\n \nTHEN\n \n1\n \nELSE\n \n0\n \nEND\n \n,\n \n0\n \n-- btree and hash have a metadata reserved block\n\n    \n)\n \nAS\n \notta\n\n  \nFROM\n \nindex_aligned\n \nAS\n \ns2\n\n    \nLEFT\n \nJOIN\n \npg_am\n \nam\n \nON\n \ns2\n.\nrelam\n \n=\n \nam\n.\noid\n\n\n),\n\n\nraw_bloat\n \nAS\n \n(\n\n    \nSELECT\n \ncurrent_database\n()\n \nas\n \ndbname\n,\n \nnspname\n,\n \nc\n.\nrelname\n \nAS\n \ntablename\n,\n \nindex_name\n,\n\n        \nbs\n*\n(\nsub\n.\nrelpages\n)::\nbigint\n \nAS\n \ntotalbytes\n,\n\n        \nCASE\n\n            \nWHEN\n \nsub\n.\nrelpages\n \n<=\n \notta\n \nTHEN\n \n0\n\n            \nELSE\n \nbs\n*\n(\nsub\n.\nrelpages\n-\notta\n)::\nbigint\n \nEND\n\n            \nAS\n \nwastedbytes\n,\n\n        \nCASE\n\n            \nWHEN\n \nsub\n.\nrelpages\n \n<=\n \notta\n\n            \nTHEN\n \n0\n \nELSE\n \nbs\n*\n(\nsub\n.\nrelpages\n-\notta\n)::\nbigint\n \n*\n \n100\n \n/\n \n(\nbs\n*\n(\nsub\n.\nrelpages\n)::\nbigint\n)\n \nEND\n\n            \nAS\n \nrealbloat\n,\n\n        \npg_relation_size\n(\nsub\n.\ntable_oid\n)\n \nas\n \ntable_bytes\n\n    \nFROM\n \notta_calc\n \nAS\n \nsub\n\n    \nJOIN\n \npg_class\n \nAS\n \nc\n \nON\n \nc\n.\noid\n=\nsub\n.\ntable_oid\n\n\n)\n\n\nSELECT\n  \nwastedbytes\n \nas\n \nbloat_bytes\n,\n      \n        \ntotalbytes\n \nas\n \nindex_size\n\n\nFROM\n \nraw_bloat\n\n\n)\n \nAS\n \ns4\n;\n\n\n\n\n\n\nIt outputs for example : \n\n\n index_size | bloat_size \n------------+------------\n 283 MB     | 199 MB\n\n\n\n\n\nCache usage\n\u00b6\n\n\nAmount of data read from disk and total for all user tables of a database and their indexes.\n\n\nWITH\n \n\nstats_heap\n \nAS\n\n\n(\n\n    \nSELECT\n\n    \n(\nSELECT\n \nsum\n(\nheap_blks_read\n)\n \nFROM\n \npg_statio_user_tables\n)::\nbigint\n \nas\n \nheap_read\n,\n \n    \n(\nSELECT\n \nsum\n(\nheap_blks_hit\n)\n \nFROM\n \npg_statio_user_tables\n)::\nbigint\n \nas\n \nheap_hit\n\n\n),\n\n\nstats_idx\n \nAS\n\n\n(\n\n    \nSELECT\n\n    \n(\nSELECT\n \nsum\n(\nidx_blks_read\n)\n \nFROM\n \npg_statio_user_indexes\n)::\nbigint\n \nas\n \nidx_read\n,\n \n    \n(\nSELECT\n \nsum\n(\nidx_blks_hit\n)\n \nFROM\n \npg_statio_user_indexes\n)::\nbigint\n \nas\n \nidx_hit\n\n\n),\n\n\nbs\n \nAS\n\n\n(\n\n    \nSELECT\n \n    \ncast\n(\ncurrent_setting\n(\n'block_size'\n)\n \nAS\n \ninteger\n)\n \nAS\n \nblock_size\n\n\n)\n\n\nSELECT\n \ncurrent_database\n()\n \nAS\n \ndbname\n,\n \n(\nheap_read\n \n*\n \nblock_size\n)::\nbigint\n \nas\n \nheap_read\n,\n \n(\nheap_hit\n \n*\n \nblock_size\n)::\nbigint\n \nas\n \nheap_hit\n,\n \n((\nheap_hit\n::\nfloat\n \n-\n \nheap_read\n::\nfloat\n)\n \n/\n \nheap_hit\n::\nfloat\n)::\nfloat\n \nas\n \nheap_ratio\n,\n\n\n(\nidx_read\n \n*\n \nblock_size\n)::\nbigint\n \nas\n \nidx_read\n,\n \n(\nidx_hit\n \n*\n \nblock_size\n)::\nbigint\n \nas\n \nidx_hit\n,\n \n((\nidx_hit\n::\nfloat\n \n-\n \nidx_read\n::\nfloat\n)\n \n/\n \nidx_hit\n::\nfloat\n)::\nfloat\n \nas\n \nidx_ratio\n \nFROM\n \nbs\n,\n \nstats_heap\n,\n \nstats_idx\n;\n\n\n\n\n\n\nThis query could work for system tables by using the \npg_statio_sys_tables\n table.\n\n\nOf course, it could be also written to analyze cache usage for a single table :\n\n\nSET\n \nyou\n.\ntable_name\n \n=\n \n'report'\n;\n\n\nWITH\n \n\nstats_heap\n \nAS\n\n\n(\n\n    \nSELECT\n\n    \n(\nSELECT\n \nheap_blks_read\n \nFROM\n \npg_statio_user_tables\n \nwhere\n \nrelname\n=\ncurrent_setting\n(\n'you.table_name'\n))::\nbigint\n \nas\n \nheap_read\n,\n \n    \n(\nSELECT\n \nheap_blks_hit\n \nFROM\n \npg_statio_user_tables\n \nwhere\n \nrelname\n=\ncurrent_setting\n(\n'you.table_name'\n))::\nbigint\n \nas\n \nheap_hit\n\n\n),\n\n\nstats_idx\n \nAS\n\n\n(\n\n    \nSELECT\n\n    \n(\nSELECT\n \nsum\n(\nidx_blks_read\n)\n \nFROM\n \npg_statio_user_indexes\n \nwhere\n \nrelname\n=\ncurrent_setting\n(\n'you.table_name'\n))::\nbigint\n \nas\n \nidx_read\n,\n \n    \n(\nSELECT\n \nsum\n(\nidx_blks_hit\n)\n \nFROM\n \npg_statio_user_indexes\n \nwhere\n \nrelname\n=\ncurrent_setting\n(\n'you.table_name'\n))::\nbigint\n \nas\n \nidx_hit\n\n\n),\n\n\nbs\n \nAS\n\n\n(\n\n    \nSELECT\n \n    \ncast\n(\ncurrent_setting\n(\n'block_size'\n)\n \nAS\n \ninteger\n)\n \nAS\n \nblock_size\n\n\n)\n\n\nSELECT\n \n    \ncurrent_database\n()\n \nAS\n \ndbname\n,\n \n    \n(\nheap_read\n \n*\n \nblock_size\n)::\nbigint\n \nas\n \nheap_read\n,\n \n    \n(\nheap_hit\n \n*\n \nblock_size\n)::\nbigint\n \nas\n \nheap_hit\n,\n \n    \nCASE\n \nWHEN\n \nheap_hit\n \n!=\n \n0\n\n        \nTHEN\n \n((\nheap_hit\n::\nfloat\n \n-\n \nheap_read\n::\nfloat\n)\n \n/\n \nheap_hit\n::\nfloat\n)::\nfloat\n\n        \nELSE\n \n0\n\n    \nEND\n \nas\n \nheap_ratio\n,\n\n    \n(\nidx_read\n \n*\n \nblock_size\n)::\nbigint\n \nas\n \nidx_read\n,\n \n    \n(\nidx_hit\n \n*\n \nblock_size\n)::\nbigint\n \nas\n \nidx_hit\n,\n \n    \nCASE\n \nWHEN\n \nidx_hit\n \n!=\n \n0\n\n        \nTHEN\n \n((\nidx_hit\n::\nfloat\n \n-\n \nidx_read\n::\nfloat\n)\n \n/\n \nidx_hit\n::\nfloat\n)::\nfloat\n\n        \nELSE\n \n0\n\n    \nEND\n \nas\n \nidx_ratio\n \n\nFROM\n \nbs\n,\n \nstats_heap\n,\n \nstats_idx\n;\n\n\n\n\n\n\n\n\nWarning\n\n\nIt's rather difficult to know if the right formula for ratio is (heap - hit) / hit or hit / (read + hit).\nIt depends if we consider that all disk read (read field) put data into the cache (hit field) or not !\n\nBoth exists on the Internet.",
            "title": "Statistics"
        },
        {
            "location": "/statistics/#statistics",
            "text": "PostgreSQL provides a lot of statistics without any extension, and some more with some fine extensions.  A very good survey of all available statistics and meaning can be found in  these slides .  Extracted fom a slide, here is a schema with all these stats and the area they are related to.   For some general explanations on stats and the meaning of each field, you could read :   this french doc quite old but still useful  this doc from datadog, a monitoring tool",
            "title": "Statistics"
        },
        {
            "location": "/statistics/#checkpoints",
            "text": "What is it ? Some  details .  It's a good starting point to be sure checkpoints behave well.  The  pg_stat_bgwriter  view contains a single row about that activity.  the_db=# select * from pg_stat_bgwriter;\n checkpoints_timed | checkpoints_req | checkpoint_write_time | checkpoint_sync_time | buffers_checkpoint | buffers_clean | maxwritten_clean | buffers_backend | buffers_backend_fsync | buffers_alloc |          stats_reset          \n-------------------+-----------------+-----------------------+----------------------+--------------------+---------------+------------------+-----------------+-----------------------+---------------+-------------------------------\n                84 |               1 |             405618914 |               403083 |            1570508 |     122328215 |          1021398 |       138562615 |                     0 |     380478655 | 2018-01-18 09:14:13.970255+01\n(1 row)   checkpoints_timed  gives the number of checkpoints triggered by the tiemout ( GOOD )  checkpoints_req  gives the number of checkpoints triggered because the  pg_xlog  folder had reached its limit  max_wal_size  ( BAD )   These specific stats could be resetted by :  select   pg_stat_reset_shared ( 'bgwriter' );",
            "title": "Checkpoints"
        },
        {
            "location": "/statistics/#average-time",
            "text": "And the following query   SELECT \n   total_checkpoints , \n   seconds_since_start   /   total_checkpoints   /   60   AS   minutes_between_checkpoints \n   FROM \n   ( SELECT \n     EXTRACT ( EPOCH   FROM   ( now ()   -   stats_reset ))   AS   seconds_since_start , \n     ( checkpoints_timed + checkpoints_req )   AS   total_checkpoints \n     FROM   pg_stat_bgwriter \n   )   AS   sub ;   gives :   the total number of checkpoints that occured since last stat reset  the average time of a checkpoint - should be closed to the  checkpoint_timeout  value",
            "title": "Average time"
        },
        {
            "location": "/statistics/#who-does-the-writing",
            "text": "More complex, the following query is given by (the great)  Greg Smith  :  SELECT \n     ( 100   *   checkpoints_req )   / \n         ( checkpoints_timed   +   checkpoints_req )   AS   checkpoints_req_pct , \n     pg_size_pretty ( buffers_checkpoint   *   block_size   / \n          ( checkpoints_timed   +   checkpoints_req ))   AS   avg_checkpoint_write , \n     pg_size_pretty ( block_size   * \n         ( buffers_checkpoint   +   buffers_clean   +   buffers_backend ))   AS   total_written , \n     100   *   buffers_checkpoint   / \n         ( buffers_checkpoint   +   buffers_clean   +   buffers_backend )   AS   checkpoint_write_pct , \n     100   *   buffers_backend   / \n         ( buffers_checkpoint   +   buffers_clean   +   buffers_backend )   AS   backend_write_pct , \n     100   *   buffers_clean   / \n         ( buffers_checkpoint   +   buffers_clean   +   buffers_backend )   AS   clean_write_pct , \n     *  FROM   pg_stat_bgwriter ,  ( SELECT   cast ( current_setting ( 'block_size' )   AS   integer )   AS   block_size )   bs ;   and provides :   % of checkpoint triggered by size (almost  100% is GOOD )  How much data the average checkpoint write ?  Total written by all processes  % of buffers written by checkpoint process (spread in time :  GOOD )   % of buffers written by backend queries (read from disk because data not in cache / flushed from cache to disk then read again because data is dirty :  BAD )  % of buffers written by clean process (because need some space in cache :  BAD )    Checkpoints are the best way to persist data to disk  because the I/O are spread on time. To be efficient, other processes (backend / clean) should use the cache most of the time, not the disk.  In addition,  pg_bg_writer  table also gives  maxwritten_clean  field that is the number of times the backgroung writer had to stop its writing (cleaning scan) because he has already written more buffers than specified in the  bgwriter_lru_maxpages  parameter.",
            "title": "Who does the writing ?"
        },
        {
            "location": "/statistics/#autovacuum",
            "text": "First of all, autovacuum activity could be traced with the same results in the  pg_stat_user_tables ,  pg_stat_sys_tables  and  pg_stat_all_tables  views.  For each table created by the user, you could find :  last_vacuum      |\nlast_autovacuum  | 2008-04-15 11:56:11.622041+02\nlast_analyze     |\nlast_autoanalyze | 2008-04-15 11:56:11.622041+02  for both manual au auto vacuum processes.  Logs are available through the conf parameter  log_autovacuum_min_duration  to trace long vacuum queries.  Some good metrics to monitor - they are expected to be stable in time :   pg_stat_all_tables.n_dead_tup  \u2013 number of dead tuples in each table (both user tables and system catalogs)  (n_dead_tup / n_live_tup)  \u2013 ratio of dead/live tuples in each table  (pg_class.relpages / pg_class.reltuples)  \u2013 space \u201cper row\u201d   New view in 9.6 provides information about in-progress vacuums :  pg_stat_progress_vacuum .  An extension is available for more stats :  pgstattuple .",
            "title": "Autovacuum"
        },
        {
            "location": "/statistics/#tables-index-size",
            "text": "Tables and index sizes could be get by :  SELECT  \n     table_name ,  \n     pg_size_pretty ( pg_table_size ( table_name ))   AS   table_size ,  \n     pg_size_pretty ( pg_indexes_size ( table_name ))   AS   index_size , \n     pg_size_pretty ( pg_total_relation_size ( table_name ))   as   total_size  FROM   ( \n   SELECT   ( ''   ||   table_schema   ||   '.'   ||   table_name   ||   '' )   AS   table_name   FROM   information_schema . tables   WHERE   table_schema = 'public'  )   AS   all_tables1   ORDER   BY   pg_total_relation_size ( table_name )   desc ;   which outputs :     table_name  table_size  index_size  total_size      public.table1  2102 MB  738 MB  2840 MB    public.table2  77 MB  130 MB  207 MB    public.table3  79 MB  98 MB  177 MB     The total for user tables is get by :   SELECT  \n     pg_size_pretty ( sum ( pg_table_size ( table_name )))   AS   table_size ,  \n     pg_size_pretty ( sum ( pg_indexes_size ( table_name )))   AS   index_size   FROM   ( \n   SELECT   ( ''   ||   table_schema   ||   '.'   ||   table_name   ||   '' )   AS   table_name   FROM   information_schema . tables   WHERE   table_schema = 'public'  )   AS   all_tables1 ;   which outputs :      table_size  index_size      422 GB  277 GB",
            "title": "Tables &amp; index size"
        },
        {
            "location": "/statistics/#table-bloating",
            "text": "Bloating queries come from there :  https://github.com/rach/pome/blob/develop/query.go .",
            "title": "Table Bloating"
        },
        {
            "location": "/statistics/#stats-per-table",
            "text": "Estimate system tables bloat size / ratio with this query :  SELECT   schemaname   as   schema ,   tblname   as   table , \n   CASE   WHEN   ( tblpages - est_tblpages_ff ) * bs   >   0 \n     THEN   (( tblpages - est_tblpages_ff ) * bs ):: bigint \n     ELSE   0 \n   END   AS   bloat_bytes , \n   CASE   WHEN   tblpages   -   est_tblpages_ff   >   0 \n     THEN   round (( 100   *   ( tblpages   -   est_tblpages_ff ) / tblpages :: float ):: numeric ,   1 ) \n     ELSE   0 \n   END   AS   bloat_ratio  FROM   ( \n   SELECT   ceil (   reltuples   /   (   ( bs - page_hdr ) / tpl_size   )   )   +   ceil (   toasttuples   /   4   )   AS   est_tblpages , \n     ceil (   reltuples   /   (   ( bs - page_hdr ) * fillfactor / ( tpl_size * 100 )   )   )   +   ceil (   toasttuples   /   4   )   AS   est_tblpages_ff , \n     tblpages ,   fillfactor ,   bs ,   tblid ,   schemaname ,   tblname ,   heappages ,   toastpages ,   is_na \n   FROM   ( \n     SELECT \n       (   4   +   tpl_hdr_size   +   tpl_data_size   +   ( 2 * ma ) \n         -   CASE   WHEN   tpl_hdr_size % ma   =   0   THEN   ma   ELSE   tpl_hdr_size % ma   END \n         -   CASE   WHEN   ceil ( tpl_data_size ):: int % ma   =   0   THEN   ma   ELSE   ceil ( tpl_data_size ):: int % ma   END \n       )   AS   tpl_size ,   bs   -   page_hdr   AS   size_per_block ,   ( heappages   +   toastpages )   AS   tblpages ,   heappages , \n       toastpages ,   reltuples ,   toasttuples ,   bs ,   page_hdr ,   tblid ,   schemaname ,   tblname ,   fillfactor ,   is_na \n     FROM   ( \n       SELECT \n         tbl . oid   AS   tblid ,   ns . nspname   AS   schemaname ,   tbl . relname   AS   tblname ,   tbl . reltuples , \n         tbl . relpages   AS   heappages ,   coalesce ( toast . relpages ,   0 )   AS   toastpages , \n         coalesce ( toast . reltuples ,   0 )   AS   toasttuples , \n         coalesce ( substring ( \n           array_to_string ( tbl . reloptions ,   ' ' ) \n           FROM   '%fillfactor=#\"__#\"%'   FOR   '#' ):: smallint ,   100 )   AS   fillfactor , \n         current_setting ( 'block_size' ):: numeric   AS   bs , \n         CASE   WHEN   version () ~ 'mingw32'   OR   version () ~ '64-bit|x86_64|ppc64|ia64|amd64'   THEN   8   ELSE   4   END   AS   ma , \n         24   AS   page_hdr , \n         23   +   CASE   WHEN   MAX ( coalesce ( null_frac , 0 ))   >   0   THEN   (   7   +   count ( * )   )   /   8   ELSE   0 :: int   END \n           +   CASE   WHEN   tbl . relhasoids   THEN   4   ELSE   0   END   AS   tpl_hdr_size , \n         sum (   ( 1 - coalesce ( s . null_frac ,   0 ))   *   coalesce ( s . avg_width ,   1024 )   )   AS   tpl_data_size , \n         bool_or ( att . atttypid   =   'pg_catalog.name' :: regtype )   AS   is_na \n       FROM   pg_attribute   AS   att \n         JOIN   pg_class   AS   tbl   ON   att . attrelid   =   tbl . oid \n         JOIN   pg_namespace   AS   ns   ON   ns . oid   =   tbl . relnamespace \n         JOIN   pg_stats   AS   s   ON   s . schemaname = ns . nspname \n           AND   s . tablename   =   tbl . relname   AND   s . inherited = false   AND   s . attname = att . attname \n         LEFT   JOIN   pg_class   AS   toast   ON   tbl . reltoastrelid   =   toast . oid \n       WHERE   att . attnum   >   0   AND   NOT   att . attisdropped \n         AND   tbl . relkind   =   'r'   AND   ns . nspname   =   'pg_catalog' \n       GROUP   BY   1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ,   tbl . relhasoids \n       ORDER   BY   2 , 3 \n     )   AS   s \n   )   AS   s2  )   AS   s3   order   by   bloat_ratio   desc ;   For user tables, set the schemaname value with  AND ns.nspname='public'  and possibly add a relname filter with  AND tbl.relname like '%Myfilter%'  .  Example of output indicating some bad bloating ratio :     schema   |       table        | bloat_bytes | bloat_ratio \n------------+--------------------+-------------+-------------\n pg_catalog | pg_class           |    39116800 |        68.3\n pg_catalog | pg_index           |    17539072 |        61.8\n pg_catalog | pg_depend          |    15507456 |        57.2\n pg_catalog | pg_type            |    15269888 |        56.1\n pg_catalog | pg_attribute       |    99336192 |        55.6\n pg_catalog | pg_constraint      |      172032 |        36.2\n pg_catalog | pg_shdepend        |      598016 |        14.5\n pg_catalog | pg_description     |       16384 |         5.9  To illustrate Vacuum effect on table bloating :   # Gather bloating stats before vacuum\n   schema   |       table        | bloat_bytes | bloat_ratio \n------------+--------------------+-------------+-------------\n public     | my_bloated_table   |   128622592 | 68.698315467075\n\n# Then vacuum\nvacuum (full, analyze) my_bloated_table;\n\n# Finally refresh stats\n   schema   |       table        | bloat_bytes | bloat_ratio \n------------+--------------------+-------------+-------------\n public     | my_bloated_table   |   6520832   | 9.74176967323461  It's much better !",
            "title": "Stats per table"
        },
        {
            "location": "/statistics/#stats-total",
            "text": "To get total bloating of all databases of a server, execute :  SELECT   pg_size_pretty ( SUM ( real_size ):: bigint )   as   real_size , \n        pg_size_pretty ( SUM ( bloat_bytes ):: bigint )   as   bloat_size   FROM   ( \n   SELECT  \n     bs * tblpages   AS   real_size , \n     CASE   WHEN   ( tblpages - est_tblpages_ff ) * bs   >   0 \n       THEN   (( tblpages - est_tblpages_ff ) * bs ):: bigint \n       ELSE   0 \n     END   AS   bloat_bytes \n   FROM   ( \n     SELECT   ceil (   reltuples   /   (   ( bs - page_hdr ) / tpl_size   )   )   +   ceil (   toasttuples   /   4   )   AS   est_tblpages , \n       ceil (   reltuples   /   (   ( bs - page_hdr ) * fillfactor / ( tpl_size * 100 )   )   )   +   ceil (   toasttuples   /   4   )   AS   est_tblpages_ff , \n       tblpages ,   fillfactor ,   bs ,   tblid ,   schemaname ,   tblname ,   heappages ,   toastpages ,   is_na \n     FROM   ( \n       SELECT \n         (   4   +   tpl_hdr_size   +   tpl_data_size   +   ( 2 * ma ) \n           -   CASE   WHEN   tpl_hdr_size % ma   =   0   THEN   ma   ELSE   tpl_hdr_size % ma   END \n           -   CASE   WHEN   ceil ( tpl_data_size ):: int % ma   =   0   THEN   ma   ELSE   ceil ( tpl_data_size ):: int % ma   END \n         )   AS   tpl_size ,   bs   -   page_hdr   AS   size_per_block ,   ( heappages   +   toastpages )   AS   tblpages ,   heappages , \n         toastpages ,   reltuples ,   toasttuples ,   bs ,   page_hdr ,   tblid ,   schemaname ,   tblname ,   fillfactor ,   is_na \n       FROM   ( \n         SELECT \n           tbl . oid   AS   tblid ,   ns . nspname   AS   schemaname ,   tbl . relname   AS   tblname ,   tbl . reltuples , \n           tbl . relpages   AS   heappages ,   coalesce ( toast . relpages ,   0 )   AS   toastpages , \n           coalesce ( toast . reltuples ,   0 )   AS   toasttuples , \n           coalesce ( substring ( \n             array_to_string ( tbl . reloptions ,   ' ' ) \n             FROM   '%fillfactor=#\"__#\"%'   FOR   '#' ):: smallint ,   100 )   AS   fillfactor , \n           current_setting ( 'block_size' ):: numeric   AS   bs , \n           CASE   WHEN   version () ~ 'mingw32'   OR   version () ~ '64-bit|x86_64|ppc64|ia64|amd64'   THEN   8   ELSE   4   END   AS   ma , \n           24   AS   page_hdr , \n           23   +   CASE   WHEN   MAX ( coalesce ( null_frac , 0 ))   >   0   THEN   (   7   +   count ( * )   )   /   8   ELSE   0 :: int   END \n             +   CASE   WHEN   tbl . relhasoids   THEN   4   ELSE   0   END   AS   tpl_hdr_size , \n           sum (   ( 1 - coalesce ( s . null_frac ,   0 ))   *   coalesce ( s . avg_width ,   1024 )   )   AS   tpl_data_size , \n           bool_or ( att . atttypid   =   'pg_catalog.name' :: regtype )   AS   is_na \n         FROM   pg_attribute   AS   att \n           JOIN   pg_class   AS   tbl   ON   att . attrelid   =   tbl . oid \n           JOIN   pg_namespace   AS   ns   ON   ns . oid   =   tbl . relnamespace \n           JOIN   pg_stats   AS   s   ON   s . schemaname = ns . nspname \n             AND   s . tablename   =   tbl . relname   AND   s . inherited = false   AND   s . attname = att . attname \n           LEFT   JOIN   pg_class   AS   toast   ON   tbl . reltoastrelid   =   toast . oid \n         WHERE   att . attnum   >   0   AND   NOT   att . attisdropped \n           AND   tbl . relkind   =   'r'   AND   ns . nspname   =   'pg_catalog' \n         GROUP   BY   1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ,   tbl . relhasoids \n         ORDER   BY   2 , 3 \n       )   AS   s \n     )   AS   s2 \n   )   AS   s3  )   as   s4 ;   It outputs for example :     real_size | bloat_size \n -----------+-------------\n  310 MB    | 179 MB",
            "title": "Stats total"
        },
        {
            "location": "/statistics/#index-bloating",
            "text": "Bloating queries come from there :  https://github.com/rach/pome/blob/develop/query.go .",
            "title": "Index bloating"
        },
        {
            "location": "/statistics/#stats-per-index",
            "text": "Estimate system index bloat size / ratio with this query :  WITH   btree_index_atts   AS   ( \n     SELECT   nspname ,   relname ,   reltuples ,   relpages ,   indrelid ,   relam , \n         regexp_split_to_table ( indkey :: text ,   ' ' ):: smallint   AS   attnum , \n         indexrelid   as   index_oid \n     FROM   pg_index \n     JOIN   pg_class   ON   pg_class . oid = pg_index . indexrelid \n     JOIN   pg_namespace   ON   pg_namespace . oid   =   pg_class . relnamespace \n     JOIN   pg_am   ON   pg_class . relam   =   pg_am . oid \n     WHERE   pg_am . amname   =   'btree'   AND   pg_namespace . nspname   =   'pg_catalog' \n     ),  index_item_sizes   AS   ( \n     SELECT \n     i . nspname ,   i . relname ,   i . reltuples ,   i . relpages ,   i . relam , \n     ( quote_ident ( s . schemaname )   ||   '.'   ||   quote_ident ( s . tablename )):: regclass   AS   starelid ,   a . attrelid   AS   table_oid ,   index_oid , \n     current_setting ( 'block_size' ):: numeric   AS   bs , \n     /* MAXALIGN: 4 on 32bits, 8 on 64bits (and mingw32 ?) */ \n     CASE \n         WHEN   version ()   ~   'mingw32'   OR   version ()   ~   '64-bit'   THEN   8 \n         ELSE   4 \n     END   AS   maxalign , \n     24   AS   pagehdr , \n     /* per tuple header: add index_attribute_bm if some cols are null-able */ \n     CASE   WHEN   max ( coalesce ( s . null_frac , 0 ))   =   0 \n         THEN   2 \n         ELSE   6 \n     END   AS   index_tuple_hdr , \n     /* data len: we remove null values save space using it fractionnal part from stats */ \n     sum (   ( 1 - coalesce ( s . null_frac ,   0 ))   *   coalesce ( s . avg_width ,   2048 )   )   AS   nulldatawidth \n     FROM   pg_attribute   AS   a \n     JOIN   pg_stats   AS   s   ON   ( quote_ident ( s . schemaname )   ||   '.'   ||   quote_ident ( s . tablename )):: regclass = a . attrelid   AND   s . attname   =   a . attname  \n     JOIN   btree_index_atts   AS   i   ON   i . indrelid   =   a . attrelid   AND   a . attnum   =   i . attnum \n     WHERE   a . attnum   >   0 \n     GROUP   BY   1 ,   2 ,   3 ,   4 ,   5 ,   6 ,   7 ,   8 ,   9  ),  index_aligned   AS   ( \n     SELECT   maxalign ,   bs ,   nspname ,   relname   AS   index_name ,   reltuples , \n         relpages ,   relam ,   table_oid ,   index_oid , \n       (   2   + \n           maxalign   -   CASE   /* Add padding to the index tuple header to align on MAXALIGN */ \n             WHEN   index_tuple_hdr % maxalign   =   0   THEN   maxalign \n             ELSE   index_tuple_hdr % maxalign \n           END \n         +   nulldatawidth   +   maxalign   -   CASE   /* Add padding to the data to align on MAXALIGN */ \n             WHEN   nulldatawidth :: integer % maxalign   =   0   THEN   maxalign \n             ELSE   nulldatawidth :: integer % maxalign \n           END \n       ):: numeric   AS   nulldatahdrwidth ,   pagehdr \n     FROM   index_item_sizes   AS   s1  ),  otta_calc   AS   ( \n   SELECT   bs ,   nspname ,   table_oid ,   index_oid ,   index_name ,   relpages ,   coalesce ( \n     ceil (( reltuples * ( 4 + nulldatahdrwidth )) / ( bs - pagehdr :: float ))   + \n       CASE   WHEN   am . amname   IN   ( 'hash' , 'btree' )   THEN   1   ELSE   0   END   ,   0   -- btree and hash have a metadata reserved block \n     )   AS   otta \n   FROM   index_aligned   AS   s2 \n     LEFT   JOIN   pg_am   am   ON   s2 . relam   =   am . oid  ),  raw_bloat   AS   ( \n     SELECT   current_database ()   as   dbname ,   nspname ,   c . relname   AS   tablename ,   index_name , \n         bs * ( sub . relpages ):: bigint   AS   totalbytes , \n         CASE \n             WHEN   sub . relpages   <=   otta   THEN   0 \n             ELSE   bs * ( sub . relpages - otta ):: bigint   END \n             AS   wastedbytes , \n         CASE \n             WHEN   sub . relpages   <=   otta \n             THEN   0   ELSE   bs * ( sub . relpages - otta ):: bigint   *   100   /   ( bs * ( sub . relpages ):: bigint )   END \n             AS   realbloat , \n         pg_relation_size ( sub . table_oid )   as   table_bytes \n     FROM   otta_calc   AS   sub \n     JOIN   pg_class   AS   c   ON   c . oid = sub . table_oid  )  SELECT    nspname   AS   schema , \n         tablename   as   table , \n         index_name   AS   index , \n         wastedbytes   as   bloat_bytes , \n         round ( realbloat ,   1 )   as   bloat_ratio ,  \n         totalbytes   as   index_size  FROM   raw_bloat  ORDER   BY   wastedbytes   DESC ;   For user tables, set the schemaname value with  AND pg_namespace.nspname='public'  and possibly add a relname filter with  WHERE tablename='probes_global_statistics'  before the last  ORDER BY .",
            "title": "Stats per index"
        },
        {
            "location": "/statistics/#stats-total_1",
            "text": "To get total bloating of all databases of a server, execute :  SELECT   pg_size_pretty ( SUM ( index_size ):: bigint )   as   index_size , \n        pg_size_pretty ( SUM ( bloat_bytes ):: bigint )   as   bloat_size  FROM   (  WITH   btree_index_atts   AS   ( \n     SELECT   nspname ,   relname ,   reltuples ,   relpages ,   indrelid ,   relam , \n         regexp_split_to_table ( indkey :: text ,   ' ' ):: smallint   AS   attnum , \n         indexrelid   as   index_oid \n     FROM   pg_index \n     JOIN   pg_class   ON   pg_class . oid = pg_index . indexrelid \n     JOIN   pg_namespace   ON   pg_namespace . oid   =   pg_class . relnamespace \n     JOIN   pg_am   ON   pg_class . relam   =   pg_am . oid \n     WHERE   pg_am . amname   =   'btree'   AND   pg_namespace . nspname   =   'pg_catalog' \n     ),  index_item_sizes   AS   ( \n     SELECT \n     i . nspname ,   i . relname ,   i . reltuples ,   i . relpages ,   i . relam , \n     ( quote_ident ( s . schemaname )   ||   '.'   ||   quote_ident ( s . tablename )):: regclass   AS   starelid ,   a . attrelid   AS   table_oid ,   index_oid , \n     current_setting ( 'block_size' ):: numeric   AS   bs , \n     /* MAXALIGN: 4 on 32bits, 8 on 64bits (and mingw32 ?) */ \n     CASE \n         WHEN   version ()   ~   'mingw32'   OR   version ()   ~   '64-bit'   THEN   8 \n         ELSE   4 \n     END   AS   maxalign , \n     24   AS   pagehdr , \n     /* per tuple header: add index_attribute_bm if some cols are null-able */ \n     CASE   WHEN   max ( coalesce ( s . null_frac , 0 ))   =   0 \n         THEN   2 \n         ELSE   6 \n     END   AS   index_tuple_hdr , \n     /* data len: we remove null values save space using it fractionnal part from stats */ \n     sum (   ( 1 - coalesce ( s . null_frac ,   0 ))   *   coalesce ( s . avg_width ,   2048 )   )   AS   nulldatawidth \n     FROM   pg_attribute   AS   a \n     JOIN   pg_stats   AS   s   ON   ( quote_ident ( s . schemaname )   ||   '.'   ||   quote_ident ( s . tablename )):: regclass = a . attrelid   AND   s . attname   =   a . attname  \n     JOIN   btree_index_atts   AS   i   ON   i . indrelid   =   a . attrelid   AND   a . attnum   =   i . attnum \n     WHERE   a . attnum   >   0 \n     GROUP   BY   1 ,   2 ,   3 ,   4 ,   5 ,   6 ,   7 ,   8 ,   9  ),  index_aligned   AS   ( \n     SELECT   maxalign ,   bs ,   nspname ,   relname   AS   index_name ,   reltuples , \n         relpages ,   relam ,   table_oid ,   index_oid , \n       (   2   + \n           maxalign   -   CASE   /* Add padding to the index tuple header to align on MAXALIGN */ \n             WHEN   index_tuple_hdr % maxalign   =   0   THEN   maxalign \n             ELSE   index_tuple_hdr % maxalign \n           END \n         +   nulldatawidth   +   maxalign   -   CASE   /* Add padding to the data to align on MAXALIGN */ \n             WHEN   nulldatawidth :: integer % maxalign   =   0   THEN   maxalign \n             ELSE   nulldatawidth :: integer % maxalign \n           END \n       ):: numeric   AS   nulldatahdrwidth ,   pagehdr \n     FROM   index_item_sizes   AS   s1  ),  otta_calc   AS   ( \n   SELECT   bs ,   nspname ,   table_oid ,   index_oid ,   index_name ,   relpages ,   coalesce ( \n     ceil (( reltuples * ( 4 + nulldatahdrwidth )) / ( bs - pagehdr :: float ))   + \n       CASE   WHEN   am . amname   IN   ( 'hash' , 'btree' )   THEN   1   ELSE   0   END   ,   0   -- btree and hash have a metadata reserved block \n     )   AS   otta \n   FROM   index_aligned   AS   s2 \n     LEFT   JOIN   pg_am   am   ON   s2 . relam   =   am . oid  ),  raw_bloat   AS   ( \n     SELECT   current_database ()   as   dbname ,   nspname ,   c . relname   AS   tablename ,   index_name , \n         bs * ( sub . relpages ):: bigint   AS   totalbytes , \n         CASE \n             WHEN   sub . relpages   <=   otta   THEN   0 \n             ELSE   bs * ( sub . relpages - otta ):: bigint   END \n             AS   wastedbytes , \n         CASE \n             WHEN   sub . relpages   <=   otta \n             THEN   0   ELSE   bs * ( sub . relpages - otta ):: bigint   *   100   /   ( bs * ( sub . relpages ):: bigint )   END \n             AS   realbloat , \n         pg_relation_size ( sub . table_oid )   as   table_bytes \n     FROM   otta_calc   AS   sub \n     JOIN   pg_class   AS   c   ON   c . oid = sub . table_oid  )  SELECT    wastedbytes   as   bloat_bytes ,       \n         totalbytes   as   index_size  FROM   raw_bloat  )   AS   s4 ;   It outputs for example :    index_size | bloat_size \n------------+------------\n 283 MB     | 199 MB",
            "title": "Stats total"
        },
        {
            "location": "/statistics/#cache-usage",
            "text": "Amount of data read from disk and total for all user tables of a database and their indexes.  WITH   stats_heap   AS  ( \n     SELECT \n     ( SELECT   sum ( heap_blks_read )   FROM   pg_statio_user_tables ):: bigint   as   heap_read ,  \n     ( SELECT   sum ( heap_blks_hit )   FROM   pg_statio_user_tables ):: bigint   as   heap_hit  ),  stats_idx   AS  ( \n     SELECT \n     ( SELECT   sum ( idx_blks_read )   FROM   pg_statio_user_indexes ):: bigint   as   idx_read ,  \n     ( SELECT   sum ( idx_blks_hit )   FROM   pg_statio_user_indexes ):: bigint   as   idx_hit  ),  bs   AS  ( \n     SELECT  \n     cast ( current_setting ( 'block_size' )   AS   integer )   AS   block_size  )  SELECT   current_database ()   AS   dbname ,   ( heap_read   *   block_size ):: bigint   as   heap_read ,   ( heap_hit   *   block_size ):: bigint   as   heap_hit ,   (( heap_hit :: float   -   heap_read :: float )   /   heap_hit :: float ):: float   as   heap_ratio ,  ( idx_read   *   block_size ):: bigint   as   idx_read ,   ( idx_hit   *   block_size ):: bigint   as   idx_hit ,   (( idx_hit :: float   -   idx_read :: float )   /   idx_hit :: float ):: float   as   idx_ratio   FROM   bs ,   stats_heap ,   stats_idx ;   This query could work for system tables by using the  pg_statio_sys_tables  table.  Of course, it could be also written to analyze cache usage for a single table :  SET   you . table_name   =   'report' ;  WITH   stats_heap   AS  ( \n     SELECT \n     ( SELECT   heap_blks_read   FROM   pg_statio_user_tables   where   relname = current_setting ( 'you.table_name' )):: bigint   as   heap_read ,  \n     ( SELECT   heap_blks_hit   FROM   pg_statio_user_tables   where   relname = current_setting ( 'you.table_name' )):: bigint   as   heap_hit  ),  stats_idx   AS  ( \n     SELECT \n     ( SELECT   sum ( idx_blks_read )   FROM   pg_statio_user_indexes   where   relname = current_setting ( 'you.table_name' )):: bigint   as   idx_read ,  \n     ( SELECT   sum ( idx_blks_hit )   FROM   pg_statio_user_indexes   where   relname = current_setting ( 'you.table_name' )):: bigint   as   idx_hit  ),  bs   AS  ( \n     SELECT  \n     cast ( current_setting ( 'block_size' )   AS   integer )   AS   block_size  )  SELECT  \n     current_database ()   AS   dbname ,  \n     ( heap_read   *   block_size ):: bigint   as   heap_read ,  \n     ( heap_hit   *   block_size ):: bigint   as   heap_hit ,  \n     CASE   WHEN   heap_hit   !=   0 \n         THEN   (( heap_hit :: float   -   heap_read :: float )   /   heap_hit :: float ):: float \n         ELSE   0 \n     END   as   heap_ratio , \n     ( idx_read   *   block_size ):: bigint   as   idx_read ,  \n     ( idx_hit   *   block_size ):: bigint   as   idx_hit ,  \n     CASE   WHEN   idx_hit   !=   0 \n         THEN   (( idx_hit :: float   -   idx_read :: float )   /   idx_hit :: float ):: float \n         ELSE   0 \n     END   as   idx_ratio   FROM   bs ,   stats_heap ,   stats_idx ;    Warning  It's rather difficult to know if the right formula for ratio is (heap - hit) / hit or hit / (read + hit).\nIt depends if we consider that all disk read (read field) put data into the cache (hit field) or not ! \nBoth exists on the Internet.",
            "title": "Cache usage"
        },
        {
            "location": "/monitoring/",
            "text": "Monitoring\n\u00b6\n\n\nExplain how to deploy via Docker / configure a TIG stack : \nTelegraf\n, \nInfluxdb\n and \nGrafana\n.\n\n\nArchitecture\n\u00b6\n\n\nCentral Server\n\u00b6\n\n\nDocker-compose.yml\n\u00b6\n\n\nversion: '2'\n\nservices:\n    ## InfluxDB container\n    influxdb:\n        image: influxdb:latest\n        restart: always\n        ports:\n          - \"80:8086\"\n        volumes:\n          - \n${\nPWD\n}\n/data/influxdb:/var/lib/influxdb\n          - \n${\nPWD\n}\n/data/influxdb/influxdb.conf:/etc/influxdb/influxdb.conf:ro\n\n## Grafana container\n    grafana:\n        image: grafana/grafana\n        restart: always\n        ports:\n          - \"3000:3000\"\n        volumes:\n          - \n${\nPWD\n}\n/data/grafana/var/lib/grafana:/var/lib/grafana\n          - \n${\nPWD\n}\n/data/grafana/etc/grafana/grafana.ini:/etc/grafana/grafana.ini\n          - \n${\nPWD\n}\n/data/grafana/var/log/grafana:/var/log/grafana\n\n\n\n\n\nInstallation\n\u00b6\n\n\nPick up a monitoring server to host influxDB & Grafana\n\n\nPull images\n\n\ndocker pull grafana/grafana\ndocker pull influxdb\n\n\n\n\n\nCreate sub-dirs\n\n\nmkdir -p /your_home/monitoring/data/grafana/etc/grafana\nmkdir -p /your_home/monitoring/data/grafana/var/lib/grafana\nmkdir -p /your_home/monitoring/data/grafana/var/log/grafana\nmkdir -p /your_home/monitoring/data/influxdb/\n\n\n\n\n\nGenerate default InfluxDB config file\n\n\ncd /your_home/monitoring\ndocker run --rm influxdb influxd config > data/influxdb/influxdb.conf\n\n\n\n\n\nGet default grafana.ini file from docker container to host\n\n\ndocker run -d grafana/grafana bash\n\n\n\n\n\nFind it\n\n\ndocker ps\n\n\n\n\n\nGo in it\n\n\ndocker exec -it CONTAINER_ID bash\n\n\n\n\n\nCat the /etc/grafana/grafana/ini file and copy its content to data/grafana/etc/grafana/grafana.ini file.\n\n\nCopy the compose yaml (see below)\n\n\n/your_home/monitoring/docker-compose-central.yml\nmv /your_home/monitoring/docker-compose-central.yml /your_home/monitoring/docker-compose.yml\n\n\n\n\n\nStart the stack\n\n\ndocker-compose up -d\n\n\n\n\n\nHere are some grafana dashboards :\n\n\n\n\nPostgreSQL overview\n\n\nPostgreSQL advanced view\n\n\nPostgreSQL single table monitoring\n\n\nCentral server (influxdb + grafana) overview\n\n\n\n\nControl influxdb\n\u00b6\n\n\nIdentify the influxDB container ID with :\n\n\ndocker ps\n\n\n\n\n\nThen\n\n\ndocker exec -it INFLUX_CONTAINER_ID bash\n\n\n\n\n\nAnd \n\n\ninflux\n\n\n\n\n\nTo get the influx cli. Connect to the telegraf db by :\n\n\nuse telegraf\n\n\n\n\n\nDisplay the tables (measurements) :\n\n\nshow measurements\n\n\n\n\n\nDisplay a measurement content :\n\n\nselect * from the_measurement where time > now() - 1h\n\n\n\n\n\nCollector\n\u00b6\n\n\nWe have to deploy a telegraf per target server.\n\n\nDocker-compose.yml\n\u00b6\n\n\nversion: '2'\n\nservices:\n## Telegraf container\n    telegraf:\n        image: telegraf:latest\n        volumes:\n            # standard config\n            - \n${\nPWD\n}\n/data/telegraf/telegraf.conf:/etc/telegraf/telegraf.conf:ro\n            # custom config - can't make it workw\n#            - \n${\nPWD\n}\n/data/telegraf/telegraf-ext/custom.config:/etc/telegraf/telegraf.d/custom.conf\n            - /var/run/docker.sock:/var/run/docker.sock:ro\n        restart: always\n\n\n\n\n\nInstallation\n\u00b6\n\n\nInstall Docker / docker-compose if necessary : \n\n\ninstall docker\ninstall docker-compose\n\n\n\n\n\nConfigure Docker to access network through proxy for example :\n\n\nsudo mkdir -p /etc/systemd/system/docker.service.d/\nsudo nano /etc/systemd/system/docker.service.d/docker.conf\n\n#Copy in this file something like\n[Service]\nExecStart=\nEnvironment=\"HTTP_PROXY=the_proxy\"\n\nsudo systemctl daemon-reload\nsudo systemctl restart docker.service\n\n\n\n\n\nPull images\n\n\ndocker pull telegraf\n\n\n\n\n\nCreate sub-dirs\n\n\nmkdir -p /your_home/monitoring/data/telegraf/\n\n\n\n\n\nGenerate default config files\n\n\ncd /your_home/monitoring\ndocker run --rm telegraf -sample-config > data/telegraf/telegraf.conf\n\n\n\n\n\nGeneral configuration, edit \ntelegraf.conf\n\n\nIn [global_tags] part, add : \n\n\ncustomer = \"the_customer\"\n\n\n\n\n\nIn [agent] part, set :\n\n\ninterval = \"60s\"\ndebug = true                    # might be useful !\nhostname = \"the_host_name\"    # or whatever\n\n\n\n\n\nIn [[outputs.influxdb]] part, set\n\n\nurls = [\"http://influx.mymonitoring.com:80\"]\n# If necessary\nhttp_proxy = \"http://corporate.proxy:3128\"\n\n\n\n\n\nCopy the compose yaml\n\n\n/your_home/monitoring/docker-compose-telegraf.yml\nmv /your_home/monitoring/docker-compose-telegraf.yml /your_home/monitoring/docker-compose.yml\n\n\n\n\n\nStart the stack\n\n\ndocker-compose up -d\n\n\n\n\n\nCheck the logs. You should see :\n\n\ntelegraf_1  | 2018/02/05 10:38:37 I! Using config file: /etc/telegraf/telegraf.conf\ntelegraf_1  | 2018-02-05T10:38:37Z D! Attempting connection to output: influxdb\ntelegraf_1  | 2018-02-05T10:38:37Z D! Successfully connected to output: influxdb\ntelegraf_1  | 2018-02-05T10:38:37Z I! Starting Telegraf v1.5.2\ntelegraf_1  | 2018-02-05T10:38:37Z I! Loaded outputs: influxdb\ntelegraf_1  | 2018-02-05T10:38:37Z I! Loaded inputs: inputs.postgresql_extensible inputs.postgresql_extensible inputs.postgresql_extensible inputs.disk inputs.diskio inputs.kernel inputs.mem inputs.postgresql inputs.processes inputs.swap inputs.system inputs.cpu\ntelegraf_1  | 2018-02-05T10:38:37Z I! Tags enabled: customer=the_customer host=the_host_name\ntelegraf_1  | 2018-02-05T10:38:37Z I! Agent Config: Interval:1m0s, Quiet:false, Hostname:\"the_host_name\", Flush Interval:10s\n\n\n\n\n\nAjouter des inputs \u00e0 telegraf (change the database IP in each \n[[inputs.postgresql]]\n block):\n\n\n[[inputs.postgresql]]\n\n  \ninterval\n \n=\n \n\"60s\"\n\n\n  ## specify address via a url matching:\n\n\n  ##   postgres://[pqgotest[:password]]@localhost[/dbname]\\\n\n\n  ##       ?sslmode=[disable|verify-ca|verify-full]\n\n\n  ## or a simple string:\n\n\n  ##   host=localhost user=pqotest password=... sslmode=... dbname=app_production\n\n\n  ##\n\n\n  ## All connection parameters are optional.\n\n\n  ##\n\n\n  ## Without the dbname parameter, the driver will default to a database\n\n\n  ## with the same name as the user. This dbname is just for instantiating a\n\n\n  ## connection with the server and doesn't restrict the databases we are trying\n\n\n  ## to grab metrics for.\n\n\n  ##\n\n\n  #address = \"host=10.2.1.188 user=my_user sslmode=disable dbname=postgres\"\n\n\n  address = \"host=10.xx.yy.196 user=my_user password=my_password sslmode=disable dbname=postgres\"\n\n\n  ## A  list of databases to explicitly ignore.  If not specified, metrics for all\n\n\n  ## databases are gathered.  Do NOT use with the 'databases' option.\n\n\n  # ignored_databases = [\"postgres\", \"template0\", \"template1\"]\n\n\n  ignored_databases = [\"postgres\", \"template0\", \"template1\"]\n\n\n  ## A list of databases to pull metrics about. If not specified, metrics for all\n\n\n  ## databases are gathered.  Do NOT use with the 'ignored_databases' option.\n\n\n\n## #######################################################\n\n\n## Checkpoints\n\n\n##\n\n\n[[inputs.postgresql_extensible]]\n\n  \ninterval\n \n=\n \n\"300s\"\n\n\n  address = \"host=10.xx.yy.196 user=my_user password=my_password sslmode=disable dbname=postgres\"\n\n\n[[inputs.postgresql_extensible.query]]\n\n  \n#Checkpoints stats\n\n  \n# checkpoints_timeout_pct | checkpoints_req_pct | avg_checkpoints_minutes | avg_checkpoint_write | total_written | checkpoint_write_pct | backend_write_pct | total_checkpoints | seconds_since_start | setting_checkpoint_timeout | setting_max_wal_size \n\n  \nsqlquery\n=\n\"SELECT (100*checkpoints_timed) / total_checkpoints as checkpoints_timeout_pct, 100 - (100*checkpoints_timed) / total_checkpoints AS checkpoints_req_pct, seconds_since_start / total_checkpoints / 60 AS avg_checkpoints_minutes, buffers_checkpoint * block_size / (checkpoints_timed + checkpoints_req) AS avg_checkpoint_write, block_size * (buffers_checkpoint + buffers_clean + buffers_backend) AS total_written, 100 * buffers_checkpoint / (buffers_checkpoint + buffers_clean + buffers_backend) AS checkpoint_write_pct, 100 * buffers_backend / (buffers_checkpoint + buffers_clean + buffers_backend) AS backend_write_pct, total_checkpoints,seconds_since_start, (select setting from pg_settings where name = 'checkpoint_timeout') as setting_checkpoint_timeout, (select (setting::bigint) * 16*1024*1024 from pg_settings where name = 'max_wal_size') as setting_max_wal_size FROM (SELECT EXTRACT(EPOCH FROM (now() - stats_reset)) AS seconds_since_start, *, (checkpoints_timed+checkpoints_req) AS total_checkpoints FROM pg_stat_bgwriter WHERE (checkpoints_timed+checkpoints_req)>0) AS sub, (SELECT cast(current_setting('block_size') AS integer) AS block_size) bs\"\n\n\n  version=901\n\n\n  withdbname=false\n\n\n  tagvalue=\"\"\n\n\n  measurement=\"pg_stat_checkpoints\"\n\n\n\n## #######################################################\n\n\n## Sizes\n\n\n##\n\n\n## Create one new block per database\n\n\n## Change de \"dbName\" in the \"address\" line\n\n\n##\n\n\n[[inputs.postgresql_extensible]]\n\n  \ninterval\n \n=\n \n\"1800s\"\n\n\n  address = \"host=10.xx.yy.196 user=my_user password=my_password sslmode=disable dbname=the_db\"\n\n\n[[inputs.postgresql_extensible.query]]\n\n  \n#Size stats\n\n  \nsqlquery\n=\n\"SELECT (SELECT current_database()) as dbname, sum(pg_table_size(table_name))::bigint AS table_size, sum(pg_indexes_size(table_name))::bigint AS index_size FROM (SELECT ('' || table_schema || '.' || table_name || '') AS table_name FROM information_schema.tables WHERE table_schema='public') AS all_tables1\"\n\n\n  version=901\n\n\n  withdbname=false\n\n\n  tagvalue=\"dbname\"\n\n\n  measurement=\"pg_stat_size\"\n\n\n\n## #######################################################\n\n\n## Bloating\n\n\n##\n\n\n## Create one new block per database\n\n\n## Change de \"dbName\" in the \"address\" line\n\n\n## \n\n\n## CAREFUL : user indexes query could be harmful for the DB load\n\n\n##\n\n\n[[inputs.postgresql_extensible]]\n\n  \ninterval\n \n=\n \n\"900s\"\n\n\n  address = \"host=10.xx.yy.196 user=my_user password=my_password sslmode=disable dbname=the_db\"\n\n\n[[inputs.postgresql_extensible.query]]\n\n  \n#Bloating tables system\n\n  \nsqlquery\n=\n\"SELECT 'tables' as relation, 'system' as owner, (SELECT current_database()) as dbname, bloat_size::bigint, real_size::bigint, (bloat_size / real_size)::float as bloat_ratio FROM (SELECT SUM(bloat_bytes) as bloat_size FROM (SELECT  CASE WHEN (tblpages-est_tblpages_ff)*bs > 0 THEN ((tblpages-est_tblpages_ff)*bs)::bigint ELSE 0 END AS bloat_bytes, CASE WHEN tblpages - est_tblpages_ff > 0 THEN round((100 * (tblpages - est_tblpages_ff)/tblpages::float)::numeric, 1) ELSE 0 END AS bloat_ratio FROM (SELECT ceil( reltuples / ( (bs-page_hdr)/tpl_size ) ) + ceil( toasttuples / 4 ) AS est_tblpages, ceil( reltuples / ( (bs-page_hdr)*fillfactor/(tpl_size*100) ) ) + ceil( toasttuples / 4 ) AS est_tblpages_ff, tblpages, bs FROM (SELECT ( 4 + tpl_hdr_size + tpl_data_size + (2*ma) - CASE WHEN tpl_hdr_size%ma = 0 THEN ma ELSE tpl_hdr_size%ma END - CASE WHEN ceil(tpl_data_size)::int%ma = 0 THEN ma ELSE ceil(tpl_data_size)::int%ma END) AS tpl_size, (heappages + toastpages) AS tblpages, reltuples, toasttuples, bs, page_hdr, fillfactor FROM (SELECT ns.nspname AS schemaname, tbl.relname AS tblname, tbl.reltuples, tbl.relpages AS heappages, coalesce(toast.relpages, 0) AS toastpages, coalesce(toast.reltuples, 0) AS toasttuples, coalesce(substring(array_to_string(tbl.reloptions, ' ') FROM '%fillfactor=#\\\"__#\\\"%' FOR '#')::smallint, 100) AS fillfactor, current_setting('block_size')::numeric AS bs, CASE WHEN version()~'mingw32' OR version()~'64-bit|x86_64|ppc64|ia64|amd64' THEN 8 ELSE 4 END AS ma, 24 AS page_hdr, 23 + CASE WHEN MAX(coalesce(null_frac,0)) > 0 THEN ( 7 + count(*) ) / 8 ELSE 0::int END + CASE WHEN tbl.relhasoids THEN 4 ELSE 0 END AS tpl_hdr_size, sum( (1-coalesce(s.null_frac, 0)) * coalesce(s.avg_width, 1024) ) AS tpl_data_size FROM pg_attribute AS att JOIN pg_class AS tbl ON att.attrelid = tbl.oid JOIN pg_namespace AS ns ON ns.oid = tbl.relnamespace JOIN pg_stats AS s ON s.schemaname=ns.nspname AND s.tablename = tbl.relname AND s.inherited=false AND s.attname=att.attname LEFT JOIN pg_class AS toast ON tbl.reltoastrelid = toast.oid WHERE att.attnum > 0 AND NOT att.attisdropped AND tbl.relkind = 'r' AND ns.nspname = 'pg_catalog' GROUP BY 1,2,3,4,5,6,7,8,9,10, tbl.relhasoids) AS s) AS s2) AS s3) AS s4) as s5, (SELECT sum(pg_table_size(table_name)) as real_size FROM (SELECT ('' || table_schema || '.' || table_name || '') AS table_name FROM information_schema.tables WHERE table_schema='pg_catalog') AS all_tables1) as p1\"\n\n\n  version=901\n\n\n  withdbname=false\n\n\n  tagvalue=\"relation,owner,dbname\"\n\n\n  measurement=\"pg_stat_bloat\"\n\n\n[[inputs.postgresql_extensible.query]]\n\n  \n#Bloating index system\n\n  \nsqlquery\n=\n\"SELECT 'index' as relation, 'system' as owner, (SELECT current_database()) as dbname, bloat_size::bigint, real_size::bigint, (bloat_size / real_size)::float as bloat_ratio FROM (SELECT SUM(bloat_bytes) as bloat_size FROM (WITH btree_index_atts AS (SELECT nspname, relname, reltuples, relpages, indrelid, relam, regexp_split_to_table(indkey::text, ' ')::smallint AS attnum, indexrelid as index_oid FROM pg_index JOIN pg_class ON pg_class.oid=pg_index.indexrelid JOIN pg_namespace ON pg_namespace.oid = pg_class.relnamespace JOIN pg_am ON pg_class.relam = pg_am.oid WHERE pg_am.amname = 'btree' AND pg_namespace.nspname='pg_catalog'), index_item_sizes AS (SELECT i.nspname, i.relname, i.reltuples, i.relpages, i.relam, (quote_ident(s.schemaname) || '.' || quote_ident(s.tablename))::regclass AS starelid, a.attrelid AS table_oid, index_oid, current_setting('block_size')::numeric AS bs, CASE WHEN version() ~ 'mingw32' OR version() ~ '64-bit' THEN 8 ELSE 4 END AS maxalign, 24 AS pagehdr, CASE WHEN max(coalesce(s.null_frac,0)) = 0 THEN 2 ELSE 6 END AS index_tuple_hdr, sum( (1-coalesce(s.null_frac, 0)) * coalesce(s.avg_width, 2048) ) AS nulldatawidth FROM pg_attribute AS a JOIN pg_stats AS s ON (quote_ident(s.schemaname) || '.' || quote_ident(s.tablename))::regclass=a.attrelid AND s.attname = a.attname  JOIN btree_index_atts AS i ON i.indrelid = a.attrelid AND a.attnum = i.attnum WHERE a.attnum > 0 GROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9), index_aligned AS (SELECT bs, reltuples, relpages, relam, table_oid, ( 2 + maxalign - CASE WHEN index_tuple_hdr%maxalign = 0 THEN maxalign ELSE index_tuple_hdr%maxalign END + nulldatawidth + maxalign - CASE WHEN nulldatawidth::integer%maxalign = 0 THEN maxalign ELSE nulldatawidth::integer%maxalign END)::numeric AS nulldatahdrwidth, pagehdr FROM index_item_sizes AS s1), otta_calc AS (SELECT bs, table_oid, relpages, coalesce(ceil((reltuples*(4+nulldatahdrwidth))/(bs-pagehdr::float)) + CASE WHEN am.amname IN ('hash','btree') THEN 1 ELSE 0 END , 0) AS otta FROM index_aligned AS s2 LEFT JOIN pg_am am ON s2.relam = am.oid), raw_bloat AS (SELECT CASE WHEN sub.relpages <= otta THEN 0 ELSE bs*(sub.relpages-otta)::bigint END AS wastedbytes FROM otta_calc AS sub JOIN pg_class AS c ON c.oid=sub.table_oid) SELECT wastedbytes as bloat_bytes FROM raw_bloat) as s1) as s2, (SELECT sum(pg_indexes_size(table_name)) as real_size FROM (SELECT ('' || table_schema || '.' || table_name || '') AS table_name FROM information_schema.tables WHERE table_schema='pg_catalog') AS all_tables1) as p1\"\n\n\n  version=901\n\n\n  withdbname=false\n\n\n  tagvalue=\"relation,owner,dbname\"\n\n\n  measurement=\"pg_stat_bloat\"\n\n\n[[inputs.postgresql_extensible.query]]\n\n  \n#Bloating tables user\n\n  \nsqlquery\n=\n\"SELECT 'tables' as relation, 'user' as owner, (SELECT current_database()) as dbname, bloat_size::bigint, real_size::bigint, (bloat_size / real_size)::float as bloat_ratio FROM (SELECT SUM(bloat_bytes) as bloat_size FROM (SELECT  CASE WHEN (tblpages-est_tblpages_ff)*bs > 0 THEN ((tblpages-est_tblpages_ff)*bs)::bigint ELSE 0 END AS bloat_bytes, CASE WHEN tblpages - est_tblpages_ff > 0 THEN round((100 * (tblpages - est_tblpages_ff)/tblpages::float)::numeric, 1) ELSE 0 END AS bloat_ratio FROM (SELECT ceil( reltuples / ( (bs-page_hdr)/tpl_size ) ) + ceil( toasttuples / 4 ) AS est_tblpages, ceil( reltuples / ( (bs-page_hdr)*fillfactor/(tpl_size*100) ) ) + ceil( toasttuples / 4 ) AS est_tblpages_ff, tblpages, bs FROM (SELECT ( 4 + tpl_hdr_size + tpl_data_size + (2*ma) - CASE WHEN tpl_hdr_size%ma = 0 THEN ma ELSE tpl_hdr_size%ma END - CASE WHEN ceil(tpl_data_size)::int%ma = 0 THEN ma ELSE ceil(tpl_data_size)::int%ma END) AS tpl_size, (heappages + toastpages) AS tblpages, reltuples, toasttuples, bs, page_hdr, fillfactor FROM (SELECT ns.nspname AS schemaname, tbl.relname AS tblname, tbl.reltuples, tbl.relpages AS heappages, coalesce(toast.relpages, 0) AS toastpages, coalesce(toast.reltuples, 0) AS toasttuples, coalesce(substring(array_to_string(tbl.reloptions, ' ') FROM '%fillfactor=#\\\"__#\\\"%' FOR '#')::smallint, 100) AS fillfactor, current_setting('block_size')::numeric AS bs, CASE WHEN version()~'mingw32' OR version()~'64-bit|x86_64|ppc64|ia64|amd64' THEN 8 ELSE 4 END AS ma, 24 AS page_hdr, 23 + CASE WHEN MAX(coalesce(null_frac,0)) > 0 THEN ( 7 + count(*) ) / 8 ELSE 0::int END + CASE WHEN tbl.relhasoids THEN 4 ELSE 0 END AS tpl_hdr_size, sum( (1-coalesce(s.null_frac, 0)) * coalesce(s.avg_width, 1024) ) AS tpl_data_size FROM pg_attribute AS att JOIN pg_class AS tbl ON att.attrelid = tbl.oid JOIN pg_namespace AS ns ON ns.oid = tbl.relnamespace JOIN pg_stats AS s ON s.schemaname=ns.nspname AND s.tablename = tbl.relname AND s.inherited=false AND s.attname=att.attname LEFT JOIN pg_class AS toast ON tbl.reltoastrelid = toast.oid WHERE att.attnum > 0 AND NOT att.attisdropped AND tbl.relkind = 'r' AND ns.nspname = 'public' GROUP BY 1,2,3,4,5,6,7,8,9,10, tbl.relhasoids) AS s) AS s2) AS s3) AS s4) as s5, (SELECT sum(pg_table_size(table_name)) as real_size FROM (SELECT ('' || table_schema || '.' || table_name || '') AS table_name FROM information_schema.tables WHERE table_schema='public') AS all_tables1) as p1\"\n\n\n  version=901\n\n\n  withdbname=false\n\n\n  tagvalue=\"relation,owner,dbname\"\n\n\n  measurement=\"pg_stat_bloat\"\n\n\n[[inputs.postgresql_extensible.query]]\n\n  \n#Bloating index user\n\n  \nsqlquery\n=\n\"SELECT 'index' as relation, 'user' as owner, (SELECT current_database()) as dbname, bloat_size::bigint, real_size::bigint, (bloat_size / real_size)::float as bloat_ratio FROM (SELECT SUM(bloat_bytes) as bloat_size FROM (WITH btree_index_atts AS (SELECT nspname, relname, reltuples, relpages, indrelid, relam, regexp_split_to_table(indkey::text, ' ')::smallint AS attnum, indexrelid as index_oid FROM pg_index JOIN pg_class ON pg_class.oid=pg_index.indexrelid JOIN pg_namespace ON pg_namespace.oid = pg_class.relnamespace JOIN pg_am ON pg_class.relam = pg_am.oid WHERE pg_am.amname = 'btree' AND pg_namespace.nspname='public'), index_item_sizes AS (SELECT i.nspname, i.relname, i.reltuples, i.relpages, i.relam, (quote_ident(s.schemaname) || '.' || quote_ident(s.tablename))::regclass AS starelid, a.attrelid AS table_oid, index_oid, current_setting('block_size')::numeric AS bs, CASE WHEN version() ~ 'mingw32' OR version() ~ '64-bit' THEN 8 ELSE 4 END AS maxalign, 24 AS pagehdr, CASE WHEN max(coalesce(s.null_frac,0)) = 0 THEN 2 ELSE 6 END AS index_tuple_hdr, sum( (1-coalesce(s.null_frac, 0)) * coalesce(s.avg_width, 2048) ) AS nulldatawidth FROM pg_attribute AS a JOIN pg_stats AS s ON (quote_ident(s.schemaname) || '.' || quote_ident(s.tablename))::regclass=a.attrelid AND s.attname = a.attname  JOIN btree_index_atts AS i ON i.indrelid = a.attrelid AND a.attnum = i.attnum WHERE a.attnum > 0 GROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9), index_aligned AS (SELECT bs, reltuples, relpages, relam, table_oid, ( 2 + maxalign - CASE WHEN index_tuple_hdr%maxalign = 0 THEN maxalign ELSE index_tuple_hdr%maxalign END + nulldatawidth + maxalign - CASE WHEN nulldatawidth::integer%maxalign = 0 THEN maxalign ELSE nulldatawidth::integer%maxalign END)::numeric AS nulldatahdrwidth, pagehdr FROM index_item_sizes AS s1), otta_calc AS (SELECT bs, table_oid, relpages, coalesce(ceil((reltuples*(4+nulldatahdrwidth))/(bs-pagehdr::float)) + CASE WHEN am.amname IN ('hash','btree') THEN 1 ELSE 0 END , 0) AS otta FROM index_aligned AS s2 LEFT JOIN pg_am am ON s2.relam = am.oid), raw_bloat AS (SELECT CASE WHEN sub.relpages <= otta THEN 0 ELSE bs*(sub.relpages-otta)::bigint END AS wastedbytes FROM otta_calc AS sub JOIN pg_class AS c ON c.oid=sub.table_oid) SELECT wastedbytes as bloat_bytes FROM raw_bloat) as s1) as s2, (SELECT sum(pg_indexes_size(table_name)) as real_size FROM (SELECT ('' || table_schema || '.' || table_name || '') AS table_name FROM information_schema.tables WHERE table_schema='public') AS all_tables1) as p1\"\n\n\n  version=901\n\n\n  withdbname=false\n\n\n  tagvalue=\"relation,owner,dbname\"\n\n\n  measurement=\"pg_stat_bloat\"\n\n\n\n## #######################################################\n\n\n## Cache hit\n\n\n##\n\n\n## Create one new block per database\n\n\n## Change de \"dbName\" in the \"address\" line\n\n\n##\n\n\n[[inputs.postgresql_extensible]]\n\n  \ninterval\n \n=\n \n\"30s\"\n\n\n  address = \"host=10.xx.yy.196 user=my_user password=my_password sslmode=disable dbname=the_db\"\n\n\n[[inputs.postgresql_extensible.query]]\n\n  \n#Cache hits stats\n\n  \nsqlquery\n=\n\"WITH stats_heap AS (SELECT (SELECT sum(heap_blks_read) FROM pg_statio_user_tables)::bigint as heap_read, (SELECT sum(heap_blks_hit) FROM pg_statio_user_tables)::bigint as heap_hit), stats_idx AS (SELECT (SELECT sum(idx_blks_read) FROM pg_statio_user_indexes)::bigint as idx_read, (SELECT sum(idx_blks_hit) FROM pg_statio_user_indexes)::bigint as idx_hit), bs AS (SELECT cast(current_setting('block_size') AS integer) AS block_size) SELECT current_database() AS dbname, (heap_read * block_size)::bigint AS heap_read, (heap_hit * block_size)::bigint AS heap_hit, ((heap_hit::float - heap_read::float) / heap_hit::float)::float as heap_ratio, (idx_read * block_size)::bigint AS idx_read, (idx_hit * block_size)::bigint AS idx_hit, ((idx_hit::float - idx_read::float) / idx_hit::float)::float as idx_ratio FROM bs, stats_heap, stats_idx\"\n\n\n  version=901\n\n\n  withdbname=false\n\n\n  tagvalue=\"dbname\"\n\n\n  measurement=\"pg_stat_cache\"\n\n\n\n## #######################################################\n\n\n## PostgreSQL Server settings\n\n\n##\n\n\n[[inputs.postgresql_extensible]]\n\n  \ninterval\n \n=\n \n\"3600s\"\n\n\n  address = \"host=10.xx.yy.196 user=my_user password=my_password sslmode=disable dbname=postgres\"\n\n\n[[inputs.postgresql_extensible.query]]\n\n  \n#Settings\n\n  \nsqlquery\n=\n\"select name, setting, unit, category from pg_settings\"\n\n\n  version=901\n\n\n  withdbname=false\n\n\n  tagvalue=\"name\"\n\n\n  measurement=\"pg_settings\"\n\n\n\n## #######################################################\n\n\n## Single table statistics\n\n\n##\n\n\n## General stats\n\n\n##\n\n\n[[inputs.postgresql_extensible]]\n\n  \ninterval\n \n=\n \n\"60s\"\n\n\n  address = \"host=10.xx.yy.196 user=my_user password=my_password sslmode=disable dbname=the_db\"\n\n\n[[inputs.postgresql_extensible.query]]\n\n  \nsqlquery\n=\n\"select (SELECT current_database()) as db_name, 'probes_global_statistics' as table_name, seq_scan::bigint, seq_tup_read::bigint, idx_scan::bigint, idx_tup_fetch::bigint as idx_tup_read, n_tup_ins::bigint, n_tup_del::bigint, (n_tup_upd + n_tup_hot_upd)::bigint as n_tup_total_upd, (CASE WHEN (n_tup_hot_upd + n_tup_upd) > 0 THEN n_tup_hot_upd::float / (n_tup_hot_upd + n_tup_upd)::float ELSE 0 END)::float as hot_upd_ratio, n_live_tup::bigint, n_dead_tup::bigint, (n_dead_tup::float / (n_live_tup + n_dead_tup)::float) as bloating_ratio, (vacuum_count + autovacuum_count)::bigint as total_vacuum, (analyze_count + autoanalyze_count)::bigint as total_analyze, EXTRACT(EPOCH FROM (now() - CASE WHEN COALESCE(last_autovacuum, to_timestamp(0)) < COALESCE(last_vacuum, to_timestamp(0)) THEN COALESCE(last_vacuum, to_timestamp(0)) ELSE COALESCE(last_autovacuum, to_timestamp(0)) END)) as duration_since_last_vacuum, EXTRACT(EPOCH FROM (now() - CASE WHEN COALESCE(last_autoanalyze, to_timestamp(0)) < COALESCE(last_analyze, to_timestamp(0)) THEN COALESCE(last_analyze, to_timestamp(0)) ELSE COALESCE(last_autoanalyze, to_timestamp(0)) END)) as duration_since_last_analyze from pg_stat_user_tables where relname='probes_global_statistics'\"\n\n\n  version=901\n\n\n  withdbname=false\n\n\n  tagvalue=\"table_name,db_name\"\n\n\n  measurement=\"pg_stat_single_table\"\n\n\n\n## #######################################################\n\n\n## Single table statistics\n\n\n##\n\n\n## Cache usage\n\n\n##\n\n\n[[inputs.postgresql_extensible]]\n\n  \ninterval\n \n=\n \n\"60s\"\n\n\n  address = \"host=10.xx.yy.196 user=my_user password=my_password sslmode=disable dbname=the_db\"\n\n\n[[inputs.postgresql_extensible.query]]\n\n  \n# single table cache usage\n\n  \nsqlquery\n=\n\"WITH stats_heap AS (SELECT (SELECT heap_blks_read FROM pg_statio_user_tables where relname='probes_global_statistics')::bigint as heap_read, (SELECT heap_blks_hit FROM pg_statio_user_tables where relname='probes_global_statistics')::bigint as heap_hit), stats_idx AS (SELECT (SELECT sum(idx_blks_read) FROM pg_statio_user_indexes where relname='probes_global_statistics')::bigint as idx_read, (SELECT sum(idx_blks_hit) FROM pg_statio_user_indexes where relname='probes_global_statistics')::bigint as idx_hit), bs AS (SELECT cast(current_setting('block_size') AS integer) AS block_size) SELECT (SELECT current_database()) as db_name, 'probes_global_statistics' as table_name, (heap_read * block_size)::bigint AS heap_read, (heap_hit * block_size)::bigint AS heap_hit, (heap_hit::float / (heap_read + heap_hit)::float)::float as heap_ratio, (idx_read * block_size)::bigint AS idx_read, (idx_hit * block_size)::bigint AS idx_hit, (idx_hit::float / (idx_read + idx_hit)::float)::float as idx_ratio FROM bs, stats_heap, stats_idx\"\n\n\n  version=901\n\n\n  withdbname=false\n\n\n  tagvalue=\"table_name,db_name\"\n\n\n  measurement=\"pg_stat_single_table_cache\"\n\n\n\n\n\n\nExample of telegraf configuration :\n\n\n\n\nfor a single database server : \nhere",
            "title": "Monitoring"
        },
        {
            "location": "/monitoring/#monitoring",
            "text": "Explain how to deploy via Docker / configure a TIG stack :  Telegraf ,  Influxdb  and  Grafana .",
            "title": "Monitoring"
        },
        {
            "location": "/monitoring/#architecture",
            "text": "",
            "title": "Architecture"
        },
        {
            "location": "/monitoring/#central-server",
            "text": "",
            "title": "Central Server"
        },
        {
            "location": "/monitoring/#docker-composeyml",
            "text": "version: '2'\n\nservices:\n    ## InfluxDB container\n    influxdb:\n        image: influxdb:latest\n        restart: always\n        ports:\n          - \"80:8086\"\n        volumes:\n          -  ${ PWD } /data/influxdb:/var/lib/influxdb\n          -  ${ PWD } /data/influxdb/influxdb.conf:/etc/influxdb/influxdb.conf:ro\n\n## Grafana container\n    grafana:\n        image: grafana/grafana\n        restart: always\n        ports:\n          - \"3000:3000\"\n        volumes:\n          -  ${ PWD } /data/grafana/var/lib/grafana:/var/lib/grafana\n          -  ${ PWD } /data/grafana/etc/grafana/grafana.ini:/etc/grafana/grafana.ini\n          -  ${ PWD } /data/grafana/var/log/grafana:/var/log/grafana",
            "title": "Docker-compose.yml"
        },
        {
            "location": "/monitoring/#installation",
            "text": "Pick up a monitoring server to host influxDB & Grafana  Pull images  docker pull grafana/grafana\ndocker pull influxdb  Create sub-dirs  mkdir -p /your_home/monitoring/data/grafana/etc/grafana\nmkdir -p /your_home/monitoring/data/grafana/var/lib/grafana\nmkdir -p /your_home/monitoring/data/grafana/var/log/grafana\nmkdir -p /your_home/monitoring/data/influxdb/  Generate default InfluxDB config file  cd /your_home/monitoring\ndocker run --rm influxdb influxd config > data/influxdb/influxdb.conf  Get default grafana.ini file from docker container to host  docker run -d grafana/grafana bash  Find it  docker ps  Go in it  docker exec -it CONTAINER_ID bash  Cat the /etc/grafana/grafana/ini file and copy its content to data/grafana/etc/grafana/grafana.ini file.  Copy the compose yaml (see below)  /your_home/monitoring/docker-compose-central.yml\nmv /your_home/monitoring/docker-compose-central.yml /your_home/monitoring/docker-compose.yml  Start the stack  docker-compose up -d  Here are some grafana dashboards :   PostgreSQL overview  PostgreSQL advanced view  PostgreSQL single table monitoring  Central server (influxdb + grafana) overview",
            "title": "Installation"
        },
        {
            "location": "/monitoring/#control-influxdb",
            "text": "Identify the influxDB container ID with :  docker ps  Then  docker exec -it INFLUX_CONTAINER_ID bash  And   influx  To get the influx cli. Connect to the telegraf db by :  use telegraf  Display the tables (measurements) :  show measurements  Display a measurement content :  select * from the_measurement where time > now() - 1h",
            "title": "Control influxdb"
        },
        {
            "location": "/monitoring/#collector",
            "text": "We have to deploy a telegraf per target server.",
            "title": "Collector"
        },
        {
            "location": "/monitoring/#docker-composeyml_1",
            "text": "version: '2'\n\nservices:\n## Telegraf container\n    telegraf:\n        image: telegraf:latest\n        volumes:\n            # standard config\n            -  ${ PWD } /data/telegraf/telegraf.conf:/etc/telegraf/telegraf.conf:ro\n            # custom config - can't make it workw\n#            -  ${ PWD } /data/telegraf/telegraf-ext/custom.config:/etc/telegraf/telegraf.d/custom.conf\n            - /var/run/docker.sock:/var/run/docker.sock:ro\n        restart: always",
            "title": "Docker-compose.yml"
        },
        {
            "location": "/monitoring/#installation_1",
            "text": "Install Docker / docker-compose if necessary :   install docker\ninstall docker-compose  Configure Docker to access network through proxy for example :  sudo mkdir -p /etc/systemd/system/docker.service.d/\nsudo nano /etc/systemd/system/docker.service.d/docker.conf\n\n#Copy in this file something like\n[Service]\nExecStart=\nEnvironment=\"HTTP_PROXY=the_proxy\"\n\nsudo systemctl daemon-reload\nsudo systemctl restart docker.service  Pull images  docker pull telegraf  Create sub-dirs  mkdir -p /your_home/monitoring/data/telegraf/  Generate default config files  cd /your_home/monitoring\ndocker run --rm telegraf -sample-config > data/telegraf/telegraf.conf  General configuration, edit  telegraf.conf  In [global_tags] part, add :   customer = \"the_customer\"  In [agent] part, set :  interval = \"60s\"\ndebug = true                    # might be useful !\nhostname = \"the_host_name\"    # or whatever  In [[outputs.influxdb]] part, set  urls = [\"http://influx.mymonitoring.com:80\"]\n# If necessary\nhttp_proxy = \"http://corporate.proxy:3128\"  Copy the compose yaml  /your_home/monitoring/docker-compose-telegraf.yml\nmv /your_home/monitoring/docker-compose-telegraf.yml /your_home/monitoring/docker-compose.yml  Start the stack  docker-compose up -d  Check the logs. You should see :  telegraf_1  | 2018/02/05 10:38:37 I! Using config file: /etc/telegraf/telegraf.conf\ntelegraf_1  | 2018-02-05T10:38:37Z D! Attempting connection to output: influxdb\ntelegraf_1  | 2018-02-05T10:38:37Z D! Successfully connected to output: influxdb\ntelegraf_1  | 2018-02-05T10:38:37Z I! Starting Telegraf v1.5.2\ntelegraf_1  | 2018-02-05T10:38:37Z I! Loaded outputs: influxdb\ntelegraf_1  | 2018-02-05T10:38:37Z I! Loaded inputs: inputs.postgresql_extensible inputs.postgresql_extensible inputs.postgresql_extensible inputs.disk inputs.diskio inputs.kernel inputs.mem inputs.postgresql inputs.processes inputs.swap inputs.system inputs.cpu\ntelegraf_1  | 2018-02-05T10:38:37Z I! Tags enabled: customer=the_customer host=the_host_name\ntelegraf_1  | 2018-02-05T10:38:37Z I! Agent Config: Interval:1m0s, Quiet:false, Hostname:\"the_host_name\", Flush Interval:10s  Ajouter des inputs \u00e0 telegraf (change the database IP in each  [[inputs.postgresql]]  block):  [[inputs.postgresql]] \n   interval   =   \"60s\"    ## specify address via a url matching:    ##   postgres://[pqgotest[:password]]@localhost[/dbname]\\    ##       ?sslmode=[disable|verify-ca|verify-full]    ## or a simple string:    ##   host=localhost user=pqotest password=... sslmode=... dbname=app_production    ##    ## All connection parameters are optional.    ##    ## Without the dbname parameter, the driver will default to a database    ## with the same name as the user. This dbname is just for instantiating a    ## connection with the server and doesn't restrict the databases we are trying    ## to grab metrics for.    ##    #address = \"host=10.2.1.188 user=my_user sslmode=disable dbname=postgres\"    address = \"host=10.xx.yy.196 user=my_user password=my_password sslmode=disable dbname=postgres\"    ## A  list of databases to explicitly ignore.  If not specified, metrics for all    ## databases are gathered.  Do NOT use with the 'databases' option.    # ignored_databases = [\"postgres\", \"template0\", \"template1\"]    ignored_databases = [\"postgres\", \"template0\", \"template1\"]    ## A list of databases to pull metrics about. If not specified, metrics for all    ## databases are gathered.  Do NOT use with the 'ignored_databases' option.  ## #######################################################  ## Checkpoints  ##  [[inputs.postgresql_extensible]] \n   interval   =   \"300s\"    address = \"host=10.xx.yy.196 user=my_user password=my_password sslmode=disable dbname=postgres\"  [[inputs.postgresql_extensible.query]] \n   #Checkpoints stats \n   # checkpoints_timeout_pct | checkpoints_req_pct | avg_checkpoints_minutes | avg_checkpoint_write | total_written | checkpoint_write_pct | backend_write_pct | total_checkpoints | seconds_since_start | setting_checkpoint_timeout | setting_max_wal_size  \n   sqlquery = \"SELECT (100*checkpoints_timed) / total_checkpoints as checkpoints_timeout_pct, 100 - (100*checkpoints_timed) / total_checkpoints AS checkpoints_req_pct, seconds_since_start / total_checkpoints / 60 AS avg_checkpoints_minutes, buffers_checkpoint * block_size / (checkpoints_timed + checkpoints_req) AS avg_checkpoint_write, block_size * (buffers_checkpoint + buffers_clean + buffers_backend) AS total_written, 100 * buffers_checkpoint / (buffers_checkpoint + buffers_clean + buffers_backend) AS checkpoint_write_pct, 100 * buffers_backend / (buffers_checkpoint + buffers_clean + buffers_backend) AS backend_write_pct, total_checkpoints,seconds_since_start, (select setting from pg_settings where name = 'checkpoint_timeout') as setting_checkpoint_timeout, (select (setting::bigint) * 16*1024*1024 from pg_settings where name = 'max_wal_size') as setting_max_wal_size FROM (SELECT EXTRACT(EPOCH FROM (now() - stats_reset)) AS seconds_since_start, *, (checkpoints_timed+checkpoints_req) AS total_checkpoints FROM pg_stat_bgwriter WHERE (checkpoints_timed+checkpoints_req)>0) AS sub, (SELECT cast(current_setting('block_size') AS integer) AS block_size) bs\"    version=901    withdbname=false    tagvalue=\"\"    measurement=\"pg_stat_checkpoints\"  ## #######################################################  ## Sizes  ##  ## Create one new block per database  ## Change de \"dbName\" in the \"address\" line  ##  [[inputs.postgresql_extensible]] \n   interval   =   \"1800s\"    address = \"host=10.xx.yy.196 user=my_user password=my_password sslmode=disable dbname=the_db\"  [[inputs.postgresql_extensible.query]] \n   #Size stats \n   sqlquery = \"SELECT (SELECT current_database()) as dbname, sum(pg_table_size(table_name))::bigint AS table_size, sum(pg_indexes_size(table_name))::bigint AS index_size FROM (SELECT ('' || table_schema || '.' || table_name || '') AS table_name FROM information_schema.tables WHERE table_schema='public') AS all_tables1\"    version=901    withdbname=false    tagvalue=\"dbname\"    measurement=\"pg_stat_size\"  ## #######################################################  ## Bloating  ##  ## Create one new block per database  ## Change de \"dbName\" in the \"address\" line  ##   ## CAREFUL : user indexes query could be harmful for the DB load  ##  [[inputs.postgresql_extensible]] \n   interval   =   \"900s\"    address = \"host=10.xx.yy.196 user=my_user password=my_password sslmode=disable dbname=the_db\"  [[inputs.postgresql_extensible.query]] \n   #Bloating tables system \n   sqlquery = \"SELECT 'tables' as relation, 'system' as owner, (SELECT current_database()) as dbname, bloat_size::bigint, real_size::bigint, (bloat_size / real_size)::float as bloat_ratio FROM (SELECT SUM(bloat_bytes) as bloat_size FROM (SELECT  CASE WHEN (tblpages-est_tblpages_ff)*bs > 0 THEN ((tblpages-est_tblpages_ff)*bs)::bigint ELSE 0 END AS bloat_bytes, CASE WHEN tblpages - est_tblpages_ff > 0 THEN round((100 * (tblpages - est_tblpages_ff)/tblpages::float)::numeric, 1) ELSE 0 END AS bloat_ratio FROM (SELECT ceil( reltuples / ( (bs-page_hdr)/tpl_size ) ) + ceil( toasttuples / 4 ) AS est_tblpages, ceil( reltuples / ( (bs-page_hdr)*fillfactor/(tpl_size*100) ) ) + ceil( toasttuples / 4 ) AS est_tblpages_ff, tblpages, bs FROM (SELECT ( 4 + tpl_hdr_size + tpl_data_size + (2*ma) - CASE WHEN tpl_hdr_size%ma = 0 THEN ma ELSE tpl_hdr_size%ma END - CASE WHEN ceil(tpl_data_size)::int%ma = 0 THEN ma ELSE ceil(tpl_data_size)::int%ma END) AS tpl_size, (heappages + toastpages) AS tblpages, reltuples, toasttuples, bs, page_hdr, fillfactor FROM (SELECT ns.nspname AS schemaname, tbl.relname AS tblname, tbl.reltuples, tbl.relpages AS heappages, coalesce(toast.relpages, 0) AS toastpages, coalesce(toast.reltuples, 0) AS toasttuples, coalesce(substring(array_to_string(tbl.reloptions, ' ') FROM '%fillfactor=#\\\"__#\\\"%' FOR '#')::smallint, 100) AS fillfactor, current_setting('block_size')::numeric AS bs, CASE WHEN version()~'mingw32' OR version()~'64-bit|x86_64|ppc64|ia64|amd64' THEN 8 ELSE 4 END AS ma, 24 AS page_hdr, 23 + CASE WHEN MAX(coalesce(null_frac,0)) > 0 THEN ( 7 + count(*) ) / 8 ELSE 0::int END + CASE WHEN tbl.relhasoids THEN 4 ELSE 0 END AS tpl_hdr_size, sum( (1-coalesce(s.null_frac, 0)) * coalesce(s.avg_width, 1024) ) AS tpl_data_size FROM pg_attribute AS att JOIN pg_class AS tbl ON att.attrelid = tbl.oid JOIN pg_namespace AS ns ON ns.oid = tbl.relnamespace JOIN pg_stats AS s ON s.schemaname=ns.nspname AND s.tablename = tbl.relname AND s.inherited=false AND s.attname=att.attname LEFT JOIN pg_class AS toast ON tbl.reltoastrelid = toast.oid WHERE att.attnum > 0 AND NOT att.attisdropped AND tbl.relkind = 'r' AND ns.nspname = 'pg_catalog' GROUP BY 1,2,3,4,5,6,7,8,9,10, tbl.relhasoids) AS s) AS s2) AS s3) AS s4) as s5, (SELECT sum(pg_table_size(table_name)) as real_size FROM (SELECT ('' || table_schema || '.' || table_name || '') AS table_name FROM information_schema.tables WHERE table_schema='pg_catalog') AS all_tables1) as p1\"    version=901    withdbname=false    tagvalue=\"relation,owner,dbname\"    measurement=\"pg_stat_bloat\"  [[inputs.postgresql_extensible.query]] \n   #Bloating index system \n   sqlquery = \"SELECT 'index' as relation, 'system' as owner, (SELECT current_database()) as dbname, bloat_size::bigint, real_size::bigint, (bloat_size / real_size)::float as bloat_ratio FROM (SELECT SUM(bloat_bytes) as bloat_size FROM (WITH btree_index_atts AS (SELECT nspname, relname, reltuples, relpages, indrelid, relam, regexp_split_to_table(indkey::text, ' ')::smallint AS attnum, indexrelid as index_oid FROM pg_index JOIN pg_class ON pg_class.oid=pg_index.indexrelid JOIN pg_namespace ON pg_namespace.oid = pg_class.relnamespace JOIN pg_am ON pg_class.relam = pg_am.oid WHERE pg_am.amname = 'btree' AND pg_namespace.nspname='pg_catalog'), index_item_sizes AS (SELECT i.nspname, i.relname, i.reltuples, i.relpages, i.relam, (quote_ident(s.schemaname) || '.' || quote_ident(s.tablename))::regclass AS starelid, a.attrelid AS table_oid, index_oid, current_setting('block_size')::numeric AS bs, CASE WHEN version() ~ 'mingw32' OR version() ~ '64-bit' THEN 8 ELSE 4 END AS maxalign, 24 AS pagehdr, CASE WHEN max(coalesce(s.null_frac,0)) = 0 THEN 2 ELSE 6 END AS index_tuple_hdr, sum( (1-coalesce(s.null_frac, 0)) * coalesce(s.avg_width, 2048) ) AS nulldatawidth FROM pg_attribute AS a JOIN pg_stats AS s ON (quote_ident(s.schemaname) || '.' || quote_ident(s.tablename))::regclass=a.attrelid AND s.attname = a.attname  JOIN btree_index_atts AS i ON i.indrelid = a.attrelid AND a.attnum = i.attnum WHERE a.attnum > 0 GROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9), index_aligned AS (SELECT bs, reltuples, relpages, relam, table_oid, ( 2 + maxalign - CASE WHEN index_tuple_hdr%maxalign = 0 THEN maxalign ELSE index_tuple_hdr%maxalign END + nulldatawidth + maxalign - CASE WHEN nulldatawidth::integer%maxalign = 0 THEN maxalign ELSE nulldatawidth::integer%maxalign END)::numeric AS nulldatahdrwidth, pagehdr FROM index_item_sizes AS s1), otta_calc AS (SELECT bs, table_oid, relpages, coalesce(ceil((reltuples*(4+nulldatahdrwidth))/(bs-pagehdr::float)) + CASE WHEN am.amname IN ('hash','btree') THEN 1 ELSE 0 END , 0) AS otta FROM index_aligned AS s2 LEFT JOIN pg_am am ON s2.relam = am.oid), raw_bloat AS (SELECT CASE WHEN sub.relpages <= otta THEN 0 ELSE bs*(sub.relpages-otta)::bigint END AS wastedbytes FROM otta_calc AS sub JOIN pg_class AS c ON c.oid=sub.table_oid) SELECT wastedbytes as bloat_bytes FROM raw_bloat) as s1) as s2, (SELECT sum(pg_indexes_size(table_name)) as real_size FROM (SELECT ('' || table_schema || '.' || table_name || '') AS table_name FROM information_schema.tables WHERE table_schema='pg_catalog') AS all_tables1) as p1\"    version=901    withdbname=false    tagvalue=\"relation,owner,dbname\"    measurement=\"pg_stat_bloat\"  [[inputs.postgresql_extensible.query]] \n   #Bloating tables user \n   sqlquery = \"SELECT 'tables' as relation, 'user' as owner, (SELECT current_database()) as dbname, bloat_size::bigint, real_size::bigint, (bloat_size / real_size)::float as bloat_ratio FROM (SELECT SUM(bloat_bytes) as bloat_size FROM (SELECT  CASE WHEN (tblpages-est_tblpages_ff)*bs > 0 THEN ((tblpages-est_tblpages_ff)*bs)::bigint ELSE 0 END AS bloat_bytes, CASE WHEN tblpages - est_tblpages_ff > 0 THEN round((100 * (tblpages - est_tblpages_ff)/tblpages::float)::numeric, 1) ELSE 0 END AS bloat_ratio FROM (SELECT ceil( reltuples / ( (bs-page_hdr)/tpl_size ) ) + ceil( toasttuples / 4 ) AS est_tblpages, ceil( reltuples / ( (bs-page_hdr)*fillfactor/(tpl_size*100) ) ) + ceil( toasttuples / 4 ) AS est_tblpages_ff, tblpages, bs FROM (SELECT ( 4 + tpl_hdr_size + tpl_data_size + (2*ma) - CASE WHEN tpl_hdr_size%ma = 0 THEN ma ELSE tpl_hdr_size%ma END - CASE WHEN ceil(tpl_data_size)::int%ma = 0 THEN ma ELSE ceil(tpl_data_size)::int%ma END) AS tpl_size, (heappages + toastpages) AS tblpages, reltuples, toasttuples, bs, page_hdr, fillfactor FROM (SELECT ns.nspname AS schemaname, tbl.relname AS tblname, tbl.reltuples, tbl.relpages AS heappages, coalesce(toast.relpages, 0) AS toastpages, coalesce(toast.reltuples, 0) AS toasttuples, coalesce(substring(array_to_string(tbl.reloptions, ' ') FROM '%fillfactor=#\\\"__#\\\"%' FOR '#')::smallint, 100) AS fillfactor, current_setting('block_size')::numeric AS bs, CASE WHEN version()~'mingw32' OR version()~'64-bit|x86_64|ppc64|ia64|amd64' THEN 8 ELSE 4 END AS ma, 24 AS page_hdr, 23 + CASE WHEN MAX(coalesce(null_frac,0)) > 0 THEN ( 7 + count(*) ) / 8 ELSE 0::int END + CASE WHEN tbl.relhasoids THEN 4 ELSE 0 END AS tpl_hdr_size, sum( (1-coalesce(s.null_frac, 0)) * coalesce(s.avg_width, 1024) ) AS tpl_data_size FROM pg_attribute AS att JOIN pg_class AS tbl ON att.attrelid = tbl.oid JOIN pg_namespace AS ns ON ns.oid = tbl.relnamespace JOIN pg_stats AS s ON s.schemaname=ns.nspname AND s.tablename = tbl.relname AND s.inherited=false AND s.attname=att.attname LEFT JOIN pg_class AS toast ON tbl.reltoastrelid = toast.oid WHERE att.attnum > 0 AND NOT att.attisdropped AND tbl.relkind = 'r' AND ns.nspname = 'public' GROUP BY 1,2,3,4,5,6,7,8,9,10, tbl.relhasoids) AS s) AS s2) AS s3) AS s4) as s5, (SELECT sum(pg_table_size(table_name)) as real_size FROM (SELECT ('' || table_schema || '.' || table_name || '') AS table_name FROM information_schema.tables WHERE table_schema='public') AS all_tables1) as p1\"    version=901    withdbname=false    tagvalue=\"relation,owner,dbname\"    measurement=\"pg_stat_bloat\"  [[inputs.postgresql_extensible.query]] \n   #Bloating index user \n   sqlquery = \"SELECT 'index' as relation, 'user' as owner, (SELECT current_database()) as dbname, bloat_size::bigint, real_size::bigint, (bloat_size / real_size)::float as bloat_ratio FROM (SELECT SUM(bloat_bytes) as bloat_size FROM (WITH btree_index_atts AS (SELECT nspname, relname, reltuples, relpages, indrelid, relam, regexp_split_to_table(indkey::text, ' ')::smallint AS attnum, indexrelid as index_oid FROM pg_index JOIN pg_class ON pg_class.oid=pg_index.indexrelid JOIN pg_namespace ON pg_namespace.oid = pg_class.relnamespace JOIN pg_am ON pg_class.relam = pg_am.oid WHERE pg_am.amname = 'btree' AND pg_namespace.nspname='public'), index_item_sizes AS (SELECT i.nspname, i.relname, i.reltuples, i.relpages, i.relam, (quote_ident(s.schemaname) || '.' || quote_ident(s.tablename))::regclass AS starelid, a.attrelid AS table_oid, index_oid, current_setting('block_size')::numeric AS bs, CASE WHEN version() ~ 'mingw32' OR version() ~ '64-bit' THEN 8 ELSE 4 END AS maxalign, 24 AS pagehdr, CASE WHEN max(coalesce(s.null_frac,0)) = 0 THEN 2 ELSE 6 END AS index_tuple_hdr, sum( (1-coalesce(s.null_frac, 0)) * coalesce(s.avg_width, 2048) ) AS nulldatawidth FROM pg_attribute AS a JOIN pg_stats AS s ON (quote_ident(s.schemaname) || '.' || quote_ident(s.tablename))::regclass=a.attrelid AND s.attname = a.attname  JOIN btree_index_atts AS i ON i.indrelid = a.attrelid AND a.attnum = i.attnum WHERE a.attnum > 0 GROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9), index_aligned AS (SELECT bs, reltuples, relpages, relam, table_oid, ( 2 + maxalign - CASE WHEN index_tuple_hdr%maxalign = 0 THEN maxalign ELSE index_tuple_hdr%maxalign END + nulldatawidth + maxalign - CASE WHEN nulldatawidth::integer%maxalign = 0 THEN maxalign ELSE nulldatawidth::integer%maxalign END)::numeric AS nulldatahdrwidth, pagehdr FROM index_item_sizes AS s1), otta_calc AS (SELECT bs, table_oid, relpages, coalesce(ceil((reltuples*(4+nulldatahdrwidth))/(bs-pagehdr::float)) + CASE WHEN am.amname IN ('hash','btree') THEN 1 ELSE 0 END , 0) AS otta FROM index_aligned AS s2 LEFT JOIN pg_am am ON s2.relam = am.oid), raw_bloat AS (SELECT CASE WHEN sub.relpages <= otta THEN 0 ELSE bs*(sub.relpages-otta)::bigint END AS wastedbytes FROM otta_calc AS sub JOIN pg_class AS c ON c.oid=sub.table_oid) SELECT wastedbytes as bloat_bytes FROM raw_bloat) as s1) as s2, (SELECT sum(pg_indexes_size(table_name)) as real_size FROM (SELECT ('' || table_schema || '.' || table_name || '') AS table_name FROM information_schema.tables WHERE table_schema='public') AS all_tables1) as p1\"    version=901    withdbname=false    tagvalue=\"relation,owner,dbname\"    measurement=\"pg_stat_bloat\"  ## #######################################################  ## Cache hit  ##  ## Create one new block per database  ## Change de \"dbName\" in the \"address\" line  ##  [[inputs.postgresql_extensible]] \n   interval   =   \"30s\"    address = \"host=10.xx.yy.196 user=my_user password=my_password sslmode=disable dbname=the_db\"  [[inputs.postgresql_extensible.query]] \n   #Cache hits stats \n   sqlquery = \"WITH stats_heap AS (SELECT (SELECT sum(heap_blks_read) FROM pg_statio_user_tables)::bigint as heap_read, (SELECT sum(heap_blks_hit) FROM pg_statio_user_tables)::bigint as heap_hit), stats_idx AS (SELECT (SELECT sum(idx_blks_read) FROM pg_statio_user_indexes)::bigint as idx_read, (SELECT sum(idx_blks_hit) FROM pg_statio_user_indexes)::bigint as idx_hit), bs AS (SELECT cast(current_setting('block_size') AS integer) AS block_size) SELECT current_database() AS dbname, (heap_read * block_size)::bigint AS heap_read, (heap_hit * block_size)::bigint AS heap_hit, ((heap_hit::float - heap_read::float) / heap_hit::float)::float as heap_ratio, (idx_read * block_size)::bigint AS idx_read, (idx_hit * block_size)::bigint AS idx_hit, ((idx_hit::float - idx_read::float) / idx_hit::float)::float as idx_ratio FROM bs, stats_heap, stats_idx\"    version=901    withdbname=false    tagvalue=\"dbname\"    measurement=\"pg_stat_cache\"  ## #######################################################  ## PostgreSQL Server settings  ##  [[inputs.postgresql_extensible]] \n   interval   =   \"3600s\"    address = \"host=10.xx.yy.196 user=my_user password=my_password sslmode=disable dbname=postgres\"  [[inputs.postgresql_extensible.query]] \n   #Settings \n   sqlquery = \"select name, setting, unit, category from pg_settings\"    version=901    withdbname=false    tagvalue=\"name\"    measurement=\"pg_settings\"  ## #######################################################  ## Single table statistics  ##  ## General stats  ##  [[inputs.postgresql_extensible]] \n   interval   =   \"60s\"    address = \"host=10.xx.yy.196 user=my_user password=my_password sslmode=disable dbname=the_db\"  [[inputs.postgresql_extensible.query]] \n   sqlquery = \"select (SELECT current_database()) as db_name, 'probes_global_statistics' as table_name, seq_scan::bigint, seq_tup_read::bigint, idx_scan::bigint, idx_tup_fetch::bigint as idx_tup_read, n_tup_ins::bigint, n_tup_del::bigint, (n_tup_upd + n_tup_hot_upd)::bigint as n_tup_total_upd, (CASE WHEN (n_tup_hot_upd + n_tup_upd) > 0 THEN n_tup_hot_upd::float / (n_tup_hot_upd + n_tup_upd)::float ELSE 0 END)::float as hot_upd_ratio, n_live_tup::bigint, n_dead_tup::bigint, (n_dead_tup::float / (n_live_tup + n_dead_tup)::float) as bloating_ratio, (vacuum_count + autovacuum_count)::bigint as total_vacuum, (analyze_count + autoanalyze_count)::bigint as total_analyze, EXTRACT(EPOCH FROM (now() - CASE WHEN COALESCE(last_autovacuum, to_timestamp(0)) < COALESCE(last_vacuum, to_timestamp(0)) THEN COALESCE(last_vacuum, to_timestamp(0)) ELSE COALESCE(last_autovacuum, to_timestamp(0)) END)) as duration_since_last_vacuum, EXTRACT(EPOCH FROM (now() - CASE WHEN COALESCE(last_autoanalyze, to_timestamp(0)) < COALESCE(last_analyze, to_timestamp(0)) THEN COALESCE(last_analyze, to_timestamp(0)) ELSE COALESCE(last_autoanalyze, to_timestamp(0)) END)) as duration_since_last_analyze from pg_stat_user_tables where relname='probes_global_statistics'\"    version=901    withdbname=false    tagvalue=\"table_name,db_name\"    measurement=\"pg_stat_single_table\"  ## #######################################################  ## Single table statistics  ##  ## Cache usage  ##  [[inputs.postgresql_extensible]] \n   interval   =   \"60s\"    address = \"host=10.xx.yy.196 user=my_user password=my_password sslmode=disable dbname=the_db\"  [[inputs.postgresql_extensible.query]] \n   # single table cache usage \n   sqlquery = \"WITH stats_heap AS (SELECT (SELECT heap_blks_read FROM pg_statio_user_tables where relname='probes_global_statistics')::bigint as heap_read, (SELECT heap_blks_hit FROM pg_statio_user_tables where relname='probes_global_statistics')::bigint as heap_hit), stats_idx AS (SELECT (SELECT sum(idx_blks_read) FROM pg_statio_user_indexes where relname='probes_global_statistics')::bigint as idx_read, (SELECT sum(idx_blks_hit) FROM pg_statio_user_indexes where relname='probes_global_statistics')::bigint as idx_hit), bs AS (SELECT cast(current_setting('block_size') AS integer) AS block_size) SELECT (SELECT current_database()) as db_name, 'probes_global_statistics' as table_name, (heap_read * block_size)::bigint AS heap_read, (heap_hit * block_size)::bigint AS heap_hit, (heap_hit::float / (heap_read + heap_hit)::float)::float as heap_ratio, (idx_read * block_size)::bigint AS idx_read, (idx_hit * block_size)::bigint AS idx_hit, (idx_hit::float / (idx_read + idx_hit)::float)::float as idx_ratio FROM bs, stats_heap, stats_idx\"    version=901    withdbname=false    tagvalue=\"table_name,db_name\"    measurement=\"pg_stat_single_table_cache\"   Example of telegraf configuration :   for a single database server :  here",
            "title": "Installation"
        },
        {
            "location": "/pgsql_perf/",
            "text": "Performance with PostgreSQL\n\u00b6\n\n\nTuning the server is worth as the expected activity is not standard : heavy workload, bulk reads, high number of clients, ...\n\n\nStandard advices and understanding to tune PostgreSQL for performance could be found \nhere\n, written by the great Greg Smith itself.\n\n\nA friend of him also answer \nthis post\n with great details !\n\n\nGood advices also \nthere\n.\n\n\nLevel of optimizations are mainly four :\n\n\n\n\nOS, hardware\n\n\nPostgreSQL configuration\n\n\ndatabase design\n\n\nsoftware, model and queries\n\n\n\n\nTuning PostgreSQL\n\u00b6\n\n\nUse separate disks\n\u00b6\n\n\nIt's important to use separate disks for :\n\n\n\n\ndata : \ndata_directory = '/var/lib/postgresql/9.6/main'\n in \npostgresql.conf\n\n\nwal : only way by symbolic link ?\n\n\nwal archives if activated\n\n\nlogs : only way by symbolic link ?\n\n\n\n\nSynchronous commit\n\u00b6\n\n\npostgresql.conf\n's parameter \nsynchronous_commit=on\n define the commit method used. If ON, it waits for WAL files to be written to disk before acknowledge client query. Depending on HDD type, it could introduce some latency.\n\n\nSetting this parameter do not harm data coherence but in case of server crash before last commit were written to disk, they could be lost. It doesn't matter in most of cases !\n\n\nSee this \nblog post\n for details.\n\n\nOther citation : \nThis is one of the most known Postgres performance tweaks\n.   \n\n\nWAL write method\n\u00b6\n\n\nThe tool \npg_test_fsync\n packaged with PostgreSQL could be useful to verify if the default value for the \npostgres.conf\n parameter \nwal_sync_method\n is the more efficient (\nfdatasync\n by default on Linux OS).\n\n\nThe tool will execute some checks to output write stats with all avalaible methods (see \ndetails\n) :\n\n\n./pg_test_fsync\n\n5 seconds per test\nO_DIRECT supported on this platform for open_datasync and open_sync.\n\nCompare file sync methods using one 8kB write:\n(in wal_sync_method preference order, except fdatasync is Linux's default)\n        open_datasync                      1002.075 ops/sec     998 usecs/op\n        fdatasync                          1062.147 ops/sec     941 usecs/op\n        fsync                               597.879 ops/sec    1673 usecs/op\n        fsync_writethrough                            n/a\n        open_sync                            34.400 ops/sec   29070 usecs/op\n\nCompare file sync methods using two 8kB writes:\n(in wal_sync_method preference order, except fdatasync is Linux's default)\n        open_datasync                       688.196 ops/sec    1453 usecs/op\n        fdatasync                          1045.286 ops/sec     957 usecs/op\n        fsync                               588.164 ops/sec    1700 usecs/op\n        fsync_writethrough                            n/a\n        open_sync                            25.560 ops/sec   39123 usecs/op\n\nCompare open_sync with different write sizes:\n(This is designed to compare the cost of writing 16kB in different write\nopen_sync sizes.)\n         1 * 16kB open_sync write            46.153 ops/sec   21667 usecs/op\n         2 *  8kB open_sync writes           16.677 ops/sec   59964 usecs/op\n         4 *  4kB open_sync writes            2.985 ops/sec  334965 usecs/op\n         8 *  2kB open_sync writes            1.169 ops/sec  855245 usecs/op\n        16 *  1kB open_sync writes            0.350 ops/sec  2860294 usecs/op\n\nTest if fsync on non-write file descriptor is honored:\n(If the times are similar, fsync() can sync data written on a different\ndescriptor.)\n        write, fsync, close                 626.905 ops/sec    1595 usecs/op\n        write, close, fsync                 681.045 ops/sec    1468 usecs/op\n\nNon-sync'ed 8kB writes:\n        write                            357014.644 ops/sec       3 usecs/op\n\n\n\n\n\nMastering Checkpoints\n\u00b6\n\n\n\n\nNote\n\n\nWhat is it ? Some \ndetails\n.\n\n\n\n\nDefault value of 5mn for checkpoints and 0.5 for the available fraction of this time to do the writing is not good for heavy workload usage.\n\n\nWe must set the checkpoint time to at least 30mn and maybe a lot of more, 90mn can be good. \n\n\nAnyway, once this time is decided, we have to know the amount of produced WAL files. \n\n\nImagine we would like to set the value as :\n\n\ncheckpoint_timeout = 30min\n\n\n\n\n\nLet's execute the following queries to get basic knowledge about the server activity :\n\n\npostgres=# SELECT pg_current_xlog_insert_location();\n pg_current_xlog_insert_location \n---------------------------------\n 3D/B4020A58\n(1 row)\n\n... after 5 minutes ...\n\npostgres=# SELECT pg_current_xlog_insert_location();\n pg_current_xlog_insert_location \n---------------------------------\n 3E/2203E0F8\n(1 row)\n\npostgres=# SELECT pg_xlog_location_diff('3E/2203E0F8', '3D/B4020A58');\n pg_xlog_location_diff \n-----------------------\n            1845614240\n(1 row)\n\n\n\n\n\nSo, every 5mn, ~1.8GB of WAL files is produced, that is 10GB for 30mn. But the \nmax_wal_size\n must be set for 3 checkpoints, so tune it to \n30GB\n.\n\n\nIt's a good habit to let the checkpoint a good amount of time to write its stuff to disk. Evaluate the \ncheckpoint_completion_target\n value as :\n\n\ncheckpoint_completion_target = (checkpoint_timeout - 2min) / checkpoint_timeout\nAnd no more then 0.9\n\n\n\n\n\nFor 30mn checkpoints, it's 0.9, that means checkpoint process have 0.9*30=27mn to write data befroe complete the checkpoint.\n\n\nFinally, activate the log to monitor the checkpoint activity :\n\n\nlog_checkpoint = on\n\n\n\n\n\nBy the way, there are many writing methods that could be used for WAL files. See \nhere\n to find the best. The \nwal_sync_method\n parameter could be verified or changed by this way.\n\n\n\n\nNote\n\n\nMonitor checkpoints \nrunning statistics\n to ensure the behavior is under control.\n\n\n\n\nAutovacuum\n\u00b6\n\n\n\n\nNote\n\n\nWhat is it ? Some \ndetails\n.\n\n\n\n\nGood article\n about tuning autovacuum, again in 2nd quadrant blog !\n\n\nRemember standard configuration of autovacuum stands for standard usages. It needs to be tuned for specific purposes.\n\n\nMany risks with autovacuum :\n\n\n\n\ntoo many writing in DB so autovacuum could not free space enough quick,\n\n\nsome tables have specific size and design that make the autovacuum job hard on them (like for timeseries). We could then observe \ntable bloating\n,\n\n\nbloating could be observed even on catalog tables (especially if there are a huge number of tables in schema) !\n\n\nand many others I don't know !\n\n\n\n\nAutovacuum parameters are :\n\n\n\n\nautovacuum_max_workers\n : (3 default) Could be increase for aggressive vacuum\n\n\nautovacuum_naptime\n : (1min default) minimum delay between autovacuum runs on any given database. Could be decrease for aggressive vacuum\n\n\nautovacuum_vacuum_threshold\n / \nautovacuum_vacuum_scale_factor\n : implied in autovacuum \ntrigger condition\n\n\nautovacuum_analyze_threshold\n / \nautovacuum_analyze_scale_factor\n : implied in autoanalyze \ntrigger condition\n\n\nautovacuum_vacuum_cost_delay\n : (20ms default) delay the process will sleep when the cost limit has been exceeded. Could be decrease for aggressive vacuum\n\n\nautovacuum_vacuum_cost_limit\n : (-1 means use vacuum_cost_limit value which is 200 default) arbitrary cost vacuums should not exceed. Coud be increase for aggressive vacuum\n\n\n\n\nOn big tables, default behavior should be customized table by table : \n\n\n-- reduce the scale factor (default is 0.2 , that is 20%)\n\n\nALTER\n \nTABLE\n \n<\ntablename\n>\n \nSET\n \nautovacuum_vacuum_scale_factor\n \n=\n \n0\n.\n01\n;\n\n\n\n\n\n\nFor a million-row table this means autovacuum would start after ten thousand rows are invalidated rather than two hundred thousand. It helps stopping bloat from getting out of control.\n\n\nEven more aggresive for some of the largest tables :\n\n\nALTER TABLE table_name SET (autovacuum_vacuum_scale_factor = 0.0);\nALTER TABLE table_name SET (autovacuum_vacuum_threshold = 5000);\nALTER TABLE table_name SET (autovacuum_analyze_scale_factor = 0.0);\nALTER TABLE table_name SET (autovacuum_analyze_threshold = 5000);\n\n\n\n\n\nAutovacuum will be triggered every 5000 inserts, updates, or deletes.\n\n\n\n\nAdvice\n\n\nIt's better autovacuum work more often and does less work than less often and does more work.\n\n\n\n\n\n\nNote\n\n\nMonitor \nvacuum efficiency\n to ensure the behavior is under control.\n\n\n\n\nAnalyze\n\u00b6\n\n\n\n\nNote\n\n\nWhat is it ? Some \ndetails\n.\n\n\n\n\nThe default behavior of automatic process rely only on how many row have been inserted or updated, so depending of chat kind of data each column keep, it could be interesting to launch manual ANALYZE in some specific cases.\n\n\nANALYZE behavior could be tuned by changing the \ndefault_statistics_target = 100\n parameter which tells the analyze process how many rows of each table it must sample to compute stats. The greater this number is, the better stats are but it costs more in computation ! \n\n\n\n\nNote\n\n\nMonitor \nanalyze efficiency\n\n\n\n\nShared buffers\n\u00b6\n\n\nAn excellent example of analyzing stats from background writer / checkpoint and shared buffer could be found in \nthis post\n.\n\n\nShared buffers should be :\n\n\n\n\nlarge enough to ensure the checkpointer process is mainly responsible for the buffer writing to disk (monitor who does the write)\n\n\nnot too large so checkpoint could handle the writing in the time allowed\n\n\n\n\nThe basic tuning is 25% of the RAM on a dedicated server.\n\n\nBut how compute the more efficient RAM size it needs ?\n\n\nQuery planner\n\u00b6\n\n\nThe parameter \nrandom_page_cost\n sets the planner's estimate of the cost of a non-sequentially-fetched disk page , like fetching blocks using the index.\n\n\nIts default value of 4.0 is quite high for modern storage system.\n\n\nSee \nPostgreSQL Wiki\n for more details. The official \nPostgreSQL documentation\n page tells to also tune the other parameter \nseq_page_cost\n mostly to the same value.  \n\n\nTuning Linux\n\u00b6\n\n\nSee details on \nthe dedicated page\n.",
            "title": "Performance with PostgreSQL"
        },
        {
            "location": "/pgsql_perf/#performance-with-postgresql",
            "text": "Tuning the server is worth as the expected activity is not standard : heavy workload, bulk reads, high number of clients, ...  Standard advices and understanding to tune PostgreSQL for performance could be found  here , written by the great Greg Smith itself.  A friend of him also answer  this post  with great details !  Good advices also  there .  Level of optimizations are mainly four :   OS, hardware  PostgreSQL configuration  database design  software, model and queries",
            "title": "Performance with PostgreSQL"
        },
        {
            "location": "/pgsql_perf/#tuning-postgresql",
            "text": "",
            "title": "Tuning PostgreSQL"
        },
        {
            "location": "/pgsql_perf/#use-separate-disks",
            "text": "It's important to use separate disks for :   data :  data_directory = '/var/lib/postgresql/9.6/main'  in  postgresql.conf  wal : only way by symbolic link ?  wal archives if activated  logs : only way by symbolic link ?",
            "title": "Use separate disks"
        },
        {
            "location": "/pgsql_perf/#synchronous-commit",
            "text": "postgresql.conf 's parameter  synchronous_commit=on  define the commit method used. If ON, it waits for WAL files to be written to disk before acknowledge client query. Depending on HDD type, it could introduce some latency.  Setting this parameter do not harm data coherence but in case of server crash before last commit were written to disk, they could be lost. It doesn't matter in most of cases !  See this  blog post  for details.  Other citation :  This is one of the most known Postgres performance tweaks .",
            "title": "Synchronous commit"
        },
        {
            "location": "/pgsql_perf/#wal-write-method",
            "text": "The tool  pg_test_fsync  packaged with PostgreSQL could be useful to verify if the default value for the  postgres.conf  parameter  wal_sync_method  is the more efficient ( fdatasync  by default on Linux OS).  The tool will execute some checks to output write stats with all avalaible methods (see  details ) :  ./pg_test_fsync\n\n5 seconds per test\nO_DIRECT supported on this platform for open_datasync and open_sync.\n\nCompare file sync methods using one 8kB write:\n(in wal_sync_method preference order, except fdatasync is Linux's default)\n        open_datasync                      1002.075 ops/sec     998 usecs/op\n        fdatasync                          1062.147 ops/sec     941 usecs/op\n        fsync                               597.879 ops/sec    1673 usecs/op\n        fsync_writethrough                            n/a\n        open_sync                            34.400 ops/sec   29070 usecs/op\n\nCompare file sync methods using two 8kB writes:\n(in wal_sync_method preference order, except fdatasync is Linux's default)\n        open_datasync                       688.196 ops/sec    1453 usecs/op\n        fdatasync                          1045.286 ops/sec     957 usecs/op\n        fsync                               588.164 ops/sec    1700 usecs/op\n        fsync_writethrough                            n/a\n        open_sync                            25.560 ops/sec   39123 usecs/op\n\nCompare open_sync with different write sizes:\n(This is designed to compare the cost of writing 16kB in different write\nopen_sync sizes.)\n         1 * 16kB open_sync write            46.153 ops/sec   21667 usecs/op\n         2 *  8kB open_sync writes           16.677 ops/sec   59964 usecs/op\n         4 *  4kB open_sync writes            2.985 ops/sec  334965 usecs/op\n         8 *  2kB open_sync writes            1.169 ops/sec  855245 usecs/op\n        16 *  1kB open_sync writes            0.350 ops/sec  2860294 usecs/op\n\nTest if fsync on non-write file descriptor is honored:\n(If the times are similar, fsync() can sync data written on a different\ndescriptor.)\n        write, fsync, close                 626.905 ops/sec    1595 usecs/op\n        write, close, fsync                 681.045 ops/sec    1468 usecs/op\n\nNon-sync'ed 8kB writes:\n        write                            357014.644 ops/sec       3 usecs/op",
            "title": "WAL write method"
        },
        {
            "location": "/pgsql_perf/#mastering-checkpoints",
            "text": "Note  What is it ? Some  details .   Default value of 5mn for checkpoints and 0.5 for the available fraction of this time to do the writing is not good for heavy workload usage.  We must set the checkpoint time to at least 30mn and maybe a lot of more, 90mn can be good.   Anyway, once this time is decided, we have to know the amount of produced WAL files.   Imagine we would like to set the value as :  checkpoint_timeout = 30min  Let's execute the following queries to get basic knowledge about the server activity :  postgres=# SELECT pg_current_xlog_insert_location();\n pg_current_xlog_insert_location \n---------------------------------\n 3D/B4020A58\n(1 row)\n\n... after 5 minutes ...\n\npostgres=# SELECT pg_current_xlog_insert_location();\n pg_current_xlog_insert_location \n---------------------------------\n 3E/2203E0F8\n(1 row)\n\npostgres=# SELECT pg_xlog_location_diff('3E/2203E0F8', '3D/B4020A58');\n pg_xlog_location_diff \n-----------------------\n            1845614240\n(1 row)  So, every 5mn, ~1.8GB of WAL files is produced, that is 10GB for 30mn. But the  max_wal_size  must be set for 3 checkpoints, so tune it to  30GB .  It's a good habit to let the checkpoint a good amount of time to write its stuff to disk. Evaluate the  checkpoint_completion_target  value as :  checkpoint_completion_target = (checkpoint_timeout - 2min) / checkpoint_timeout\nAnd no more then 0.9  For 30mn checkpoints, it's 0.9, that means checkpoint process have 0.9*30=27mn to write data befroe complete the checkpoint.  Finally, activate the log to monitor the checkpoint activity :  log_checkpoint = on  By the way, there are many writing methods that could be used for WAL files. See  here  to find the best. The  wal_sync_method  parameter could be verified or changed by this way.   Note  Monitor checkpoints  running statistics  to ensure the behavior is under control.",
            "title": "Mastering Checkpoints"
        },
        {
            "location": "/pgsql_perf/#autovacuum",
            "text": "Note  What is it ? Some  details .   Good article  about tuning autovacuum, again in 2nd quadrant blog !  Remember standard configuration of autovacuum stands for standard usages. It needs to be tuned for specific purposes.  Many risks with autovacuum :   too many writing in DB so autovacuum could not free space enough quick,  some tables have specific size and design that make the autovacuum job hard on them (like for timeseries). We could then observe  table bloating ,  bloating could be observed even on catalog tables (especially if there are a huge number of tables in schema) !  and many others I don't know !   Autovacuum parameters are :   autovacuum_max_workers  : (3 default) Could be increase for aggressive vacuum  autovacuum_naptime  : (1min default) minimum delay between autovacuum runs on any given database. Could be decrease for aggressive vacuum  autovacuum_vacuum_threshold  /  autovacuum_vacuum_scale_factor  : implied in autovacuum  trigger condition  autovacuum_analyze_threshold  /  autovacuum_analyze_scale_factor  : implied in autoanalyze  trigger condition  autovacuum_vacuum_cost_delay  : (20ms default) delay the process will sleep when the cost limit has been exceeded. Could be decrease for aggressive vacuum  autovacuum_vacuum_cost_limit  : (-1 means use vacuum_cost_limit value which is 200 default) arbitrary cost vacuums should not exceed. Coud be increase for aggressive vacuum   On big tables, default behavior should be customized table by table :   -- reduce the scale factor (default is 0.2 , that is 20%)  ALTER   TABLE   < tablename >   SET   autovacuum_vacuum_scale_factor   =   0 . 01 ;   For a million-row table this means autovacuum would start after ten thousand rows are invalidated rather than two hundred thousand. It helps stopping bloat from getting out of control.  Even more aggresive for some of the largest tables :  ALTER TABLE table_name SET (autovacuum_vacuum_scale_factor = 0.0);\nALTER TABLE table_name SET (autovacuum_vacuum_threshold = 5000);\nALTER TABLE table_name SET (autovacuum_analyze_scale_factor = 0.0);\nALTER TABLE table_name SET (autovacuum_analyze_threshold = 5000);  Autovacuum will be triggered every 5000 inserts, updates, or deletes.   Advice  It's better autovacuum work more often and does less work than less often and does more work.    Note  Monitor  vacuum efficiency  to ensure the behavior is under control.",
            "title": "Autovacuum"
        },
        {
            "location": "/pgsql_perf/#analyze",
            "text": "Note  What is it ? Some  details .   The default behavior of automatic process rely only on how many row have been inserted or updated, so depending of chat kind of data each column keep, it could be interesting to launch manual ANALYZE in some specific cases.  ANALYZE behavior could be tuned by changing the  default_statistics_target = 100  parameter which tells the analyze process how many rows of each table it must sample to compute stats. The greater this number is, the better stats are but it costs more in computation !    Note  Monitor  analyze efficiency",
            "title": "Analyze"
        },
        {
            "location": "/pgsql_perf/#shared-buffers",
            "text": "An excellent example of analyzing stats from background writer / checkpoint and shared buffer could be found in  this post .  Shared buffers should be :   large enough to ensure the checkpointer process is mainly responsible for the buffer writing to disk (monitor who does the write)  not too large so checkpoint could handle the writing in the time allowed   The basic tuning is 25% of the RAM on a dedicated server.  But how compute the more efficient RAM size it needs ?",
            "title": "Shared buffers"
        },
        {
            "location": "/pgsql_perf/#query-planner",
            "text": "The parameter  random_page_cost  sets the planner's estimate of the cost of a non-sequentially-fetched disk page , like fetching blocks using the index.  Its default value of 4.0 is quite high for modern storage system.  See  PostgreSQL Wiki  for more details. The official  PostgreSQL documentation  page tells to also tune the other parameter  seq_page_cost  mostly to the same value.",
            "title": "Query planner"
        },
        {
            "location": "/pgsql_perf/#tuning-linux",
            "text": "See details on  the dedicated page .",
            "title": "Tuning Linux"
        },
        {
            "location": "/query_optimizations/",
            "text": "Query optimizations\n\u00b6\n\n\nFill factor\n\u00b6\n\n\nFill factor \nreading\n.\n\n\nThe fill factor (default value 100%) tells Postgres what to do with all pages of an updated tuple. In case of 100%, even if a tuple length is less than the minimal page size (8KB by default), the whole page is flagged as dirty and cannot be reused, even for new tuple insertion (it allocates a new page).\n\n\nSo for heavily updated tables with rows less than 8KB, it's worth decreasing the fillfactor value (no less than 10) : it saves many pages allocation, so may IOs and RAM, and speed up updates as well as select (table data is less fragmented).\n\n\nTo measure a table row size, use the following query :\n\n\nSELECT\n \nl\n.\nwhat\n,\n \nl\n.\nnr\n \nAS\n \n\"bytes/ct\"\n\n     \n,\n \nCASE\n \nWHEN\n \nis_size\n \nTHEN\n \npg_size_pretty\n(\nnr\n)\n \nEND\n \nAS\n \nbytes_pretty\n\n     \n,\n \nCASE\n \nWHEN\n \nis_size\n \nTHEN\n \nnr\n \n/\n \nx\n.\nct\n \nEND\n          \nAS\n \nbytes_per_row\n\n\nFROM\n  \n(\n\n   \nSELECT\n \nmin\n(\ntableoid\n)\n        \nAS\n \ntbl\n      \n-- same as 'public.tbl'::regclass::oid\n\n        \n,\n \ncount\n(\n*\n)\n             \nAS\n \nct\n\n        \n,\n \nsum\n(\nlength\n(\nt\n::\ntext\n))\n \nAS\n \ntxt_len\n  \n-- length in characters\n\n   \nFROM\n   \npublic\n.\nmy_table\n \nt\n  \n-- provide table name *once*\n\n   \n)\n \nx\n\n \n,\n \nLATERAL\n \n(\n\n   \nVALUES\n\n      \n(\ntrue\n \n,\n \n'core_relation_size'\n               \n,\n \npg_relation_size\n(\ntbl\n))\n\n    \n,\n \n(\ntrue\n \n,\n \n'visibility_map'\n                   \n,\n \npg_relation_size\n(\ntbl\n,\n \n'vm'\n))\n\n    \n,\n \n(\ntrue\n \n,\n \n'free_space_map'\n                   \n,\n \npg_relation_size\n(\ntbl\n,\n \n'fsm'\n))\n\n    \n,\n \n(\ntrue\n \n,\n \n'table_size_incl_toast'\n            \n,\n \npg_table_size\n(\ntbl\n))\n\n    \n,\n \n(\ntrue\n \n,\n \n'indexes_size'\n                     \n,\n \npg_indexes_size\n(\ntbl\n))\n\n    \n,\n \n(\ntrue\n \n,\n \n'total_size_incl_toast_and_indexes'\n,\n \npg_total_relation_size\n(\ntbl\n))\n\n    \n,\n \n(\ntrue\n \n,\n \n'live_rows_in_text_representation'\n \n,\n \ntxt_len\n)\n\n    \n,\n \n(\nfalse\n,\n \n'------------------------------'\n   \n,\n \nNULL\n)\n\n    \n,\n \n(\nfalse\n,\n \n'row_count'\n                        \n,\n \nct\n)\n\n    \n,\n \n(\nfalse\n,\n \n'live_tuples'\n                      \n,\n \npg_stat_get_live_tuples\n(\ntbl\n))\n\n    \n,\n \n(\nfalse\n,\n \n'dead_tuples'\n                      \n,\n \npg_stat_get_dead_tuples\n(\ntbl\n))\n\n   \n)\n \nl\n(\nis_size\n,\n \nwhat\n,\n \nnr\n);\n\n\n\n\n\n\nHOT update\n\u00b6\n\n\nGood reading\n.\n\n\nHOT (Heap Only Tuple) update is related to the understanding of the fillfactor parameter of each table.\n\n\nHOT concept allows PostgreSQL to reuse a dead page from a previous transaction for an UPDATE or an INSERT. It limits the table fragmentation and speed up the process not to have to create a new tuple.\n\n\n\n\nWarning\n\n\nHOT updates are possible only when there is no index linked to the updated columns. \n\n\n\n\n\n\nTip\n\n\nIf you have big updates, instead of changing large portions of the table at once, you might want to split them up in a couple of chunks.\n\n\n\n\n\n\nTip\n\n\nYou might not be blocking HOT update by adding indexes. You might get better overall performance without them. \n\n\n\n\nLong queries\n\u00b6\n\n\nFor example, to list all queries that last for more than 2 minutes, try the query :\n\n\nSELECT\n \nEXTRACT\n(\nEPOCH\n \nFROM\n \n(\nnow\n()\n \n-\n \nquery_start\n))::\nbigint\n \nas\n \n\"runtime\"\n,\n \nusename\n,\n \napplication_name\n,\n \ndatname\n,\n \nstate\n,\n \nquery\n\n  \nFROM\n  \npg_stat_activity\n\n  \nWHERE\n \nnow\n()\n \n-\n \nquery_start\n \n>\n \ninterval\n \n'2 minutes'\n \nand\n \nstate\n \n=\n \n'active'\n\n\nORDER\n \nBY\n \nruntime\n \nDESC\n;",
            "title": "Query optimizations"
        },
        {
            "location": "/query_optimizations/#query-optimizations",
            "text": "",
            "title": "Query optimizations"
        },
        {
            "location": "/query_optimizations/#fill-factor",
            "text": "Fill factor  reading .  The fill factor (default value 100%) tells Postgres what to do with all pages of an updated tuple. In case of 100%, even if a tuple length is less than the minimal page size (8KB by default), the whole page is flagged as dirty and cannot be reused, even for new tuple insertion (it allocates a new page).  So for heavily updated tables with rows less than 8KB, it's worth decreasing the fillfactor value (no less than 10) : it saves many pages allocation, so may IOs and RAM, and speed up updates as well as select (table data is less fragmented).  To measure a table row size, use the following query :  SELECT   l . what ,   l . nr   AS   \"bytes/ct\" \n      ,   CASE   WHEN   is_size   THEN   pg_size_pretty ( nr )   END   AS   bytes_pretty \n      ,   CASE   WHEN   is_size   THEN   nr   /   x . ct   END            AS   bytes_per_row  FROM    ( \n    SELECT   min ( tableoid )          AS   tbl        -- same as 'public.tbl'::regclass::oid \n         ,   count ( * )               AS   ct \n         ,   sum ( length ( t :: text ))   AS   txt_len    -- length in characters \n    FROM     public . my_table   t    -- provide table name *once* \n    )   x \n  ,   LATERAL   ( \n    VALUES \n       ( true   ,   'core_relation_size'                 ,   pg_relation_size ( tbl )) \n     ,   ( true   ,   'visibility_map'                     ,   pg_relation_size ( tbl ,   'vm' )) \n     ,   ( true   ,   'free_space_map'                     ,   pg_relation_size ( tbl ,   'fsm' )) \n     ,   ( true   ,   'table_size_incl_toast'              ,   pg_table_size ( tbl )) \n     ,   ( true   ,   'indexes_size'                       ,   pg_indexes_size ( tbl )) \n     ,   ( true   ,   'total_size_incl_toast_and_indexes' ,   pg_total_relation_size ( tbl )) \n     ,   ( true   ,   'live_rows_in_text_representation'   ,   txt_len ) \n     ,   ( false ,   '------------------------------'     ,   NULL ) \n     ,   ( false ,   'row_count'                          ,   ct ) \n     ,   ( false ,   'live_tuples'                        ,   pg_stat_get_live_tuples ( tbl )) \n     ,   ( false ,   'dead_tuples'                        ,   pg_stat_get_dead_tuples ( tbl )) \n    )   l ( is_size ,   what ,   nr );",
            "title": "Fill factor"
        },
        {
            "location": "/query_optimizations/#hot-update",
            "text": "Good reading .  HOT (Heap Only Tuple) update is related to the understanding of the fillfactor parameter of each table.  HOT concept allows PostgreSQL to reuse a dead page from a previous transaction for an UPDATE or an INSERT. It limits the table fragmentation and speed up the process not to have to create a new tuple.   Warning  HOT updates are possible only when there is no index linked to the updated columns.     Tip  If you have big updates, instead of changing large portions of the table at once, you might want to split them up in a couple of chunks.    Tip  You might not be blocking HOT update by adding indexes. You might get better overall performance without them.",
            "title": "HOT update"
        },
        {
            "location": "/query_optimizations/#long-queries",
            "text": "For example, to list all queries that last for more than 2 minutes, try the query :  SELECT   EXTRACT ( EPOCH   FROM   ( now ()   -   query_start )):: bigint   as   \"runtime\" ,   usename ,   application_name ,   datname ,   state ,   query \n   FROM    pg_stat_activity \n   WHERE   now ()   -   query_start   >   interval   '2 minutes'   and   state   =   'active'  ORDER   BY   runtime   DESC ;",
            "title": "Long queries"
        },
        {
            "location": "/table_design/",
            "text": "Table design\n\u00b6\n\n\nSome explanations and best practices to design well tables, especially tables with special usage :\n\n\n\n\nhundred of millions / billions of rows\n\n\nor / and heavily updated, inserted or deleted\n\n\n\n\nUsing \npgstattuple\n extension, it's easy to get an accurate row size (without having to perform any VACUUM FULL), which is very useful to verify / understand how PostgreSQL deal with data types, NULL values, ... \n\n\nRow size\n\u00b6\n\n\nSome example of good explanations : \nhere\n and \nhere\n.\n\n\nOfficial doc is \nhere\n.\n\n\nRow is made of a :\n\n\n\n\nheader (23 bytes)\n\n\n1 byte of data alignement (8 bytes per block) --> could be used for NULL bitmap : see \nbelow\n for details \n\n\ndepending on each column type (for example, \nnumerical types sizes\n from official documentation) \n\n\nsome padding to respect data alignment (see \ntypealign\n field description of this \nofficial doc page\n)\n\n\n\n\nPadding doesn't apply for the last column.\n\n\nBest practices\n\u00b6\n\n\nColumn order could change the row size because some types have a padding to respect data alignement.\n\n\nExample :\n\n\ninteger are 4 bytes and need a data alignement of 4 bytes\n\n\n\n\n\nLet's create / fill a table with 2 integer columns.\n\n\nDROP\n \nTABLE\n \nIF\n \nEXISTS\n \nmy_table\n;\n \n\nCREATE\n \nTABLE\n \nmy_table\n \nAS\n \nSELECT\n \n        \ngenerate_series\n(\n1\n,\n100000\n)\n \nas\n \na1\n\n        \n,\ngenerate_series\n(\n1\n,\n100000\n)\n \nas\n \na2\n;\n\n\n\n\n\n\nThe row size is : \n32\n as expected : 24 (header) + 2 * 4 (2 integers)\n\n\nBut with 3 integers and 1 timestamp (expected to be 8 bytes, see \nofficial doc\n), \n\n\nDROP\n \nTABLE\n \nIF\n \nEXISTS\n \nmy_table\n;\n \n\nCREATE\n \nTABLE\n \nmy_table\n \nAS\n \nSELECT\n \n        \n1\n \nas\n \na1\n\n        \n,\n2\n \nas\n \na2\n\n        \n,\n3\n \nas\n \na3\n\n        \n,\ngenerate_series\n(\n'2018-01-01'\n::\ntimestamp\n,\n \n'2018-12-31'\n::\ntimestamp\n,\n \n'1 day'\n::\ninterval\n)\n \nas\n \na4\n;\n\n\n\n\n\n\nThe row size is : \n48\n that is : 24 (header) + 3 * 4 (12 integers) + 4 (padding of last integer) + 8 (timestamp)\n\n\nThe 2nd integer cost 8 bytes, and not 4 !\n\n\nSo imagine column orders including this padding constraint, then check a solution with this methodology. \n\n\nNULL and disk space\n\u00b6\n\n\nEach row is composed of a header then other blocks for data.\n\n\nAt the end of the header (23 bytes), there is a byte free for a NULL bitmap that could be used to flag NULL in any of the first 8 columns. \n\n\nSo, since a table has a 9th column (with a \nNOT NULL\n constraint or not), the null bitmap is extended of an additional 8 bytes (\nMAXALIGN\n on 64bits CPUs) to flag any of the next 64 (8*8) columns.\n\n\nAnd so on.\n\n\nReference \nhere\n.\nAlso \nhere\n.\n\n\nAssuming :\n\n\n\n\na table with all columns as integer (can be null),\n\n\nan integer is 4 bytes long,\n\n\nselect tuple_len / tuple_count from pgstattuple('my_table')\n query gives the tuple size,\n\n\n\n\nwe get :\n\n\n\n\n\n\n\n\nnumber of columns\n\n\nconstraints\n\n\nnb columns full of null\n\n\ntuple size\n\n\nexplanation\n\n\n\n\n\n\n\n\n\n\n8\n\n\nnone\n\n\n8\n\n\n24\n\n\nstandard header = 23+1\n\n\n\n\n\n\n9\n\n\nnone\n\n\n9\n\n\n32\n\n\n24 (standard header) + 8 (new NULL bitmap)\n\n\n\n\n\n\n9\n\n\nnone\n\n\n8\n\n\n36\n\n\n32 (previous) + 4 (int)\n\n\n\n\n\n\n9\n\n\nnone\n\n\n1\n\n\n64\n\n\n24 (standard header)+ 8 * 4 (8 int) + 8 (new NULL bitmap)\n\n\n\n\n\n\n9\n\n\n9th NOT NULL\n\n\n1\n\n\n64\n\n\n24 (standard header)+ 8 * 4 (8 int) + 8 (new NULL bitmap)\n\n\n\n\n\n\n9\n\n\n1th NOT NULL\n\n\n1\n\n\n64\n\n\n24 (standard header)+ 8 * 4 (8 int) + 8 (new NULL bitmap)\n\n\n\n\n\n\n\n\nMethodology of tests\n\u00b6\n\n\nFor the last row of the test, you could use the following steps :\n\n\nCreate / fill the table (100000 rows)\n\n\nDROP\n \nTABLE\n \nIF\n \nEXISTS\n \nmy_table\n;\n \n\nCREATE\n \nTABLE\n \nmy_table\n \nAS\n \nSELECT\n \n        \ngenerate_series\n(\n1\n,\n100000\n)\n \nas\n \na1\n\n        \n,\ngenerate_series\n(\n1\n,\n100000\n)\n \nas\n \na2\n\n        \n,\ngenerate_series\n(\n1\n,\n100000\n)\n \nas\n \na3\n\n        \n,\ngenerate_series\n(\n1\n,\n100000\n)\n \nas\n \na4\n\n        \n,\ngenerate_series\n(\n1\n,\n100000\n)\n \nas\n \na5\n\n        \n,\ngenerate_series\n(\n1\n,\n100000\n)\n \nas\n \na6\n\n        \n,\ngenerate_series\n(\n1\n,\n100000\n)\n \nas\n \na7\n\n        \n,\ngenerate_series\n(\n1\n,\n100000\n)\n \nas\n \na8\n\n        \n,\ngenerate_series\n(\n1\n,\n100000\n)\n \nas\n \na9\n;\n\n\n\n\n\n\nthen\n\n\n-- add the NOT NULL constraint\n\n\nALTER\n \nTABLE\n \nmy_table\n \nALTER\n \nCOLUMN\n \na1\n \nSET\n \nNOT\n \nNULL\n;\n\n\n\n-- fill a column with null values\n\n\nupdate\n \nmy_table\n \nset\n \na5\n=\nnull\n;\n\n\n\n\n\n\nand get the tuple size \n\n\nselect\n \ntuple_len\n \n/\n \ntuple_count\n \nfrom\n \npgstattuple\n(\n'my_table'\n);\n\n\n\n\n\n\nBest practices\n\u00b6\n\n\nEspecially useful for tables with a lot of rows. \n\n\n\n\nbe aware of the data types : minimize the size (use integer for seconds instead of bigint for millis if possible)\n\n\nbe aware of columns order because of alignment padding - it could help you save a lot of disk space and speed up writes, reads, ...\n\n\nNOT NULL : constraint must whenever it's possible (more for explicit coherence reasons than performance - see this \ndiscussion\n).\n\n\nNo special rules on how NULL columns or NOT NULL columns must be ordered.\n\n\n\n\nWhen modifying a table :\n\n\n\n\nit's very cheap to append a new column, even to a huge table, if it could be NULL (only set a bit in the null bitmap).\n\n\nthink about it before adding a 9th column - it's very hard to understand what triggers the 8 bytes extension of the null bitmap for all rows containing at least one NULL. But when it occurs, it will have to rewrite all these rows (because the null bitmap is located between the header and the data).",
            "title": "Table design"
        },
        {
            "location": "/table_design/#table-design",
            "text": "Some explanations and best practices to design well tables, especially tables with special usage :   hundred of millions / billions of rows  or / and heavily updated, inserted or deleted   Using  pgstattuple  extension, it's easy to get an accurate row size (without having to perform any VACUUM FULL), which is very useful to verify / understand how PostgreSQL deal with data types, NULL values, ...",
            "title": "Table design"
        },
        {
            "location": "/table_design/#row-size",
            "text": "Some example of good explanations :  here  and  here .  Official doc is  here .  Row is made of a :   header (23 bytes)  1 byte of data alignement (8 bytes per block) --> could be used for NULL bitmap : see  below  for details   depending on each column type (for example,  numerical types sizes  from official documentation)   some padding to respect data alignment (see  typealign  field description of this  official doc page )   Padding doesn't apply for the last column.",
            "title": "Row size"
        },
        {
            "location": "/table_design/#best-practices",
            "text": "Column order could change the row size because some types have a padding to respect data alignement.  Example :  integer are 4 bytes and need a data alignement of 4 bytes  Let's create / fill a table with 2 integer columns.  DROP   TABLE   IF   EXISTS   my_table ;   CREATE   TABLE   my_table   AS   SELECT  \n         generate_series ( 1 , 100000 )   as   a1 \n         , generate_series ( 1 , 100000 )   as   a2 ;   The row size is :  32  as expected : 24 (header) + 2 * 4 (2 integers)  But with 3 integers and 1 timestamp (expected to be 8 bytes, see  official doc ),   DROP   TABLE   IF   EXISTS   my_table ;   CREATE   TABLE   my_table   AS   SELECT  \n         1   as   a1 \n         , 2   as   a2 \n         , 3   as   a3 \n         , generate_series ( '2018-01-01' :: timestamp ,   '2018-12-31' :: timestamp ,   '1 day' :: interval )   as   a4 ;   The row size is :  48  that is : 24 (header) + 3 * 4 (12 integers) + 4 (padding of last integer) + 8 (timestamp)  The 2nd integer cost 8 bytes, and not 4 !  So imagine column orders including this padding constraint, then check a solution with this methodology.",
            "title": "Best practices"
        },
        {
            "location": "/table_design/#null-and-disk-space",
            "text": "Each row is composed of a header then other blocks for data.  At the end of the header (23 bytes), there is a byte free for a NULL bitmap that could be used to flag NULL in any of the first 8 columns.   So, since a table has a 9th column (with a  NOT NULL  constraint or not), the null bitmap is extended of an additional 8 bytes ( MAXALIGN  on 64bits CPUs) to flag any of the next 64 (8*8) columns.  And so on.  Reference  here .\nAlso  here .  Assuming :   a table with all columns as integer (can be null),  an integer is 4 bytes long,  select tuple_len / tuple_count from pgstattuple('my_table')  query gives the tuple size,   we get :     number of columns  constraints  nb columns full of null  tuple size  explanation      8  none  8  24  standard header = 23+1    9  none  9  32  24 (standard header) + 8 (new NULL bitmap)    9  none  8  36  32 (previous) + 4 (int)    9  none  1  64  24 (standard header)+ 8 * 4 (8 int) + 8 (new NULL bitmap)    9  9th NOT NULL  1  64  24 (standard header)+ 8 * 4 (8 int) + 8 (new NULL bitmap)    9  1th NOT NULL  1  64  24 (standard header)+ 8 * 4 (8 int) + 8 (new NULL bitmap)",
            "title": "NULL and disk space"
        },
        {
            "location": "/table_design/#methodology-of-tests",
            "text": "For the last row of the test, you could use the following steps :  Create / fill the table (100000 rows)  DROP   TABLE   IF   EXISTS   my_table ;   CREATE   TABLE   my_table   AS   SELECT  \n         generate_series ( 1 , 100000 )   as   a1 \n         , generate_series ( 1 , 100000 )   as   a2 \n         , generate_series ( 1 , 100000 )   as   a3 \n         , generate_series ( 1 , 100000 )   as   a4 \n         , generate_series ( 1 , 100000 )   as   a5 \n         , generate_series ( 1 , 100000 )   as   a6 \n         , generate_series ( 1 , 100000 )   as   a7 \n         , generate_series ( 1 , 100000 )   as   a8 \n         , generate_series ( 1 , 100000 )   as   a9 ;   then  -- add the NOT NULL constraint  ALTER   TABLE   my_table   ALTER   COLUMN   a1   SET   NOT   NULL ;  -- fill a column with null values  update   my_table   set   a5 = null ;   and get the tuple size   select   tuple_len   /   tuple_count   from   pgstattuple ( 'my_table' );",
            "title": "Methodology of tests"
        },
        {
            "location": "/table_design/#best-practices_1",
            "text": "Especially useful for tables with a lot of rows.    be aware of the data types : minimize the size (use integer for seconds instead of bigint for millis if possible)  be aware of columns order because of alignment padding - it could help you save a lot of disk space and speed up writes, reads, ...  NOT NULL : constraint must whenever it's possible (more for explicit coherence reasons than performance - see this  discussion ).  No special rules on how NULL columns or NOT NULL columns must be ordered.   When modifying a table :   it's very cheap to append a new column, even to a huge table, if it could be NULL (only set a bit in the null bitmap).  think about it before adding a 9th column - it's very hard to understand what triggers the 8 bytes extension of the null bitmap for all rows containing at least one NULL. But when it occurs, it will have to rewrite all these rows (because the null bitmap is located between the header and the data).",
            "title": "Best practices"
        },
        {
            "location": "/other_points/",
            "text": "Other points\n\u00b6\n\n\nMany points could be considered to improve Postgres performance for each well defined use case. Some of them are listed below with external references, and a summary of what is the expected gain.\n\n\nAutovacuum\n\u00b6\n\n\nLet it be more aggressive on big tables. At least, down these parameters :\n\n\nautovacuum_vacuum_scale_factor = 0.01       # or even less\nautovacuum_analyze_scale_factor = 0.01      # or even less\n\n\n\n\n\nIt's a good idea to also make it more aggressive (with the same parameter values ?) on main catalog tables :\n\n\n\n\n\n\n\n\ntable_name\n\n\ntable_size\n\n\nindex_size\n\n\ntotal_size\n\n\n\n\n\n\n\n\n\n\npg_catalog.pg_statistic\n\n\n351 MB\n\n\n6856 kB\n\n\n358 MB\n\n\n\n\n\n\npg_catalog.pg_attribute\n\n\n170 MB\n\n\n81 MB\n\n\n252 MB\n\n\n\n\n\n\npg_catalog.pg_class\n\n\n55 MB\n\n\n120 MB\n\n\n174 MB\n\n\n\n\n\n\npg_catalog.pg_depend\n\n\n26 MB\n\n\n54 MB\n\n\n80 MB\n\n\n\n\n\n\npg_catalog.pg_type\n\n\n26 MB\n\n\n27 MB\n\n\n53 MB\n\n\n\n\n\n\npg_catalog.pg_index\n\n\n27 MB\n\n\n9760 kB\n\n\n37 MB\n\n\n\n\n\n\n\n\nA good reading : \nhttps://wiki.postgresql.org/images/5/5e/Autovacuum_pgconfeu_gorthx.pdf\n.\n\n\nAutoanalyze\n\u00b6\n\n\nTo be tweacked at least like autovacuum.\n\n\nSome day, when other parameters will be well mastered, you could have a look to analyze tuning by column. It could be interesting to consider two main cases, both related to huge tables :\n\n\n\n\nthe default \ndefault_statistics_target = 100\n could be too small to compute good stats from a so small sample (100 rows),\n\n\nif the data distribution of a column do not change in the time, maybe we could turn autoanlyze off for this column. \n\n\n\n\nIndex\n\u00b6\n\n\nTo design indexes for a table, one gold rule to follow is : \n\n\nIndex first for equality, then for ranges.\n\n\n\n\n\nGood reading (in french) to understand multi-column indexes, partial indexes, ... : \nhttp://use-the-index-luke.com/fr/sql/la-clause-where/rechercher-un-intervalle\n. \n\n\nBRIN indexes\n\u00b6\n\n\nBRIN (\nBlock Range INdexes\n) indexes were introduced in PostgreSQL 9.5 and were designed as range indexes. Link to \nofficial doc\n.\n\n\nThey took much less place than B-tree indexes and are quite slower. Their overhead on INSERT is very tiny, unlike for B-tree.\n\n\nTo resume their interest.\nBRIN indexes can speed things up a lot, but only if your data has a strong natural ordering to begin with. In our case, using a BRIN index was 500 times more space efficient than a B-tree index, and ran a bit faster too. We managed to improve our best case time by a factor of 2.6 and our worst case time by a factor of 30 with some simple optimizations\n. Citation from \nthis post\n.\n\n\n\n\nNote\n\n\nThey could be of huge interest for time series data : all queries contains range clause for dates, never equality.\nIn case of aggregation queries, it let the opportunity to bench the interest to add B-tree indexes on columns involved in GROUP BY clauses.\n\n\n\n\nDetailed statistics\n\u00b6\n\n\nLog analyzer\n\u00b6\n\n\nPgBadger\n is a powerful log analyzer that produce nice & useful reports. Just have to activate some logging flags. The most responsible for possible overhead is \nlog_min_duration_statement = 0\n. On busy server, possibly increase this value not to log all queries.  \n\n\nStatements\n\u00b6\n\n\nThe official extension \nhttps://www.postgresql.org/docs/current/static/pgstatstatements.html\n provides tracking of execution statistics on the server.\n\n\nTo reset the stats gathered by this extension : \nSELECT pg_stat_statements_reset();\n.\n\n\n\n\nNote\n\n\nThis extension seems not to could cause a significant overhead.\n\n\n\n\nVacuum\n\u00b6\n\n\nThe official extension \nhttps://www.postgresql.org/docs/current/static/pgstattuple.html\n provides various functions to obtain tuple-level / page-level statistics. \n\n\n\n\nWarning\n\n\nNo idea if this extension could cause or not a significant overhead.\n\n\n\n\nCache\n\u00b6\n\n\nThe official extension \nhttps://www.postgresql.org/docs/current/static/pgbuffercache.html\n provides a means for examining what's happening in the shared buffer cache in real time.\n\n\n\n\nWarning\n\n\nNo idea if this extension could cause or not a significant overhead.",
            "title": "Other points"
        },
        {
            "location": "/other_points/#other-points",
            "text": "Many points could be considered to improve Postgres performance for each well defined use case. Some of them are listed below with external references, and a summary of what is the expected gain.",
            "title": "Other points"
        },
        {
            "location": "/other_points/#autovacuum",
            "text": "Let it be more aggressive on big tables. At least, down these parameters :  autovacuum_vacuum_scale_factor = 0.01       # or even less\nautovacuum_analyze_scale_factor = 0.01      # or even less  It's a good idea to also make it more aggressive (with the same parameter values ?) on main catalog tables :     table_name  table_size  index_size  total_size      pg_catalog.pg_statistic  351 MB  6856 kB  358 MB    pg_catalog.pg_attribute  170 MB  81 MB  252 MB    pg_catalog.pg_class  55 MB  120 MB  174 MB    pg_catalog.pg_depend  26 MB  54 MB  80 MB    pg_catalog.pg_type  26 MB  27 MB  53 MB    pg_catalog.pg_index  27 MB  9760 kB  37 MB     A good reading :  https://wiki.postgresql.org/images/5/5e/Autovacuum_pgconfeu_gorthx.pdf .",
            "title": "Autovacuum"
        },
        {
            "location": "/other_points/#autoanalyze",
            "text": "To be tweacked at least like autovacuum.  Some day, when other parameters will be well mastered, you could have a look to analyze tuning by column. It could be interesting to consider two main cases, both related to huge tables :   the default  default_statistics_target = 100  could be too small to compute good stats from a so small sample (100 rows),  if the data distribution of a column do not change in the time, maybe we could turn autoanlyze off for this column.",
            "title": "Autoanalyze"
        },
        {
            "location": "/other_points/#index",
            "text": "To design indexes for a table, one gold rule to follow is :   Index first for equality, then for ranges.  Good reading (in french) to understand multi-column indexes, partial indexes, ... :  http://use-the-index-luke.com/fr/sql/la-clause-where/rechercher-un-intervalle .",
            "title": "Index"
        },
        {
            "location": "/other_points/#brin-indexes",
            "text": "BRIN ( Block Range INdexes ) indexes were introduced in PostgreSQL 9.5 and were designed as range indexes. Link to  official doc .  They took much less place than B-tree indexes and are quite slower. Their overhead on INSERT is very tiny, unlike for B-tree.  To resume their interest. BRIN indexes can speed things up a lot, but only if your data has a strong natural ordering to begin with. In our case, using a BRIN index was 500 times more space efficient than a B-tree index, and ran a bit faster too. We managed to improve our best case time by a factor of 2.6 and our worst case time by a factor of 30 with some simple optimizations . Citation from  this post .   Note  They could be of huge interest for time series data : all queries contains range clause for dates, never equality.\nIn case of aggregation queries, it let the opportunity to bench the interest to add B-tree indexes on columns involved in GROUP BY clauses.",
            "title": "BRIN indexes"
        },
        {
            "location": "/other_points/#detailed-statistics",
            "text": "",
            "title": "Detailed statistics"
        },
        {
            "location": "/other_points/#log-analyzer",
            "text": "PgBadger  is a powerful log analyzer that produce nice & useful reports. Just have to activate some logging flags. The most responsible for possible overhead is  log_min_duration_statement = 0 . On busy server, possibly increase this value not to log all queries.",
            "title": "Log analyzer"
        },
        {
            "location": "/other_points/#statements",
            "text": "The official extension  https://www.postgresql.org/docs/current/static/pgstatstatements.html  provides tracking of execution statistics on the server.  To reset the stats gathered by this extension :  SELECT pg_stat_statements_reset(); .   Note  This extension seems not to could cause a significant overhead.",
            "title": "Statements"
        },
        {
            "location": "/other_points/#vacuum",
            "text": "The official extension  https://www.postgresql.org/docs/current/static/pgstattuple.html  provides various functions to obtain tuple-level / page-level statistics.    Warning  No idea if this extension could cause or not a significant overhead.",
            "title": "Vacuum"
        },
        {
            "location": "/other_points/#cache",
            "text": "The official extension  https://www.postgresql.org/docs/current/static/pgbuffercache.html  provides a means for examining what's happening in the shared buffer cache in real time.   Warning  No idea if this extension could cause or not a significant overhead.",
            "title": "Cache"
        },
        {
            "location": "/linux_tuning/",
            "text": "Tuning Linux for PostgreSQL\n\u00b6\n\n\nA reference conf could be found \nhere\n and its slides \nhere\n. \nGood advices \nthere\n.\n\n\nKernel cache tuning\n\u00b6\n\n\nKernel parameters \ndirty*\n manage the cache behavior. A good understanding of how cache works could be found \nthere\n.  \n\n\nBy default :\n\n\nscaillet@Sylvain ~ $ sudo sysctl -a \n|\n grep dirty\n\nvm.dirty_background_bytes \n=\n \n0\n       \n# ratio limit applies\n\nvm.dirty_background_ratio \n=\n \n10\n      \n# 10% of RAM is available for cache --> huge on modern servers ! \n\nvm.dirty_bytes \n=\n \n0\n                  \n# ratio limit applies\n\nvm.dirty_expire_centisecs \n=\n \n3000\n    \n# cache has to flush at least every 30s\n\nvm.dirty_ratio \n=\n \n20\n                 \n# hard limit of 20% of RAM\n\nvm.dirty_writeback_centisecs \n=\n \n500\n  \nvm.dirtytime_expire_seconds \n=\n \n43200\n\n\n\n\n\n\nThey are set for basic OS usage. In case of Database server with high workload, it should be tuned not to reduce the impact of \nthe checkpoint tuning\n, for example.\n\n\nOne of the main drawback of bad tuning is heavy I/O pikes. They could occured because :\n\n\n\n\nthe cache write asynchroneously most of the time\n\n\nlot of I/O overwhelm the cache\n\n\nthe cache decided to flush everything by turning writings synchroneously  \n\n\n\n\nIf we force the cache to flush its data more frequently, it might reduce the heavy pike cases.\n\n\nYou can also see statistics on the page cache in /proc/vmstat:\n\n\n$ cat /proc/vmstat \n|\n egrep \n\"dirty|writeback\"\n\n nr_dirty \n878\n\n nr_writeback \n0\n\n nr_writeback_temp \n0\n\n\n\n\n\n\nIn my case I have 878 dirty pages waiting to be written to disk.\n\n\nTypically for our usage (heavy workload), a good way is to reduce the cache size by lowering ratios :\n\n\nvm.dirty_background_ratio = 5\nvm.dirty_ratio = 10\n\n\n\n\n\nAnother approch is given by Ilya Kosmodemiansky in \nthe slide 39\n.\n\n\nvm.dirty_background_bytes = 67108864\nvm.dirty_bytes = 536870912\n\n\n\n\n\nThese values looks more reasonable for RAID with 512MB cache on board.\n\n\nIn fact, it's good to configure the cache to have max some hundreds of MB to synchronize, no more : less than 1GB.\n\n\nWhat on VM ??\n\nIn the following \nslides\n, the 13th gives a definitive advice : no VM for Postgres !\n\n\nKernel other parameters\n\u00b6\n\n\nOn dedicated servers, we could tune some other kernel parameters.\n\n\n\n\n\n\nSwap is bad for SGBD. Set : \n\n\nvm.swappiness = 5 # or 10\n\n\n\n\n\n\nOvercommit is also bad. Set : \n\n\nvm.overcommit_memory = 2 # to deactivate\nvm.overcommit_ratio = (RAM \u2013 SWAP) * 100 / RAM\n\n\n\n\n\n\nTo know how much swap is configured on a server, type \nswapon -s\n to display the size in Kbytes.\n\n\nTo learn more about overcommit values, see the \nlinux documentation\n.\nPossible values are : \n\n\n0\n:\n \nheuristic\n \novercommit\n \n(\nthis\n \nis\n \nthe\n \ndefault\n)\n\n\n1\n:\n \nalways\n \novercommit\n,\n \nnever\n \ncheck\n\n\n2\n:\n \nalways\n \ncheck\n,\n \nnever\n \novercommit\n\n\n\n\n\n\n\n\n\n\nNUMA architecture is also bad for PostgreSQL : \n\n\nvm.zone_reclaim_mode = 0 # to deactivate",
            "title": "Linux tuning"
        },
        {
            "location": "/linux_tuning/#tuning-linux-for-postgresql",
            "text": "A reference conf could be found  here  and its slides  here . \nGood advices  there .",
            "title": "Tuning Linux for PostgreSQL"
        },
        {
            "location": "/linux_tuning/#kernel-cache-tuning",
            "text": "Kernel parameters  dirty*  manage the cache behavior. A good understanding of how cache works could be found  there .    By default :  scaillet@Sylvain ~ $ sudo sysctl -a  |  grep dirty\n\nvm.dirty_background_bytes  =   0         # ratio limit applies \nvm.dirty_background_ratio  =   10        # 10% of RAM is available for cache --> huge on modern servers !  \nvm.dirty_bytes  =   0                    # ratio limit applies \nvm.dirty_expire_centisecs  =   3000      # cache has to flush at least every 30s \nvm.dirty_ratio  =   20                   # hard limit of 20% of RAM \nvm.dirty_writeback_centisecs  =   500   \nvm.dirtytime_expire_seconds  =   43200   They are set for basic OS usage. In case of Database server with high workload, it should be tuned not to reduce the impact of  the checkpoint tuning , for example.  One of the main drawback of bad tuning is heavy I/O pikes. They could occured because :   the cache write asynchroneously most of the time  lot of I/O overwhelm the cache  the cache decided to flush everything by turning writings synchroneously     If we force the cache to flush its data more frequently, it might reduce the heavy pike cases.  You can also see statistics on the page cache in /proc/vmstat:  $ cat /proc/vmstat  |  egrep  \"dirty|writeback\" \n nr_dirty  878 \n nr_writeback  0 \n nr_writeback_temp  0   In my case I have 878 dirty pages waiting to be written to disk.  Typically for our usage (heavy workload), a good way is to reduce the cache size by lowering ratios :  vm.dirty_background_ratio = 5\nvm.dirty_ratio = 10  Another approch is given by Ilya Kosmodemiansky in  the slide 39 .  vm.dirty_background_bytes = 67108864\nvm.dirty_bytes = 536870912  These values looks more reasonable for RAID with 512MB cache on board.  In fact, it's good to configure the cache to have max some hundreds of MB to synchronize, no more : less than 1GB.  What on VM ?? \nIn the following  slides , the 13th gives a definitive advice : no VM for Postgres !",
            "title": "Kernel cache tuning"
        },
        {
            "location": "/linux_tuning/#kernel-other-parameters",
            "text": "On dedicated servers, we could tune some other kernel parameters.    Swap is bad for SGBD. Set :   vm.swappiness = 5 # or 10    Overcommit is also bad. Set :   vm.overcommit_memory = 2 # to deactivate\nvm.overcommit_ratio = (RAM \u2013 SWAP) * 100 / RAM    To know how much swap is configured on a server, type  swapon -s  to display the size in Kbytes.  To learn more about overcommit values, see the  linux documentation .\nPossible values are :   0 :   heuristic   overcommit   ( this   is   the   default )  1 :   always   overcommit ,   never   check  2 :   always   check ,   never   overcommit     NUMA architecture is also bad for PostgreSQL :   vm.zone_reclaim_mode = 0 # to deactivate",
            "title": "Kernel other parameters"
        },
        {
            "location": "/useful-tools/",
            "text": "useful tools\n\u00b6\n\n\nIn /var/lib/postgres/bin/\n\u00b6\n\n\npg_test_fsync\n\u00b6\n\n\nTest what write methods is the more efficient. See \ndoc for details\n.",
            "title": "useful tools"
        },
        {
            "location": "/useful-tools/#useful-tools",
            "text": "",
            "title": "useful tools"
        },
        {
            "location": "/useful-tools/#in-varlibpostgresbin",
            "text": "",
            "title": "In /var/lib/postgres/bin/"
        },
        {
            "location": "/useful-tools/#pg_test_fsync",
            "text": "Test what write methods is the more efficient. See  doc for details .",
            "title": "pg_test_fsync"
        },
        {
            "location": "/useful-scripts/",
            "text": "useful scripts\n\u00b6\n\n\nSome scripts are particularly useful to have a good snapshot of a PostgreSQL server health.\n\n\nPlenty are available at this address : \nhttps://github.com/pgexperts/pgx_scripts",
            "title": "useful scripts"
        },
        {
            "location": "/useful-scripts/#useful-scripts",
            "text": "Some scripts are particularly useful to have a good snapshot of a PostgreSQL server health.  Plenty are available at this address :  https://github.com/pgexperts/pgx_scripts",
            "title": "useful scripts"
        }
    ]
}